











# Start time
import time
script_start_time = time.time()

# Ignore warning messages only
import warnings
warnings.filterwarnings("ignore")
from sklearn.exceptions import ConvergenceWarning

# Suppress only ConvergenceWarning
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# garbage collection
import gc
import copy

# Importing the Pandas library for data manipulation and analysis
import pandas as pd

# Importing the NumPy library for numerical operations
import numpy as np
import math

# Importing Matplotlib for data visualization
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Importing train_test_split function to split the dataset into training and testing sets
from sklearn.model_selection import train_test_split

# Importing Logistic Regression model from scikit-learn for binary classification
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import GridSearchCV

# Importing Decision Tree Classifier from scikit-learn for classification tasks
from sklearn.tree import DecisionTreeClassifier

# Importing Random Forest Classifier from scikit-learn for ensemble learning
from sklearn.ensemble import RandomForestClassifier

from sklearn.impute import KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import LabelEncoder
from statsmodels.stats.outliers_influence import variance_inflation_factor
from fancyimpute import KNN
from sklearn.metrics.pairwise import nan_euclidean_distances
from sklearn.neighbors import NearestNeighbors
from sklearn.tree import plot_tree
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, roc_curve, make_scorer, f1_score
from itertools import combinations
from scipy.stats import chi2_contingency


# Functions for use in the main code

# Data processing functions
def summarize_dataframe(sdf):
    """
    Summarizes the structure and content of a DataFrame.
    """
    return pd.DataFrame({
        'Column': sdf.columns,
        'Non-Null Count': sdf.notnull().sum(),
        'Data Type': sdf.dtypes,
        'Unique Values': sdf.nunique(),
        'Missing Values': sdf.isnull().sum(),
        'Missing Values %': ((sdf.isnull().sum()*100)/len(sdf)).round(2)
    }).reset_index(drop=True)


def impute_missing_values(df):
    """
    Imputes missing values in a dataset using KNN for low/moderate missingness 
    and Iterative Imputer for high missingness. Categorical columns are encoded 
    using LabelEncoder and can be converted back to their original values.

    Parameters:
    df (pd.DataFrame): The input dataset with missing values.

    Returns:
    pd.DataFrame: The dataset with missing values imputed.
    dict: A dictionary of LabelEncoder objects for categorical columns.
    """

# Column segregation
    columns_knn = [
        'mortgage_amount_due', 'property_current_value',
        'loan_request_reason', 'applicant_job_type',
        'age_of_oldest_credit_line', 'existing_credit_lines'
    ]
    columns_iterative = [
        'debt_to_income_ratio', 'major_derogatory_reports',
        'delinquent_credit_lines', 'years_at_present_job',
        'recent_credit_inquiries'
    ]
    
    # Encode categorical columns for KNN
    categorical_columns = ['loan_request_reason', 'applicant_job_type']
    # Convert object columns to category dtype before encoding
    original_categories = {}
    for col in categorical_columns:
        df[col] = df[col].astype('category')  # Convert to category
        original_categories[col] = df[col].cat.categories  # Save the original categories

    for col in categorical_columns:
        df[col] = df[col].astype('category').cat.codes.replace(-1, np.nan)  # Encode categories and handle NaN
    
    # Impute with KNN
    knn_imputer = KNN(k=5, verbose=False)
    df[columns_knn] = knn_imputer.fit_transform(df[columns_knn])
    
    # Impute with IterativeImputer
    iterative_imputer = IterativeImputer(max_iter=10, random_state=42, verbose=0)
    df[columns_iterative] = iterative_imputer.fit_transform(df[columns_iterative])

    # Return the fully imputed dataset and label encoders
    return df, original_categories


def decode_categorical_columns(df, label_encoders):
    """
    Decodes categorical columns back to their original values using LabelEncoder objects.

    Parameters:
    df (pd.DataFrame): The DataFrame with encoded categorical columns.
    label_encoders (dict): A dictionary of LabelEncoder objects for each categorical column.

    Returns:
    pd.DataFrame: The DataFrame with categorical columns decoded to their original values.
    """
    df_decoded = df.copy()
    for col, le in label_encoders.items():
        df_decoded[col] = le.inverse_transform(df_decoded[col].astype(int))
    return df_decoded


def calculate_corr_and_vif(input_data, target):
    """
    Calculate the correlation matrix and Variance Inflation Factor (VIF)
    for numerical predictor columns in a given DataFrame, excluding the target variable.

    Parameters:
    dataframe (pd.DataFrame): The input DataFrame.
    target_variable (str): The name of the target variable to exclude.

    Returns:
    tuple: A tuple containing the correlation matrix (pd.DataFrame)
           and VIF values (pd.DataFrame).
    """
    # Copy the dataset
    dataframe = input_data.copy()
    
    # Drop the target variable
    dataframe.drop(target, axis=1, inplace=True)
    
    # Select only numerical columns
    numeric_df = dataframe.select_dtypes(include=[np.number])
    
    # Calculate the correlation matrix
    corr_matrix = numeric_df.corr()
    
    # Calculate VIF for each numerical column
    vif_data = pd.DataFrame()
    vif_data["Feature"] = numeric_df.columns
    vif_data["VIF"] = [
        variance_inflation_factor(numeric_df.values, i)
        for i in range(numeric_df.shape[1])
    ]
    
    return corr_matrix, vif_data


# Data visualization functions


def plot_grouped_barplots(data, features, target, figsize=(15, 6)):
    """
    Plots grouped barplots for a list of features against a target variable.

    Parameters:
        data (pd.DataFrame): The input dataset.
        features (list): List of features to group by.
        target (str): The target variable to sum for each group.
        figsize (tuple): Size of each subplot figure. Default is (15, 6).

    Returns:
        None: Displays the plots.
    """
    # Determine grid size for subplots
    num_features = len(features)
    n_cols = 3  # Number of columns per row
    n_rows = math.ceil(num_features / n_cols)

    # Create subplots
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(figsize[0], figsize[1] * n_rows))
    axes = axes.flatten()  # Flatten the axes array for easy indexing

    # Iterate over features and create barplots
    for i, feature in enumerate(features):
        # Group and sum data
        summed_data = data.groupby(feature)[target].sum().reset_index()

        # Create barplot
        sns.barplot(x=feature, y=target, data=summed_data, ax=axes[i])
        axes[i].set_title(f'Sum of {target} by {feature}')
        axes[i].set_xlabel(feature)
        axes[i].set_ylabel(f'Sum of {target}')
        axes[i].tick_params(axis='x', rotation=90)  # Rotate x-axis labels for better readability
        total = summed_data[target].sum()
        for bar in axes[i].patches:
            height = bar.get_height()
            percentage = (height / total) * 100  # Calculate percentage
            if percentage > 0:
                axes[i].text(bar.get_x() + bar.get_width() / 2, height, f'{percentage:.1f}%', ha='center', va='bottom', fontsize=10)
        sns.despine()

    # Turn off any unused subplots
    for j in range(num_features, len(axes)):
        axes[j].axis('off')

    # Adjust layout and display the plots
    plt.tight_layout()
    plt.show()


def histogram_boxplot(data, feature, figsize=(12, 7), kde=True, bins=None):
    """
    Boxplot and histogram combined with automatic bins and x-axis ticks.
    Ensures the x-axis starts at zero.

    Parameters:
    - data: DataFrame
    - feature: Column in the DataFrame
    - figsize: Size of the figure (default (12, 7))
    - kde: Whether to show density curve (default True)
    - bins: Number of bins or sequence of bin edges for histogram (default None)
    """

    # Create the subplots for boxplot and histogram
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,      # Number of rows of the subplot grid = 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )

    # Boxplot with the mean indicated
    sns.boxplot(data=data, x=feature, ax=ax_box2, showmeans=True, color="violet")

    # Calculate automatic bins if not provided
    if bins is None:
        q75, q25 = data[feature].quantile([0.75, 0.25])  # Interquartile range
        iqr = q75 - q25
        if len(data[feature]) == 0:  # Handle empty feature
            raise ValueError(f"Feature '{feature}' contains no data.")
        if data[feature].max() - data[feature].min() == 0:  # Handle zero range
            bins = 1
        else:
            bin_width = 2 * iqr / (len(data[feature]) ** (1 / 3))
            if bin_width <= 0:  # Handle zero or negative bin width
                bin_width = 1  # Default fallback
            bins = int((data[feature].max() - data[feature].min()) / bin_width)
            bins = max(1, bins)  # Ensure at least 1 bin

    # Create the histogram
    sns.histplot(data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, color="blue")

    # Add mean and median to the histogram
    ax_hist2.axvline(data[feature].mean(), color="green", linestyle="--", label="Mean")
    ax_hist2.axvline(data[feature].median(), color="black", linestyle="-", label="Median")
    ax_hist2.legend()

    # Ensure x-axis starts at 0
    ax_hist2.set_xlim(left=0)  # Set the minimum x-axis value to 0

    # Automatically adjust x-axis ticks
    x_min, x_max = ax_hist2.get_xlim()  # Get the updated limits of the x-axis
    ticks = np.linspace(x_min, x_max, num=10)  # Generate evenly spaced ticks
    ax_hist2.set_xticks(ticks)  # Set the new ticks

    plt.show()


def plot_barplots(data, variables, target_variable, nrows=3, ncols=2, figsize=(18, 10)):
    """
    Create bar plots showing the sum of a target variable grouped by each variable in the list.

    Parameters:
    - data (DataFrame): The DataFrame containing the data.
    - variables (list): List of variables to plot.
    - target_variable (str): The target variable to aggregate.
    - nrows (int): Number of rows in the subplot grid.
    - ncols (int): Number of columns in the subplot grid.
    - figsize (tuple): Figure size for the plot.
    """
    # Create subplots
    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)
    axes = axes.flatten()  # Flatten the array for easier indexing

    for i, var in enumerate(variables):
        if i >= len(axes):
            break  # Avoid indexing errors if there are more variables than subplots

        # Calculate the sum of the target variable for each category of the variable
        var_sums = data.groupby(var)[target_variable].sum().reset_index()

        # Create the barplot
        sns.barplot(x=var, y=target_variable, data=var_sums, ax=axes[i])
        axes[i].set_title(f'Sum of {target_variable} by {var}')
        axes[i].set_xlabel(var)
        axes[i].set_ylabel(f'Sum of {target_variable}')
        axes[i].tick_params(axis='x', rotation=90)  # Rotate x-axis labels for better readability

    # Remove any extra subplots if the number of variables is less than the grid size
    for j in range(len(variables), len(axes)):
        fig.delaxes(axes[j])

    # Adjust layout and display the plot
    plt.tight_layout()
    plt.show()


def plot_binned_features(data, features, status, bins=5, title=None):
    """
    Plots subplots for binned bar charts of multiple features against a status variable.

    Parameters:
        data (pd.DataFrame): The dataset containing the features and status.
        features (list): A list of continuous variables to be binned and plotted.
        status (str): The variable to sum for each bin, plotted on the y-axis.
        bins (int): Number of bins to create for the features.
        title (str, optional): Title of the overall plot. Default is None.

    Returns:
        None: Displays the subplots.
    """
    # Calculate the grid dimensions
    n_rows, n_cols = 6, 2  # Fixed 4 rows and 3 columns
    total_subplots = n_rows * n_cols
    num_features = len(features)

    # Create a figure with subplots
    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))
    fig.suptitle(title if title else f"Features vs {status}", fontsize=16)

    # Flatten axes for easy indexing
    axes = axes.flatten()

    # Iterate over features and create plots
    for i, feature in enumerate(features):
        ax = axes[i]

        # Group by feature and calculate the sum of the status variable
        grouped_data = data.groupby(feature)[status].sum().reset_index()

        # Create bins for the feature
        grouped_data['feature_bin'] = pd.cut(grouped_data[feature], bins=bins)

        # Group by bins and calculate the sum of the status variable
        binned_data = grouped_data.groupby('feature_bin')[status].sum().reset_index()

        # Generate bin labels
        bin_edges = grouped_data['feature_bin'].cat.categories
        bin_labels = [f"{int(interval.left)}-{int(interval.right)}" for interval in bin_edges]

        # Bar plot
        sns.barplot(data=binned_data, x='feature_bin', y=status, ax=ax)

        # Add title and labels
        ax.set_title(f"{feature} vs {status}")
        ax.set_xlabel(f"{feature}")
        ax.set_ylabel(f"{status}")

        # Set x-axis tick labels
        ax.set_xticks(range(len(bin_labels)))
        ax.set_xticklabels(bin_labels, rotation=45)
        total = grouped_data[status].sum()
        for bar in ax.patches:
            height = bar.get_height()
            percentage = (height / total) * 100  # Calculate percentage
            if percentage > 0:
                ax.text(bar.get_x() + bar.get_width() / 2, height, f'{percentage:.1f}%', ha='center', va='bottom', fontsize=10)
        sns.despine()

    # Turn off extra subplots
    for i in range(num_features, total_subplots):
        axes[i].axis('off')

    # Adjust layout and show the plot
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()


def plot_feature_importance(importances, columns):
    """
    Plots feature importances as a bar chart.

    Args:
      importances: A list or array of feature importances.
      columns: A list of feature names corresponding to the importances.
    """

    # Create a DataFrame for the feature importances
    importance_df = pd.DataFrame(importances, index=columns, columns=['Importance']).sort_values(by='Importance', ascending=False)

    # Plot the feature importances
    plt.figure(figsize=(13, 13))
    sns.barplot(x=importance_df.Importance, y=importance_df.index, palette="magma") 
    plt.title("Feature Importances", fontsize=16)
    plt.xlabel("Importance", fontsize=14)
    plt.ylabel("Features", fontsize=14)
    plt.show()


def visualize_decision_tree(dt, X, max_depth=4):
    """
    Visualizes a decision tree using matplotlib.

    Args:
      dt: The trained decision tree model.
      X: The DataFrame or array containing the features used for training.
      max_depth (int): The maximum depth of the tree to display (default: 4).
    """

    plt.figure(figsize=(30, 20))
    plot_tree(dt, 
              max_depth=max_depth, 
              feature_names=list(X.columns), 
              filled=True, 
              fontsize=12, 
              node_ids=True, 
              class_names=None)
    plt.show()


def plot_distributions(column, original_data, imputed_data):
    """
    Plots the distributions of a feature before and after imputation.

    Parameters:
    column (str): The column name to plot.
    original_data (DataFrame): The original dataset with missing values.
    imputed_data (DataFrame): The dataset after imputation.
    """
    plt.figure(figsize=(10, 6))
    sns.kdeplot(original_data[column], label="Before Imputation", fill=True, alpha=0.5, color="blue")
    sns.kdeplot(imputed_data[column], label="After Imputation", fill=True, alpha=0.5, color="green")
    plt.title(f'Distribution of {column}: Before vs. After Imputation', fontsize=14)
    plt.xlabel(column)
    plt.ylabel('Density')
    plt.legend()
    plt.grid(True)
    plt.show()


def plot_correlation_heatmap(data, title):
    """
    Plots the correlation heatmap of the dataset.

    Parameters:
    data (DataFrame): The dataset to plot.
    title (str): The title of the heatmap.
    """
    plt.figure(figsize=(12, 10))
    correlation_matrix = data.corr()
    sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm")
    plt.title(title, fontsize=16)
    plt.show()


def plot_missingness_heatmap(data, title):
    """
    Plots a heatmap of missing values in the dataset.

    Parameters:
    data (DataFrame): The dataset to plot.
    title (str): The title of the heatmap.
    """
    plt.figure(figsize=(12, 8))
    sns.heatmap(data.isnull(), cbar=False, cmap="viridis")
    plt.title(title, fontsize=16)
    plt.show()


# Model evaluation functions


def metrics_score(actual, predicted):
    """
    Calculates and displays classification metrics and confusion matrix.
    """
    print(classification_report(actual, predicted))
    cm = confusion_matrix(actual, predicted)
    plt.figure(figsize=(8, 5))
    sns.heatmap(cm, annot=True, fmt='.2f', xticklabels=['Not defaulted', 'Defaulted'],
                yticklabels=['Not Defaulted', 'Defaulted'])
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()


def model_performance_classification(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier

    predictors: independent variables

    target: dependent variable
    """

    # Predicting using the independent variables
    pred = model.predict(predictors)

    recall = recall_score(target, pred,average = 'macro')                 # To compute recall

    precision = precision_score(target, pred, average = 'macro')              # To compute precision

    acc = accuracy_score(target, pred)                                 # To compute accuracy score


    # Creating a dataframe of metrics

    df_perf = pd.DataFrame(
        {
            "Precision":  precision,
            "Recall":  recall,
            "Accuracy": acc,
        },

        index = [0],
    )

    return df_perf


def chi_squared_test(data, target, bins=5):
    """
    Perform the Chi-Squared test for independence between all features
    and the target variable, handling both categorical and continuous features.

    Parameters:
        data (pd.DataFrame): The input dataset.
        target (str): The name of the target variable.
        bins (int): Number of bins to discretize continuous features. Default is 5.

    Returns:
        pd.DataFrame: A DataFrame containing features, their P-values, 
                      and whether they are significant.
    """
    # Separate target column
    target_data = data[target]
    features = data.drop(columns=[target])

    # List to store results for each feature
    results = []

    for feature in features.columns:
        if features[feature].dtype in ['object', 'category']:
            # Categorical feature: Use directly
            feature_data = features[feature]
        else:
            # Continuous feature: Discretize into bins
            feature_data = pd.cut(features[feature], bins=bins, labels=False)

        # Create a contingency table for the current feature and target
        contingency_table = pd.crosstab(feature_data, target_data)

        # Perform the Chi-Squared test for independence
        chi2, p, dof, expected = chi2_contingency(contingency_table)

        # Append the feature and p-value to the results list
        results.append({'Feature': feature, 'P-value': p, 'Significant': p < 0.05})

    # Convert results to a DataFrame for better visualization
    results_df = pd.DataFrame(results)

    # Sort the results by P-value in ascending order
    sorted_results = results_df.sort_values(by='P-value', ascending=True)

    return sorted_results


# General utilities functions


def segregate_columns_by_dtype(df):
    """
    Segregates DataFrame columns by their data types.
    """
    return {dtype.name: df.select_dtypes(include=[dtype]).columns.tolist() for dtype in df.dtypes.unique()}


def calculate_expected_with_confidence(original_data, loan_value, interest_rate=0.085):
    """
    Calculate expected profit, explicitly accounting for FP, FN, and confidence.

    Parameters:
    - original_data: pd.DataFrame, containing 'prob_default', 'actual', and loan value columns.
    - loan_value: str, name of the column containing loan values.
    - interest_rate: float, interest rate applied to loans (default: 8.5%).

    Returns:
    - float: Total expected profit with confidence-adjusted revenue.
    - float: Average confidence level.
    """
    # Add P(non-default) = 1 - P(default) to the dataset
    original_data['prob_non_default'] = 1 - original_data['prob_default']
    
    # Calculate confidence scores
    original_data['confidence'] = original_data[['prob_non_default', 'prob_default']].max(axis=1)
    
    # Revenue from True Negatives (TN): Correctly predicted non-defaults
    original_data['revenue_tn'] = (
        original_data['prob_non_default'] 
        * (1 - original_data['client_defaulted_on_loan']) 
        * original_data['confidence']  # Adjust by confidence
        * original_data[loan_value] 
        * interest_rate
    )
    
    # Loss from False Positives (FP): Predicted default but actual non-default
    original_data['loss_fp'] = (
        original_data['prob_default'] 
        * (1 - original_data['client_defaulted_on_loan']) 
        * (1 - original_data['confidence'])  # Adjust by lack of confidence
        * original_data[loan_value] 
        * interest_rate
    )
    
    # Loss from False Negatives (FN): Predicted non-default but actual default
    original_data['loss_fn'] = (
        original_data['prob_non_default'] 
        * original_data['client_defaulted_on_loan'] 
        * (1 - original_data['confidence'])  # Adjust by lack of confidence
        * original_data[loan_value] 
        * (1 + interest_rate)
    )
    
    # Calculate total profit
    total_profit = original_data['revenue_tn'].sum() - original_data['loss_fp'].sum() - original_data['loss_fn'].sum()
    
    # Calculate average confidence
    avg_confidence = original_data['confidence'].mean()
    
    return total_profit, avg_confidence


# Plot explained variance ratio
def plot_pca(pca_object):
    plt.figure(figsize=(8, 6))
    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o')
    plt.title('Cumulative Explained Variance')
    plt.xlabel('Number of Principal Components')
    plt.ylabel('Cumulative Variance Explained')
    plt.grid()
    plt.show()





# Import data from CSV file into a Pandas DataFrame
df = pd.read_csv('../data/hmeq.csv')


# Make a copy of the original dataframe for processing, so the original data stays intact
default = df.copy()


# Update column names for improved readability
default.rename(columns={
    'BAD': 'client_defaulted_on_loan',  # 1 = Client defaulted on loan, 0 = loan repaid
    'LOAN': 'loan_amount_approved',      # Amount of loan approved
    'MORTDUE': 'mortgage_amount_due',    # Amount due on the existing mortgage
    'VALUE': 'property_current_value',    # Current value of the property
    'REASON': 'loan_request_reason',      # Reason for the loan request
    'JOB': 'applicant_job_type',          # Type of job that loan applicant has
    'YOJ': 'years_at_present_job',        # Years at present job
    'DEROG': 'major_derogatory_reports',  # Number of major derogatory reports
    'DELINQ': 'delinquent_credit_lines',  # Number of delinquent credit lines
    'CLAGE': 'age_of_oldest_credit_line',  # Age of the oldest credit line in months
    'NINQ': 'recent_credit_inquiries',     # Number of recent credit inquiries
    'CLNO': 'existing_credit_lines',       # Number of existing credit lines
    'DEBTINC': 'debt_to_income_ratio'      # Debt-to-income ratio
}, inplace=True)


# Check the shape of the dataset
default.shape


# Use the summarize_dataframe function and display the summary
summarize_dataframe(default)


# Check for duplicates and drop if found
num_duplicates = default.duplicated().sum()  # Count duplicates

if num_duplicates > 0:
    print(f"Number of duplicate rows: {num_duplicates}")
    default.drop_duplicates(keep='first', inplace=True)  # Drop duplicates, keeping the first
    print(len(df) - len(default), "Duplicate rows dropped (keeping the first occurrence).")
    default.reset_index(drop=True, inplace=True)  # Reset index after dropping duplicates
else:
    print("No duplicate rows found.")


# Iterate through each non-numeric column and print its unique values
for column in default.select_dtypes(include=['object']):
    print(f"{column}: {default[column].value_counts()}")





# Check for inconsisties in the data
# Typical mortgage to property value ratio is ~ 0.8. Lets confirm it
default['ltv'] = default['mortgage_amount_due'] / default['property_current_value']  # create a column ltv (loan to value ratio)
high_ltv = default[default['ltv'] >= 2]  # Create a temp dataframe of rows with ltv > 2

# Plot the count of clients with ltv greater than 2.
plt.hist(high_ltv['ltv'], bins=np.arange(int(high_ltv['ltv'].min()), int(high_ltv['ltv'].max()) + 2) - 0.5, edgecolor='black')
plt.xlabel('ltv Value')
plt.ylabel('Count')
plt.title('Histogram of ltv with Integer Bins')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()


# Drop all records with ltv greater than or equal to 2 and the ltv column as it is no longer necessary
default.drop(default[default['ltv'] >= 2].index, axis=0, inplace=True)
default.drop('ltv', axis=1, inplace=True)


# delete the temporary dataframe
del high_ltv
# Trigger garbage collection and print the number of objects collected
collected_objects = gc.collect()
print(f"Garbage collector freed {collected_objects} objects.")








# Review the key statistics of the data, transposed for better readability
default.describe().T




















# Treat missing values using the custom function 'impute_missing_values', which implements the KNN and Iterative imputation based on % missing values for each column
default_imputed_num, categories = impute_missing_values(default)

# Upon inspection of the dataset after imputation, it is observed that in some of the rows, property value and mortgage due were imputed to 0. These zero values were set to the median value of the column
for col in ['property_current_value', 'mortgage_amount_due']:
    median_value = default_imputed_num[col].median()
    default_imputed_num.loc[default_imputed_num[col] == 0, col] = median_value


# Create a copy of the imputed data in numeric format (could be of use later!)
default_imputed = default_imputed_num.copy()

# Convert categorical columns back to original categories by using the saved original categories
for col in ['loan_request_reason', 'applicant_job_type']:
    default_imputed[col] = pd.Categorical.from_codes(default_imputed_num[col].round().astype(int), categories=categories[col])
# Confirm no more missing values
summarize_dataframe(default_imputed)





# Create list of columns by dtype(s)
col_types = segregate_columns_by_dtype(default_imputed)

# get numeric columns from the column type list
num_cols = col_types['float64'] + col_types['int64']

# Remove target column
num_cols.remove('client_defaulted_on_loan')

# get categorical columns from the column type list
cat_cols = col_types['category']





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[0])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[1])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[2])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[3])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[4])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[5])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[6])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[7])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[8])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[9])





# Plot distribution of categorical columns
# Determine the number of rows and columns for subplots
numof_cols = len(cat_cols)
cols_per_row = 3  # Number of subplots per row
num_rows = math.ceil(numof_cols / cols_per_row)  # Calculate the required number of rows

# Create subplots
fig, axes = plt.subplots(num_rows, cols_per_row, figsize=(15, 5 * num_rows))  # Adjust size as needed
axes = axes.flatten()  # Flatten the axes array for easy iteration

# Plot each column
for i, col in enumerate(cat_cols):
    sns.histplot(default_imputed, x=col, ax=axes[i])  # Use the corresponding subplot axis
    axes[i].set_title(f"Histogram for {col}")
    total = len(default_imputed[col])
    for bar in axes[i].patches:
        height = bar.get_height()
        percentage = (height / total) * 100  # Calculate percentage
        axes[i].text(bar.get_x() + bar.get_width() / 2, height, f'{percentage:.1f}%', ha='center', va='bottom', fontsize=10)
    sns.despine()
    
# Hide any unused subplots
for j in range(i + 1, len(axes)):
    axes[j].set_visible(False)

# Adjust layout
plt.tight_layout()
plt.show()











# Calculate the correlation matrix
correlation_matrix = default_imputed[num_cols].corr()
# Plot the heatmap
plt.figure(figsize=(16, 8))  # Adjust figure size if needed
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", vmin=-1, vmax=1)
plt.title('Correlation Heatmap')
plt.show()


# Sort correlations and exclude duplicates
sorted_unique_correlations = (
    correlation_matrix
    .stack()                            # Flatten the matrix into a Series
    .reset_index()                      # Reset index to access column pairs
    .rename(columns={0: 'correlation'})  # Rename correlation column
    .query("level_0 < level_1")         # Keep only one direction (unique pairs)
    .sort_values(by='correlation', ascending=False)  # Sort by correlation
)
# Print the columns with correlation values
print(sorted_unique_correlations)








# Function to plot binned features
# Parameters:
# - `default`: DataFrame containing the data
# - `binned_cols_to_plot`: List of columns to be binned and plotted
# - `'client_defaulted_on_loan_num'`: Target variable for analysis (e.g., 1 = defaulted, 0 = not defaulted)
# - `40`: Number of bins to divide the numerical values into for plotting
plot_binned_features(default_imputed, num_cols, 'client_defaulted_on_loan', 20)








# Plot the categorical variable against the target variable
plot_grouped_barplots(default_imputed, cat_cols, 'client_defaulted_on_loan')





# Perform the Chi-squared test to see if any of the features are not significant to predicting the target variable
print(chi_squared_test(default_imputed, 'client_defaulted_on_loan'))





# Calculate correlation matrix and VIF, excluding the target variable
corr_matrix, vif_values = calculate_corr_and_vif(default_imputed, 'client_defaulted_on_loan')

# Display results
print("Correlation Matrix:")

sorted_corr_matrix = corr_matrix[corr_matrix.columns[corr_matrix.iloc[0].argsort()[::-1]]]

sorted_corr_matrix


# Print VIF values
print(vif_values)














# Standardize the data using the dataset with all numeric values from the impute missing values function output (PCA works only with numeric values)
default_imputed_scaled = default_imputed_num.drop('client_defaulted_on_loan', axis=1) # Drop the target variable
 # Scale the data using Z transformation
scaler = StandardScaler()
default_imputed_scaled = pd.DataFrame(scaler.fit_transform(default_imputed_scaled), columns=default_imputed_scaled.columns)


# Apply PCA
pca = PCA(n_components=10)  # Reduce to 10 principal components
default_pca = pd.DataFrame(pca.fit_transform(default_imputed_scaled), columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])


# Plot the cumulative variance explained by each principle component for 
plot_pca(pca)
# Print explained variance by each component
for i, ratio in enumerate(pca.explained_variance_ratio_, start=1):
    print(f"PC{i}: {ratio:.2%}")








# Create a DataFrame to represent the principal component loadings
# Rows are the principal components (PCs), and columns are the original features
loadings = pd.DataFrame(
    pca.components_,  # PCA component weights
    columns=default_imputed_scaled.columns,  # Original feature names
    index=[f'PC{i+1}' for i in range(len(pca.components_))]  # Label PCs as PC1, PC2, etc.
)

# Sort the loadings of the first principal component (PC1) in descending order
# loadings.iloc[0] refers to the first row (PC1), and argsort()[::-1] sorts indices in descending order
sorted_loadings = loadings[loadings.columns[loadings.iloc[0].argsort()[::-1]]]

# Display the sorted loadings
sorted_loadings





# Create a df for engineered features
default_imputed_scaled_eng = default_imputed_scaled.copy()

# New feature is the ratio of mortgate due to property value
default_imputed_scaled_eng['mortgage_to_property_ratio'] = default_imputed_scaled_eng['mortgage_amount_due'] / default_imputed_scaled_eng['property_current_value']

# Drop the original features
default_imputed_scaled_eng.drop(['mortgage_amount_due', 'property_current_value'], axis=1, inplace=True)

# Scale the new feature
default_imputed_scaled_eng['mortgage_to_property_ratio'] = StandardScaler().fit_transform(
    default_imputed_scaled_eng[['mortgage_to_property_ratio']]
)


# Apply PCA
pca_nopv = PCA(n_components=10)  # First 10 PCs
default_pcanopv = pd.DataFrame(pca_nopv.fit_transform(default_imputed_scaled_eng), columns=['PCe1', 'PCe2', 'PCe3', 'PCe4', 'PCe5', 'PCe6', 'PCe7', 'PCe8', 'PCe9', 'PCe10'])


# Plot explained variance ratio
plot_pca(pca_nopv)

# Print explained variance by each component
for i, ratio in enumerate(pca_nopv.explained_variance_ratio_, start=1):
    print(f"PCe{i}: {ratio:.2%}")


loadings_noPV = pd.DataFrame(pca_nopv.components_, columns=default_imputed_scaled_eng.columns, index=[f'PCe{i+1}' for i in range(len(pca_nopv.components_))])
sorted_loadings_noPV = loadings_noPV[loadings_noPV.columns[loadings_noPV.iloc[0].argsort()[::-1]]]
sorted_loadings_noPV

















# Create a dataframe for modeling from original imputed dataframe
default_model = copy.deepcopy(default_imputed)
default_model['ltv'] = default_model['mortgage_amount_due']/default_model['property_current_value']  # Create the new ltv feature


default_model.drop(columns=['mortgage_amount_due', 'property_current_value'], inplace=True)  # Drop the original feature used to create ltv


# Transform the categorical variables using one-hot encoding
default_model = pd.get_dummies(default_model, columns=['loan_request_reason', 'applicant_job_type'])


# Separate the target variable
Y = default_model.client_defaulted_on_loan
X = default_model.drop('client_defaulted_on_loan', axis=1)


Y.value_counts()


# Scale the data
sc = StandardScaler()

X_scaled = sc.fit_transform(X)

X_scaled = pd.DataFrame(X_scaled, columns = X.columns)


# Create the training anb testing datasets
x_train, x_test, y_train, y_test = train_test_split(X_scaled, Y, test_size = 0.3, random_state = 1, stratify = Y)
# Reset index of test set for alignment
x_test = x_test.reset_index(drop=True)
y_test = y_test.reset_index(drop=True)











model_metrics = pd.DataFrame(columns = ['Model Name', 'Model Type', 'Grid Search (Yes/No)', 'Recall', 'Precision', 'f1', 'Accuracy', 'FPs', 'FNs', 'Revenue Prediction', 'Confidence'])
model_metrics





# Fit logistic regression model to training data with default parameters except solver = liblinear, since liblinear solver is optimized for linear problems with small datasets and binary classification
lg = LogisticRegression(verbose=1, solver='liblinear', random_state=1)
lg.fit(x_train, y_train)


# Check the performance on the training data
y_pred_train = lg.predict(x_train)
metrics_score(y_train, y_pred_train)


# Checking the performance on the test data

# Check the metrics on the test data
y_pred_test_lg = lg.predict(x_test)
metrics_score(y_test, y_pred_test_lg)

# get the probabilities and update in the dataset
default_model_lg = default_model.copy()
pred_prob_lg = lg.predict_proba(X_scaled)
default_model_lg['prob_default'] = pred_prob_lg[:, 1]  # Column for P(default)

# Calculate the expected revenue
revenue_lg, confidence_lg = calculate_expected_with_confidence(default_model_lg, 'loan_amount_approved')
print("Total expected revenue:", revenue_lg, 'with confidence of:', confidence_lg)


cr = classification_report(y_test, y_pred_test_lg, output_dict=True)
cm = confusion_matrix(y_test, y_pred_test_lg)
lg_metrics_compare = {'Model Name': 'lg',
                      'Model Type': 'Logistic Regression',
                      'Grid Search (Yes/No)': 'No',
                      'Recall': cr['1']['recall'],
                      'Precision':cr['1']['precision'],
                      'f1': cr['1']['f1-score'],
                      'Accuracy':cr['accuracy'], 
                      'FPs': cm[0][1],
                      'FNs': cm[1][0],
                      'Revenue Prediction': revenue_lg,
                      'Confidence': confidence_lg
                     }
model_metrics.loc[len(model_metrics)] = lg_metrics_compare





# Driving Factors: Feature Importance
coefficients = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': lg.coef_[0]
}).sort_values(by='Coefficient', ascending=False)

# Plot 1: Feature Coefficients
plt.figure(figsize=(10, 6))
plt.bar(coefficients['Feature'], coefficients['Coefficient'], alpha=0.7)
plt.title('Driving Factors: Feature Coefficients', fontsize=14)
plt.xlabel('Features', fontsize=12)
plt.ylabel('Coefficient Value', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# Predict probabilities and labels
y_pred_prob = lg.predict_proba(x_test)[:, 1]
y_pred = lg.predict(x_test)

# Compute confusion matrix to identify false positives
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

# Filter false positives (predicted 1, actual 0)
false_positives_df = x_test[(y_pred == 1) & (y_test == 0)]

# Visualize mean feature values
false_positive_features = false_positives_df.mean().sort_values(ascending=False)

# Plot
plt.figure(figsize=(10, 6))
plt.bar(false_positive_features.index, false_positive_features.values, alpha=0.7)
plt.title('Mean Feature Values for False Positives', fontsize=14)
plt.xlabel('Features', fontsize=12)
plt.ylabel('Mean Value', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()





# Define the model
log_reg = LogisticRegression(random_state=1)

# Define the parameter grid to search over
param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization techniques
    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength
    'solver': ['lbfgs', 'liblinear', 'saga'],  # Solvers compatible with l1 and elasticnet penalties
    'max_iter': [500, 1000, 2000, 3000]  # Number of iterations for convergence
}

# Define scoring metric
optimum = ['accuracy', 'f1', 'recall', 'precision']

results = {}  # Dictionary to store all results dynamically

for opt in optimum:
    # Set up GridSearchCV
    grid_search = GridSearchCV(
        estimator=log_reg,
        param_grid=param_grid,
        scoring=opt,  # Choose your metric here
        cv=5,  # 5-fold cross-validation
        verbose=1,  # Print progress
        n_jobs=-1  # Use all available processors
    )
    
    # Perform the grid search on the training data
    grid_search.fit(x_train, y_train)
    
    # Test the optimized model on the test set
    best_model = grid_search.best_estimator_
    best_model.fit(x_train, y_train)
    y_pred = best_model.predict(x_test)

    # Calculate metrics
    recall = recall_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)
    
    # Confusion Matrix to extract FPs and FNs
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

    # Predict probabilities for revenue prediction
    pred_prob = best_model.predict_proba(X_scaled)
    default_model_copy = default_model.copy()
    default_model_copy['prob_default'] = pred_prob[:, 1]

    # Calculate expected profit and confidence
    profit, confidence = calculate_expected_with_confidence(default_model_copy, 'loan_amount_approved')

    # Append results to the DataFrame
    new_row = {
        "Model Name": f"lg_{opt}",
        "Model Type": "Logistic Regression",
        "Grid Search (Yes/No)": "Yes",
        "Recall": recall,
        "Precision": precision,
        "f1": f1,
        "Accuracy": accuracy,
        "FPs": fp,
        "FNs": fn,
        "Revenue Prediction": profit,
        "Confidence": confidence
    }
    model_metrics.loc[len(model_metrics)] = new_row
    print('Expected profit', profit, 'with confidence', confidence)
    metrics_score(y_test, y_pred)


model_metrics








# Building decision tree model
dt = DecisionTreeClassifier(criterion='entropy', random_state=1)


# Fitting decision tree model
dt.fit(x_train, y_train)


# Checking performance on the training dataset
y_train_pred_dt = dt.predict(x_train)

metrics_score(y_train, y_train_pred_dt)


# Checking the performance on the test data
pred_prob_dt = dt.predict_proba(X_scaled)
default_model['prob_default'] = pred_prob_dt[:, 1]  # Column for P(default)
# Calculate profit and loss for the tailored dataset

revenue_dt, confidence_dt = calculate_expected_with_confidence(default_model, 'loan_amount_approved')
print("Total expected revenue:", revenue_dt, 'with confidence of:', confidence_dt)

# Checking performance on the training dataset
y_test_pred_dt = dt.predict(x_test)

metrics_score(y_test, y_test_pred_dt)


cr_dt = classification_report(y_test, y_test_pred_dt, output_dict=True)
cm_dt = confusion_matrix(y_test, y_test_pred_dt)
dt_metrics_compare = {'Model Name': 'dt',
                      'Model Type': 'Decision Tree',
                      'Grid Search (Yes/No)': 'No',
                      'Recall': cr_dt['1']['recall'],
                      'Precision':cr_dt['1']['precision'],
                      'f1': cr_dt['1']['f1-score'],
                      'Accuracy':cr_dt['accuracy'], 
                      'FPs': cm_dt[0][1],
                      'FNs': cm_dt[1][0],
                      'Revenue Prediction': revenue_dt,
                      'Confidence': confidence_dt
                     }
model_metrics.loc[len(model_metrics)] = dt_metrics_compare


# Retrieve Feature Importances
importances = dt.feature_importances_

# Create a DataFrame for Feature Importances
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Plotting with Proper Label Alignment
x_labels = importance_df['Feature']
y_values = importance_df['Importance']

plt.bar(np.arange(len(x_labels)), y_values, align='center')
plt.xticks(np.arange(len(x_labels)), x_labels, rotation=45, ha='right')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Feature Importances from Decision Tree')
plt.tight_layout()  # Adjust layout to fit labels
plt.show()








# Choose the type of classifier
dtree_estimator_grid = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
parameters = {
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'max_features': ['sqrt', 'log2', 0.5],
    'criterion': ['gini', 'entropy']
}

# Define scoring metric
optimum = ['accuracy', 'f1', 'recall', 'precision']

for opt in optimum:
    # Set up GridSearchCV
    grid_search = GridSearchCV(
        estimator=dtree_estimator_grid,
        param_grid=parameters,
        scoring=opt,  # Choose your metric here
        cv=5,  # 5-fold cross-validation
        n_jobs=-1  # Use all available processors
    )

    # Perform the grid search on the training data
    grid_search.fit(x_train, y_train)
    
    # Get the best parameters and best score
    best_params = grid_search.best_params_
    best_score = grid_search.best_score_
    
    print("Best Parameters:", best_params)
    print("Best Cross-Validation Score:", best_score)
    
    # Test the optimized model on the test set
    best_model = grid_search.best_estimator_
    best_model.fit(x_train, y_train)
    test_accuracy = best_model.score(x_test, y_test)

    # Make predictions
    y_grid = best_model.predict(x_test)

    # Calculate metrics
    recall = recall_score(y_test, y_grid)
    precision = precision_score(y_test, y_grid)
    f1 = f1_score(y_test, y_grid)
    accuracy = accuracy_score(y_test, y_grid)
    
    # Confusion Matrix to extract FPs and FNs
    tn, fp, fn, tp = confusion_matrix(y_test, y_grid).ravel()

    # Predict probabilities for revenue prediction
    pred_prob_best = best_model.predict_proba(X_scaled)
    default_model_copy = default_model.copy()
    default_model_copy['prob_default'] = pred_prob_best[:, 1]

    # Calculate expected profit and confidence
    profit, confidence = calculate_expected_with_confidence(default_model_copy, 'loan_amount_approved')
    print("Total expected profit:", profit, "with confidence of:", confidence)

    # Append results to the DataFrame
    new_row = {
        "Model Name": f"dt_{opt}",
        "Model Type": "Decision Tree",
        "Grid Search (Yes/No)": "Yes",
        "Recall": recall,
        "Precision": precision,
        "f1": f1,
        "Accuracy": accuracy,
        "FPs": fp,
        "FNs": fn,
        "Revenue Prediction": profit,
        "Confidence": confidence
    }
    model_metrics.loc[len(model_metrics)] = new_row








# Fitting the Random Forest classifier on the training data
rf_estimator = RandomForestClassifier(criterion='entropy', random_state=1)

rf_estimator.fit(x_train, y_train)


# Checking performance on the training data
y_pred_train_rf = rf_estimator.predict(x_train)

metrics_score(y_train, y_pred_train_rf)


# Checking performance on the testing data
pred_prob_rf = rf_estimator.predict_proba(X_scaled)
default_model_rf = default_model.copy()
default_model_rf['prob_default'] = pred_prob_rf[:, 1]  # Column for P(default)
# Calculate profit and loss for the tailored dataset

revenue_rf, confidence_rf = calculate_expected_with_confidence(default_model_rf, 'loan_amount_approved')
print("Total expected profit:", revenue_rf, 'with confidence of:', confidence_rf)


y_pred_test_rf = rf_estimator.predict(x_test)

metrics_score(y_test, y_pred_test_rf)


cr_rf = classification_report(y_test, y_pred_test_rf, output_dict=True)
cm_rf = confusion_matrix(y_test, y_pred_test_rf)
rf_metrics_compare = {'Model Name': 'rf_estimator',
                      'Model Type': 'Random Forest',
                      'Grid Search (Yes/No)': 'No',
                      'Recall': cr_rf['1']['recall'],
                      'Precision':cr_rf['1']['precision'],
                      'f1': cr_rf['1']['f1-score'],
                      'Accuracy':cr_rf['accuracy'], 
                      'FPs': cm_rf[0][1],
                      'FNs': cm_rf[1][0],
                      'Revenue Prediction': revenue_dt,
                      'Confidence': confidence_dt
                     }
model_metrics.loc[len(model_metrics)] = rf_metrics_compare


model_metrics


from sklearn.metrics import precision_recall_curve

# Predict probabilities
y_probs = rf_estimator.predict_proba(x_test)[:, 1]

# Compute precision-recall curve
precision, recall, thresholds = precision_recall_curve(y_test, y_probs)

# Plot values of precisions, recalls, and thresholds
plt.figure(figsize = (10, 7))

plt.plot(thresholds, precision[:-1], 'b--', label = 'precision')

plt.plot(thresholds, recall[:-1], 'g--', label = 'recall')

plt.xlabel('Threshold')

plt.legend(loc = 'upper left')

plt.ylim([0, 1])

plt.show()


# Adjust the threshold
threshold = 0.3  # Lower than default (0.5)
y_pred_threshold = (y_probs >= threshold).astype(int)

metrics_score(y_test, y_pred_threshold)


# Retrieve Feature Importances
importances = rf_estimator.feature_importances_

# Create a DataFrame for Feature Importances
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Plotting with Proper Label Alignment
x_labels = importance_df['Feature']
y_values = importance_df['Importance']

plt.bar(np.arange(len(x_labels)), y_values, align='center')
plt.xticks(np.arange(len(x_labels)), x_labels, rotation=45, ha='right')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Feature Importances from RandomForect classifier')
plt.tight_layout()  # Adjust layout to fit labels
plt.show()








# Choose the type of classifier
rf_estimator_tuned = RandomForestClassifier()

# Grid of parameters to choose from
params_rf = {
    'criterion': ['gini', 'entropy'],
    'n_estimators': [100, 200, 500],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'max_features': ['sqrt', 'log2', 0.5],
    'bootstrap': [True, False]
}
# Define scoring metric
optimum = ['accuracy', 'f1', 'recall', 'precision']

# for opt in optimum:
#     # Set up GridSearchCV
#     grid_search = GridSearchCV(
#         estimator=rf_estimator_tuned,
#         param_grid=params_rf,
#         scoring=opt,  # Choose your metric here
#         cv=5,  # 5-fold cross-validation
#         n_jobs=-1  # Use all available processors
#     )

#     # Perform the grid search on the training data
#     grid_search.fit(x_train, y_train)
    
#     # Get the best parameters and best score
#     best_params = grid_search.best_params_
#     best_score = grid_search.best_score_
    
#     print("Best Parameters:", best_params)
#     print("Best Cross-Validation Score:", best_score)
    
#     # Test the optimized model on the test set
#     best_model = grid_search.best_estimator_
#     best_model.fit(x_train, y_train)
#     test_accuracy = best_model.score(x_test, y_test)
#     print(f"{opt}:", grid_search.best_score_)

#     pred_prob_best = best_model.predict_proba(X_scaled)
#     default_model['prob_default'] = pred_prob_best[:, 1] # Add predictions to the original dataset

#     profit, confidence = calculate_expected_with_confidence(default_model, 'loan_amount_approved')
#     print("Total expected profit:", profit, 'with confidence of:', confidence)

#     y_grid = best_model.predict(x_test)
#     metrics_score(y_test, y_grid)


# Trigger garbage collection and print the number of objects collected
collected_objects = gc.collect()
print(f"Garbage collector freed {collected_objects} objects.")














print('All Working')
# End time
script_end_time = time.time()
total_time = script_end_time - script_start_time
print(f"Total execution time for the script: {total_time:.2f} seconds")


# Importing the XGBClassifier from the xgboost library
from xgboost import XGBClassifier

# XGBoost Classifier
xgb = XGBClassifier(random_state = 1, eval_metric = 'logloss')

# Fitting the model
xgb.fit(x_train,y_train)

# Model Performance on the test data
xgb_perf_test = model_performance_classification(xgb,x_test,y_test)

xgb_perf_test


from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier




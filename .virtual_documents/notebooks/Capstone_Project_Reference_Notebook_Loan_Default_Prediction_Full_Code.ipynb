











import time
import warnings
import gc
import copy
import math

# Data manipulation and analysis
import pandas as pd
import numpy as np

# Data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocessing and dimensionality reduction
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Model selection and training
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Imputation and missing value handling
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from fancyimpute import KNN

# Metrics and evaluation
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score, 
    precision_score, recall_score, f1_score, precision_recall_curve
)
from statsmodels.stats.outliers_influence import variance_inflation_factor
from scipy.stats import chi2_contingency

# Ignore warnings selectively
warnings.filterwarnings("ignore")
warnings.filterwarnings("ignore", message=".*Some other specific message.*")
warnings.filterwarnings("ignore", message=".*'force_all_finite' was renamed to 'ensure_all_finite'.*")

# Start time
script_start_time = time.time()

# Garbage collection
gc.collect()

# Add any additional functions or utility imports here


# Data Processing Functions
def summarize_dataframe(df):
    """
    Summarizes the structure and content of a DataFrame.
    """
    summary = {
        "Column": df.columns,
        "Non-Null Count": df.notnull().sum(),
        "Data Type": df.dtypes,
        "Unique Values": df.nunique(),
        "Missing Values": df.isnull().sum(),
        "Missing Values %": (df.isnull().sum() / len(df) * 100).round(2),
    }
    return pd.DataFrame(summary).reset_index(drop=True)


def segregate_columns_by_dtype(df):
    """
    Segregates DataFrame columns by their data types.
    """
    return {dtype.name: df.select_dtypes(include=[dtype]).columns.tolist() for dtype in df.dtypes.unique()}


def impute_missing_values(df, knn_cols, iterative_cols, categorical_cols):
    """
    Imputes missing values using KNN and IterativeImputer.
    """
    original_categories = {}
    
    # Encode categorical columns for KNN
    for col in categorical_cols:
        df[col] = df[col].astype("category")
        original_categories[col] = df[col].cat.categories
        df[col] = df[col].cat.codes.replace(-1, np.nan)
    
    # Impute with KNN
    df[knn_cols] = KNN(k=5, verbose=False).fit_transform(df[knn_cols])
    
    # Impute with IterativeImputer
    df[iterative_cols] = IterativeImputer(max_iter=10, random_state=42).fit_transform(df[iterative_cols])

    return df, original_categories


def decode_categorical_columns(df, original_categories):
    """
    Decodes categorical columns back to original values.
    """
    for col, categories in original_categories.items():
        df[col] = pd.Categorical.from_codes(df[col], categories)
    return df


def calculate_corr_and_vif(df, target):
    """
    Calculates correlation matrix and VIF for numerical features.
    """
    df = df.drop(columns=[target], errors="ignore")
    numeric_df = df.select_dtypes(include=[np.number])
    
    corr_matrix = numeric_df.corr()
    vif_data = pd.DataFrame({
        "Feature": numeric_df.columns,
        "VIF": [variance_inflation_factor(numeric_df.values, i) for i in range(numeric_df.shape[1])]
    })
    return corr_matrix, vif_data


# Visualization Functions
def plot_correlation_heatmap(data, title="Correlation Heatmap"):
    """
    Plots a heatmap of correlations in the dataset.
    """
    plt.figure(figsize=(12, 10))
    sns.heatmap(data.corr(), annot=True, fmt=".2f", cmap="coolwarm")
    plt.title(title, fontsize=16)
    plt.show()


def plot_pca(pca_object, title="Cumulative Explained Variance"):
    """
    Plots the cumulative explained variance ratio from a PCA object.
    """
    plt.figure(figsize=(8, 6))
    plt.plot(range(1, len(pca_object.explained_variance_ratio_) + 1),
             np.cumsum(pca_object.explained_variance_ratio_), 'r', marker="o")
    plt.title(title)
    plt.xlabel("Number of Principal Components")
    plt.ylabel("Cumulative Variance Explained")
    # plt.grid()
    plt.show()


def engineer_and_scale_features(data, ratio_cols, new_feature_name, drop_original=True):
    """
    Engineer a new feature as the ratio of two columns, scale it, and optionally drop the original columns.

    Parameters:
        data (pd.DataFrame): Input DataFrame.
        ratio_cols (tuple): A tuple of two column names (numerator, denominator) to compute the ratio.
        drop_original (bool): Whether to drop the original columns after creating the new feature. Default is True.

    Returns:
        pd.DataFrame: DataFrame with the new engineered and scaled feature.
    """
    df = data.copy()

    # Create the new feature
    df[new_feature_name] = df[ratio_cols[0]] / df[ratio_cols[1]]

    # Drop the original columns if specified
    if drop_original:
        df.drop(list(ratio_cols), axis=1, inplace=True)

    # Scale the new feature
    df[new_feature_name] = StandardScaler().fit_transform(df[[new_feature_name]])

    return df


# Model Evaluation Functions
def metrics_score(actual, predicted):
    """
    Calculates classification metrics and plots a confusion matrix.
    """
    print(classification_report(actual, predicted))
    cm = confusion_matrix(actual, predicted)
    plt.figure(figsize=(8, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="rainbow_r",
                xticklabels=["Not Defaulted", "Defaulted"],
                yticklabels=["Not Defaulted", "Defaulted"])
    plt.ylabel("Actual")
    plt.xlabel("Predicted")
    plt.title("Confusion Matrix")
    plt.show()

def calculate_expected_with_confidence(data, loan_value_col, interest_rate=0.085):
    """
    Calculate the expected revenue/loss and average confidence from predictions.

    Parameters:
        data (pd.DataFrame): Dataset containing the columns 'prob_default', 'actual', and the loan value column.
        loan_value_col (str): The column name representing the loan values.
        interest_rate (float): The interest rate applied to loans. Default is 8.5%.

    Returns:
        tuple: A tuple containing:
            - Total expected revenue (float)
            - Average confidence level (float)
    """
    # Add P(non-default) = 1 - P(default) to the dataset
    data['prob_non_default'] = 1 - data['prob_default']
    
    # Calculate confidence scores as the maximum probability of the prediction
    data['confidence'] = data[['prob_default', 'prob_non_default']].max(axis=1)
    
    # Revenue from True Negatives (TN): Correctly predicted non-defaults
    data['revenue_tn'] = (
        data['prob_non_default'] 
        * (1 - data['client_defaulted_on_loan'])  # Actual is non-default
        * data['confidence']  # Adjust by confidence level
        * data[loan_value_col] 
        * interest_rate
    )
    
    # Loss from False Positives (FP): Predicted default but actual non-default
    data['loss_fp'] = (
        data['prob_default'] 
        * (1 - data['client_defaulted_on_loan'])  # Actual is non-default
        * (1 - data['confidence'])  # Adjust by lack of confidence
        * data[loan_value_col] 
        * interest_rate
    )
    
    # Loss from False Negatives (FN): Predicted non-default but actual default
    data['loss_fn'] = (
        data['prob_non_default'] 
        * data['client_defaulted_on_loan']  # Actual is default
        * (1 - data['confidence'])  # Adjust by lack of confidence
        * data[loan_value_col] 
        * (1 + interest_rate)  # Account for the loan principal and interest
    )
    
    # Total revenue = Sum of revenue from TN - Sum of losses from FP and FN
    total_revenue = data['revenue_tn'].sum() - data['loss_fp'].sum() - data['loss_fn'].sum()
    
    # Average confidence level
    avg_confidence = data['confidence'].mean()
    
    return total_revenue, avg_confidence


def model_performance_classification(model, predictors, target):
    """
    Evaluates the performance of a classification model.
    """
    pred = model.predict(predictors)
    scores = {
        "Precision": precision_score(target, pred, average="macro"),
        "Recall": recall_score(target, pred, average="macro"),
        "Accuracy": accuracy_score(target, pred),
    }
    return pd.DataFrame([scores])

# Function to evaluate the model and append results
def evaluate_model_and_metrics(
    model, x_test, y_test, X_scaled, default_model, model_name, model_type,
    loan_value_col='loan_amount_approved', grid_search=False
):
    """
    Evaluates the model on test data, calculates metrics, updates the dataset with probabilities, 
    and calculates expected revenue and confidence.

    Parameters:
        model: Trained machine learning model.
        x_test (pd.DataFrame): Test set features.
        y_test (pd.Series): Test set labels.
        X_scaled (pd.DataFrame): Scaled dataset used for probability predictions.
        default_model (pd.DataFrame): Original dataset to update with probabilities.
        model_name (str): Name of the model for tracking.
        model_type (str): Type of the model (e.g., Logistic Regression, Random Forest).
        loan_value_col (str): Name of the column representing loan values (default: 'loan_amount_approved').
        grid_search (bool): Whether the model was trained using GridSearchCV. Default is False.

    Returns:
        dict: A dictionary containing evaluation metrics for appending to the model_metrics DataFrame.
    """
    # Make predictions and calculate metrics
    y_pred = model.predict(x_test)
    metrics_score(y_test, y_pred)  # Display classification report and confusion matrix

    # Update probabilities in the dataset
    default_model_copy = default_model.copy()
    pred_prob = model.predict_proba(X_scaled)
    default_model_copy['prob_default'] = pred_prob[:, 1]

    # Calculate expected revenue and confidence
    revenue, confidence = calculate_expected_with_confidence(default_model_copy, loan_value_col)
    print(f"Total expected revenue: {revenue} with confidence of: {confidence}")

    # Generate classification report and confusion matrix
    cr = classification_report(y_test, y_pred, output_dict=True)
    cm = confusion_matrix(y_test, y_pred)

    # Compile metrics into a dictionary
    metrics_dict = {
        'Model Name': model_name,
        'Model Type': model_type,
        'Grid Search (Yes/No)': 'Yes' if grid_search else 'No',
        'Recall': cr['1']['recall'],
        'Precision': cr['1']['precision'],
        'f1': cr['1']['f1-score'],
        'Accuracy': cr['accuracy'],
        'FPs': cm[0][1],
        'FNs': cm[1][0],
        'Revenue Prediction': revenue,
        'Confidence': confidence,
    }

    return metrics_dict

# Data Visualization Functions
def plot_grouped_barplots(data, features, target, figsize=(15, 6)):
    """
    Plots grouped barplots for a list of features against a target variable.
    """
    n_cols = 3
    n_rows = math.ceil(len(features) / n_cols)
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(figsize[0], figsize[1] * n_rows))
    axes = axes.flatten()
    
    for i, feature in enumerate(features):
        grouped_data = data.groupby(feature, observed=True)[target].sum().reset_index()
        sns.barplot(x=feature, y=target, data=grouped_data, ax=axes[i], color='green')
        axes[i].set_title(f"{target} by {feature}")
        axes[i].tick_params(axis="x", rotation=90)
    
    for j in range(len(features), len(axes)):
        axes[j].axis("off")
    
    plt.tight_layout()
    plt.show()

def plot_histogram(data, column, bins=None, xlabel=None, ylabel='Count', title=None, grid=True):
    """
    Plots a histogram for a given column with customizable bins and styling.

    Parameters:
    - data (pd.DataFrame): The DataFrame containing the data.
    - column (str): The name of the column to plot.
    - bins (int, list, or np.ndarray, optional): Custom bins for the histogram. Default is None.
    - xlabel (str, optional): Label for the x-axis. Default is the column name.
    - ylabel (str, optional): Label for the y-axis. Default is 'Count'.
    - title (str, optional): Title of the histogram. Default is None.
    - grid (bool, optional): Whether to show gridlines. Default is True.
    """
    plt.figure(figsize=(10, 6))
    plt.hist(data[column], bins=bins, color='green', edgecolor='black', alpha=0.7)

    # Set axis labels and title
    plt.xlabel(xlabel if xlabel else column)
    plt.ylabel(ylabel)
    plt.title(title if title else f'Histogram of {column}')

    # Add grid styling
    if grid:
        plt.grid(axis='y', linestyle='--', alpha=0.7)

    plt.show()

def histogram_boxplot(data, feature, figsize=(12, 7), kde=True, bins=None):
    """
    Combines a boxplot and histogram for a given feature in the dataset.

    Parameters:
    - data (pd.DataFrame): The input dataset.
    - feature (str): The feature to plot.
    - figsize (tuple): Size of the figure (default: (12, 7)).
    - kde (bool): Whether to show a kernel density estimate on the histogram (default: True).
    - bins (int, list, or np.ndarray): Number of bins or edges for the histogram (default: None).
    """
    # Create the subplots for boxplot and histogram
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,         # Number of rows in the subplot grid
        sharex=True,     # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize, # Figure size
    )

    # Boxplot with the mean indicated
    sns.boxplot(data=data, x=feature, ax=ax_box2, showmeans=True, color="red")
    ax_box2.set_title(f"Boxplot of {feature}")

    # Automatically calculate bins if not provided
    if bins is None:
        q75, q25 = data[feature].quantile([0.75, 0.25])  # Interquartile range
        iqr = q75 - q25
        bin_width = 2 * iqr / (len(data[feature]) ** (1 / 3))  # Freedman-Diaconis rule
        bin_width = max(bin_width, 1)  # Ensure minimum bin width of 1
        bins = int((data[feature].max() - data[feature].min()) / bin_width)
        bins = max(bins, 1)  # Ensure at least 1 bin

    # Histogram
    sns.histplot(data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, color="green", alpha=0.6)
    ax_hist2.set_title(f"Histogram of {feature}")
    ax_hist2.set_xlabel(feature)
    ax_hist2.set_ylabel("Count")

    # Add mean and median lines to the histogram
    mean = data[feature].mean()
    median = data[feature].median()
    ax_hist2.axvline(mean, color="green", linestyle="--", label=f"Mean: {mean:.2f}")
    ax_hist2.axvline(median, color="black", linestyle="-", label=f"Median: {median:.2f}")
    ax_hist2.legend()

    # Ensure the x-axis starts at 0 (optional, adjust as needed)
    ax_hist2.set_xlim(left=0)

    plt.tight_layout()
    plt.show()

def plot_categorical_distributions(data, categorical_columns, cols_per_row=3, figsize=(15, 5)):
    """
    Plots the distribution of categorical columns in the given dataset.

    Parameters:
    - data (pd.DataFrame): The dataset containing categorical columns.
    - categorical_columns (list): List of categorical column names to plot.
    - cols_per_row (int): Number of subplots per row. Default is 3.
    - figsize (tuple): Base size of the figure. Default is (15, 5).

    Returns:
    - None: Displays the plots.
    """
    # Determine the number of rows and columns for subplots
    num_cols = len(categorical_columns)
    num_rows = math.ceil(num_cols / cols_per_row)

    # Create subplots
    fig, axes = plt.subplots(num_rows, cols_per_row, figsize=(figsize[0], figsize[1] * num_rows))
    axes = axes.flatten()  # Flatten the axes array for easy iteration

    # Plot each categorical column
    for i, col in enumerate(categorical_columns):
        sns.histplot(data, x=col, ax=axes[i], color='green')  # Use the corresponding subplot axis
        axes[i].set_title(f"Histogram for {col}")
        total = len(data[col])

        # Add percentage annotations
        for bar in axes[i].patches:
            height = bar.get_height()
            percentage = (height / total) * 100  # Calculate percentage
            axes[i].text(bar.get_x() + bar.get_width() / 2, height, f'{percentage:.1f}%', 
                         ha='center', va='bottom', fontsize=10)
        sns.despine()

    # Hide unused subplots
    for j in range(i + 1, len(axes)):
        axes[j].set_visible(False)

    # Adjust layout and display the plots
    plt.tight_layout()
    plt.show()

def plot_binned_features(data, features, status, bins=5, title=None):
    """
    Plots subplots for binned bar charts of multiple features against a status variable.

    Parameters:
        data (pd.DataFrame): The dataset containing the features and status.
        features (list): A list of continuous variables to be binned and plotted.
        status (str): The variable to sum for each bin, plotted on the y-axis.
        bins (int): Number of bins to create for the features.
        title (str, optional): Title of the overall plot. Default is None.

    Returns:
        None: Displays the subplots.
    """
    # Calculate the grid dimensions
    n_rows, n_cols = 6, 2  # Fixed number of rows and columns
    total_subplots = n_rows * n_cols
    num_features = len(features)

    # Create a figure with subplots
    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))
    fig.suptitle(title if title else f"Features vs {status}", fontsize=16)

    # Flatten axes for easy indexing
    axes = axes.flatten()

    # Iterate over features and create plots
    for i, feature in enumerate(features):
        ax = axes[i]

        # Group by feature and calculate the sum of the status variable
        grouped_data = data.groupby(feature)[status].sum().reset_index()

        # Create bins for the feature
        grouped_data['feature_bin'] = pd.cut(grouped_data[feature], bins=bins)

        # Group by bins and calculate the sum of the status variable
        binned_data = grouped_data.groupby('feature_bin', observed=True)[status].sum().reset_index()

        # Generate bin labels
        bin_edges = grouped_data['feature_bin'].cat.categories
        bin_labels = [f"{int(interval.left)}-{int(interval.right)}" for interval in bin_edges]

        # Bar plot
        sns.barplot(data=binned_data, x='feature_bin', y=status, ax=ax, color='green')

        # Add title and labels
        ax.set_title(f"{feature} vs {status}")
        ax.set_xlabel(f"{feature}")
        ax.set_ylabel(f"{status}")

        # Set x-axis tick labels
        ax.set_xticks(range(len(bin_labels)))
        ax.set_xticklabels(bin_labels, rotation=90)
        total = grouped_data[status].sum()
        for bar in ax.patches:
            height = bar.get_height()
            percentage = (height / total) * 100  # Calculate percentage
            if percentage > 0:
                ax.text(bar.get_x() + bar.get_width() / 2, height, f'{percentage:.1f}%', ha='center', va='bottom', fontsize=10)
        sns.despine()

    # Turn off extra subplots
    for i in range(num_features, total_subplots):
        axes[i].axis('off')

    # Adjust layout and show the plot
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

def plot_feature_importances_or_coefficients(model, X, model_type="general"):
    """
    Plots feature importances or coefficients for a given model, 
    accounting for positive and negative coefficients in logistic regression.

    Parameters:
        model: Trained machine learning model.
        X (pd.DataFrame): Feature DataFrame used for training the model.
        model_type (str): Specify "logistic_regression" for logistic regression models 
                          or "general" for models with feature_importances_.

    Returns:
        None: Displays a bar chart of feature importances or coefficients.
    """
    # Retrieve importances or coefficients based on the model type
    if model_type == "logistic_regression":
        if hasattr(model, "coef_"):
            importances = model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_
            title = "Feature Coefficients (Logistic Regression)"
        else:
            raise ValueError("The provided model does not support coefficients.")
    elif hasattr(model, "feature_importances_"):
        importances = model.feature_importances_
        title = "Feature Importances"
    else:
        raise ValueError("The provided model does not support feature importances or coefficients.")

    # Create DataFrame for sorting and plotting
    importance_df = pd.DataFrame({
        'Feature': X.columns,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False, key=abs if model_type == "logistic_regression" else None)

    # Plotting
    plt.figure(figsize=(12, 6))
    colors = ['green' if val > 0 else 'red' for val in importance_df['Importance']]  # Positive/negative colors
    plt.bar(importance_df['Feature'], importance_df['Importance'], color=colors, align='center')
    plt.xticks(rotation=45, ha='right')
    plt.xlabel('Features')
    plt.ylabel('Importance' if model_type != "logistic_regression" else 'Coefficient')
    plt.title(title)
    plt.axhline(0, color='black', linewidth=0.7, linestyle='--')  # Zero line for logistic regression
    plt.tight_layout()
    plt.show()


def analyze_subset_features(model, x_test, y_test, subset='false_negatives'):
    """
    Analyzes feature values for a specific subset of classification results (e.g., false negatives or false positives),
    and aligns the color palette with the feature importances function.

    Parameters:
        model: Trained classification model.
        x_test (pd.DataFrame): Test set features.
        y_test (pd.Series): Test set labels.
        subset (str): Subset to analyze. Options:
                      - 'false_negatives': Predicted 0, Actual 1
                      - 'false_positives': Predicted 1, Actual 0

    Returns:
        None: Displays a bar chart of mean feature values for the specified subset.
    """
    # Predict probabilities and labels
    y_pred = model.predict(x_test)

    # Compute confusion matrix
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

    # Filter data based on the selected subset
    if subset == 'false_negatives':
        subset_df = x_test[(y_pred == 0) & (y_test == 1)]
        title = "Mean Feature Values for False Negatives"
    elif subset == 'false_positives':
        subset_df = x_test[(y_pred == 1) & (y_test == 0)]
        title = "Mean Feature Values for False Positives"
    else:
        raise ValueError("Invalid subset option. Choose 'false_negatives' or 'false_positives'.")

    # Calculate mean feature values
    subset_features = subset_df.mean().sort_values(ascending=False)

    # Apply colors based on positive/negative values
    colors = ['green' if val > 0 else 'red' for val in subset_features.values]

    # Plot mean feature values
    plt.figure(figsize=(10, 6))
    plt.bar(subset_features.index, subset_features.values, color=colors, align='center')
    plt.title(title, fontsize=14)
    plt.xlabel('Features', fontsize=12)
    plt.ylabel('Mean Value', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.axhline(0, color='black', linewidth=0.7, linestyle='--')  # Zero line for reference
    plt.grid(axis='y', linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.show()




def chi_squared_test(data, target, bins=5):
    """
    Perform the Chi-Squared test for independence between all features
    and the target variable, handling both categorical and continuous features.

    Parameters:
        data (pd.DataFrame): The input dataset.
        target (str): The name of the target variable.
        bins (int): Number of bins to discretize continuous features. Default is 5.

    Returns:
        pd.DataFrame: A DataFrame containing features, their P-values, 
                      and whether they are significant.
    """
    # Separate target column
    target_data = data[target]
    features = data.drop(columns=[target], errors="ignore")

    # List to store results for each feature
    results = []

    for feature in features.columns:
        if features[feature].dtype in ['object', 'category']:
            # Categorical feature: Use directly
            feature_data = features[feature]
        else:
            # Continuous feature: Discretize into bins
            feature_data = pd.cut(features[feature], bins=bins, labels=False)

        # Create a contingency table for the current feature and target
        contingency_table = pd.crosstab(feature_data, target_data)

        # Perform the Chi-Squared test for independence
        chi2, p, dof, expected = chi2_contingency(contingency_table)

        # Append the feature and p-value to the results list
        results.append({'Feature': feature, 'P-value': p, 'Significant': p < 0.05})

    # Convert results to a DataFrame for better visualization
    results_df = pd.DataFrame(results)

    # Sort the results by P-value in ascending order
    sorted_results = results_df.sort_values(by='P-value', ascending=True)

    return sorted_results





# Import data from CSV file into a Pandas DataFrame
df = pd.read_csv('../data/hmeq.csv')


# Make a copy of the original dataframe for processing, so the original data stays intact
default = df.copy()


# Update column names for improved readability
default.rename(columns={
    'BAD': 'client_defaulted_on_loan',  # 1 = Client defaulted on loan, 0 = loan repaid
    'LOAN': 'loan_amount_approved',      # Amount of loan approved
    'MORTDUE': 'mortgage_amount_due',    # Amount due on the existing mortgage
    'VALUE': 'property_current_value',    # Current value of the property
    'REASON': 'loan_request_reason',      # Reason for the loan request
    'JOB': 'applicant_job_type',          # Type of job that loan applicant has
    'YOJ': 'years_at_present_job',        # Years at present job
    'DEROG': 'major_derogatory_reports',  # Number of major derogatory reports
    'DELINQ': 'delinquent_credit_lines',  # Number of delinquent credit lines
    'CLAGE': 'age_of_oldest_credit_line',  # Age of the oldest credit line in months
    'NINQ': 'recent_credit_inquiries',     # Number of recent credit inquiries
    'CLNO': 'existing_credit_lines',       # Number of existing credit lines
    'DEBTINC': 'debt_to_income_ratio'      # Debt-to-income ratio
}, inplace=True)


# Check the shape of the dataset
default.shape


# Use the summarize_dataframe function and display the summary
summarize_dataframe(default)


# Check for duplicates and drop if found
num_duplicates = default.duplicated().sum()  # Count duplicates

if num_duplicates > 0:
    print(f"Number of duplicate rows: {num_duplicates}")
    default.drop_duplicates(keep='first', inplace=True)  # Drop duplicates, keeping the first
    print(len(df) - len(default), "Duplicate rows dropped (keeping the first occurrence).")
    default.reset_index(drop=True, inplace=True)  # Reset index after dropping duplicates
else:
    print("No duplicate rows found.")


# Iterate through each non-numeric column and print its unique values
for column in default.select_dtypes(include=['object']):
    print(f"{column}: {default[column].value_counts()}")





# Check for inconsisties in the data
# Typical mortgage to property value ratio is ~ 0.8. Lets confirm it
default['ltv'] = default['mortgage_amount_due'] / default['property_current_value']  # create a column ltv (loan to value ratio)
high_ltv = default[default['ltv'] >= 2]  # Create a temp dataframe of rows with ltv > 2

# Define bins as integer steps from the min to max value
bins = np.arange(int(high_ltv['ltv'].min()), int(high_ltv['ltv'].max()) + 2) - 0.5

# Plot the count of clients with ltv greater than 2.
plot_histogram(
    data=high_ltv,
    column='ltv',
    bins=bins,
    xlabel='LTV Value',
    ylabel='Count',
    title='Histogram of LTV with Integer Bins'
)


# Drop all records with ltv greater than or equal to 2 and the ltv column as it is no longer necessary
default.drop(default[default['ltv'] >= 2].index, axis=0, inplace=True)
default.drop('ltv', axis=1, inplace=True)


# delete the temporary dataframe
del high_ltv
# Trigger garbage collection and print the number of objects collected
collected_objects = gc.collect()
print(f"Garbage collector freed {collected_objects} objects.")








# Review the key statistics of the data, transposed for better readability
default.describe().T




















# Columns for KNN imputation
columns_knn = [
    'mortgage_amount_due', 'property_current_value',
    'loan_request_reason', 'applicant_job_type',
    'age_of_oldest_credit_line', 'existing_credit_lines'
]
# Columns for iterative imputation
columns_iterative = [
        'debt_to_income_ratio', 'major_derogatory_reports',
        'delinquent_credit_lines', 'years_at_present_job',
        'recent_credit_inquiries'
    ]
# Categorical Columns
categorical_columns = ['loan_request_reason', 'applicant_job_type']


# Treat missing values using the custom function 'impute_missing_values', which implements the KNN and Iterative imputation based on % missing values for each column
default_imputed_num, categories = impute_missing_values(default, columns_knn, columns_iterative, categorical_columns)

# Upon inspection of the dataset after imputation, it is observed that in some of the rows, property value and mortgage due were imputed to 0. These zero values were set to the median value of the column
for col in ['property_current_value', 'mortgage_amount_due']:
    median_value = default_imputed_num[col].median()
    default_imputed_num.loc[default_imputed_num[col] == 0, col] = median_value


# Create a copy of the imputed data in numeric format (could be of use later!)
default_imputed = default_imputed_num.copy()

# Convert categorical columns back to original categories by using the saved original categories
for col in ['loan_request_reason', 'applicant_job_type']:
    default_imputed[col] = pd.Categorical.from_codes(default_imputed_num[col].round().astype(int), categories=categories[col])
# Confirm no more missing values
summarize_dataframe(default_imputed)





# Create list of columns by dtype(s)
col_types = segregate_columns_by_dtype(default_imputed)

# get numeric columns from the column type list
num_cols = col_types['float64'] + col_types['int64']

# Remove target column
num_cols.remove('client_defaulted_on_loan')

# get categorical columns from the column type list
cat_cols = col_types['category']





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[0])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[1])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[2])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[3])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[4])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[5])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[6])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[7])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[8])





# Plot the numeric variables as histograms and box plots to check skewness and outliers
histogram_boxplot(default_imputed, num_cols[9])





# Plot distribution of categorical columns
plot_categorical_distributions(data=default_imputed, categorical_columns=categorical_columns, cols_per_row=3)











# Calculate the correlation matrix
correlation_matrix = default_imputed[num_cols].corr()
# Plot the heatmap
plt.figure(figsize=(16, 8))  # Adjust figure size if needed
sns.heatmap(correlation_matrix, annot=True, cmap='rainbow_r', fmt=".2f", vmin=-1, vmax=1)
plt.title('Correlation Heatmap')
plt.show()


# Sort correlations and exclude duplicates
sorted_unique_correlations = (
    correlation_matrix
    .stack()                            # Flatten the matrix into a Series
    .reset_index()                      # Reset index to access column pairs
    .rename(columns={0: 'correlation'})  # Rename correlation column
    .query("level_0 < level_1")         # Keep only one direction (unique pairs)
    .sort_values(by='correlation', ascending=False)  # Sort by correlation
)
# Print the columns with correlation values
print(sorted_unique_correlations)








plot_binned_features(default_imputed, num_cols, 'client_defaulted_on_loan', 10)








# Plot the categorical variable against the target variable
plot_grouped_barplots(default_imputed, cat_cols, 'client_defaulted_on_loan')





# Perform the Chi-squared test to see if any of the features are not significant to predicting the target variable
print(chi_squared_test(default_imputed, 'client_defaulted_on_loan'))





# Calculate correlation matrix and VIF, excluding the target variable
corr_matrix, vif_values = calculate_corr_and_vif(default_imputed, 'client_defaulted_on_loan')

# Display results
print("Correlation Matrix:")

sorted_corr_matrix = corr_matrix[corr_matrix.columns[corr_matrix.iloc[0].argsort()[::-1]]]

sorted_corr_matrix


# Print VIF values
print(vif_values)














# Standardize the data using the dataset with all numeric values from the impute missing values function output (PCA works only with numeric values)
default_imputed_scaled = default_imputed_num.drop('client_defaulted_on_loan', axis=1) # Drop the target variable
 # Scale the data using Z transformation
scaler = StandardScaler()
default_imputed_scaled = pd.DataFrame(scaler.fit_transform(default_imputed_scaled), columns=default_imputed_scaled.columns)


# Apply PCA
pca = PCA(n_components=10)  # Reduce to 10 principal components
default_pca = pd.DataFrame(pca.fit_transform(default_imputed_scaled), columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])


# Plot the cumulative variance explained by each principle component for 
plot_pca(pca)
# Print explained variance by each component
for i, ratio in enumerate(pca.explained_variance_ratio_, start=1):
    print(f"PC{i}: {ratio:.2%}")








# Create a DataFrame to represent the principal component loadings
# Rows are the principal components (PCs), and columns are the original features
loadings = pd.DataFrame(
    pca.components_,  # PCA component weights
    columns=default_imputed_scaled.columns,  # Original feature names
    index=[f'PC{i+1}' for i in range(len(pca.components_))]  # Label PCs as PC1, PC2, etc.
)

# Sort the loadings of the first principal component (PC1) in descending order
# loadings.iloc[0] refers to the first row (PC1), and argsort()[::-1] sorts indices in descending order
sorted_loadings = loadings[loadings.columns[loadings.iloc[0].argsort()[::-1]]]

# Display the sorted loadings
sorted_loadings





default_imputed_scaled_eng = engineer_and_scale_features(
    data=default_imputed_scaled,
    ratio_cols=('mortgage_amount_due', 'property_current_value'),
    new_feature_name='ltv',
    drop_original=True
)


# Apply PCA
pca_nopv = PCA(n_components=10)  # First 10 PCs
default_pcanopv = pd.DataFrame(pca_nopv.fit_transform(default_imputed_scaled_eng), columns=['PCe1', 'PCe2', 'PCe3', 'PCe4', 'PCe5', 'PCe6', 'PCe7', 'PCe8', 'PCe9', 'PCe10'])


# Plot explained variance ratio
plot_pca(pca_nopv)

# Print explained variance by each component
for i, ratio in enumerate(pca_nopv.explained_variance_ratio_, start=1):
    print(f"PCe{i}: {ratio:.2%}")


loadings_noPV = pd.DataFrame(pca_nopv.components_, columns=default_imputed_scaled_eng.columns, index=[f'PCe{i+1}' for i in range(len(pca_nopv.components_))])
sorted_loadings_noPV = loadings_noPV[loadings_noPV.columns[loadings_noPV.iloc[0].argsort()[::-1]]]
sorted_loadings_noPV

















# Create a dataframe for modeling from original imputed dataframe
default_model = copy.deepcopy(default_imputed)
default_model['ltv'] = default_model['mortgage_amount_due']/default_model['property_current_value']  # Create the new ltv feature


default_model.drop(columns=['mortgage_amount_due', 'property_current_value'], inplace=True)  # Drop the original feature used to create ltv


default_model = engineer_and_scale_features(
    data=default_imputed,
    ratio_cols=('mortgage_amount_due', 'property_current_value'),
    new_feature_name='ltv',
    drop_original=True
)


# Transform the categorical variables using one-hot encoding
default_model = pd.get_dummies(default_model, columns=['loan_request_reason', 'applicant_job_type'])


# Separate the target variable
Y = default_model.client_defaulted_on_loan
X = default_model.drop('client_defaulted_on_loan', axis=1)


Y.value_counts()


# Scale the data
sc = StandardScaler()
X_scaled = sc.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns = X.columns)


# Create the training anb testing datasets
x_train, x_test, y_train, y_test = train_test_split(X_scaled, Y, test_size = 0.3, random_state = 1, stratify = Y)
# Reset index of test set for alignment
x_test = x_test.reset_index(drop=True)
y_test = y_test.reset_index(drop=True)


# Define scoring metrics to be optimized during GridSearch
optimum = ['accuracy', 'f1', 'recall', 'precision']











model_metrics = pd.DataFrame(columns = ['Model Name', 'Model Type', 'Grid Search (Yes/No)', 'Recall', 'Precision', 'f1', 'Accuracy', 'FPs', 'FNs', 'Revenue Prediction', 'Confidence'])
model_metrics





# Fit logistic regression model to training data with default parameters except solver = liblinear, since liblinear solver is optimized for linear problems with small datasets and binary classification
lg = LogisticRegression(verbose=1, solver='liblinear', random_state=1)
lg.fit(x_train, y_train)


# Check the performance on the training data
y_pred_train = lg.predict(x_train)
metrics_score(y_train, y_pred_train)


# Checking the performance on the test data
lg_metric_dict = evaluate_model_and_metrics(lg, x_test, y_test, X_scaled, default_model, 'lg', 'Logistic Regression')


model_metrics.loc[len(model_metrics)] = lg_metric_dict





plot_feature_importances_or_coefficients(lg, X, 'logistic_regression')





analyze_subset_features(
    model=lg,
    x_test=x_test,
    y_test=y_test,
    subset='false_negatives'
)





# Define the model
log_reg = LogisticRegression(solver='liblinear', random_state=1)

# Define the parameter grid to search over
param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization techniques
    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength
    'solver': ['lbfgs', 'liblinear', 'saga'],  # Solvers compatible with l1 and elasticnet penalties
    'max_iter': [500, 1000, 2000, 3000]  # Number of iterations for convergence
}

results = {}  # Dictionary to store all results dynamically

# Perform GridSearchCV for each scoring metric
for opt in optimum:
    print(f"Optimizing for: {opt}")

    # Set up GridSearchCV
    grid_search = GridSearchCV(
        estimator=log_reg,
        param_grid=param_grid,
        scoring=opt,
        cv=5,
        verbose=1,
        n_jobs=-1
    )

    # Perform the grid search on the training data
    grid_search.fit(x_train, y_train)

    # Store best parameters and cross-validation score
    results[opt] = {
        'best_params': grid_search.best_params_,
        'best_score': grid_search.best_score_,
        'best_model': grid_search.best_estimator_
    }

    print(f"Best Parameters for {opt}: {results[opt]['best_params']}")
    print(f"Best Cross-Validation Score for {opt}: {results[opt]['best_score']}")

    # Evaluate the best model and append results
    model_metrics.loc[len(model_metrics)] = evaluate_model_and_metrics(results[opt]['best_model'], x_test, y_test, X_scaled, default_model, f'log_reg_{opt}', 'Logistic Regression', grid_search=True)









# Building decision tree model
dt = DecisionTreeClassifier(criterion='entropy', random_state=1)


# Fitting decision tree model
dt.fit(x_train, y_train)


# Check the performance on the training data
y_pred_train = lg.predict(x_train)
metrics_score(y_train, y_pred_train)


# Checking the performance on the test data
model_metrics.loc[len(model_metrics)] = evaluate_model_and_metrics(dt, x_test, y_test, X_scaled, default_model, 'dt', 'Decision Tree')


model_metrics


plot_feature_importances_or_coefficients(dt, X)


analyze_subset_features(
    model=dt,
    x_test=x_test,
    y_test=y_test,
    subset='false_negatives'
)








# Define the Decision Tree model
dtree_estimator_grid = DecisionTreeClassifier(random_state=1)

# Define the parameter grid to search over
parameters = {
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'max_features': ['sqrt', 'log2', 0.5],
    'criterion': ['gini', 'entropy']
}

# Initialize results dictionary and DataFrame
results = {}

# Perform GridSearchCV for each scoring metric
for opt in optimum:
    print(f"Optimizing for: {opt}")

    # Set up GridSearchCV
    grid_search = GridSearchCV(
        estimator=dtree_estimator_grid,
        param_grid=parameters,
        scoring=opt,
        cv=5,
        n_jobs=-1,
        verbose=1
    )

    # Perform the grid search on the training data
    grid_search.fit(x_train, y_train)

    # Store best parameters and cross-validation score
    results[opt] = {
        'best_params': grid_search.best_params_,
        'best_score': grid_search.best_score_,
        'best_model': grid_search.best_estimator_
    }

    print(f"Best Parameters for {opt}: {results[opt]['best_params']}")
    print(f"Best Cross-Validation Score for {opt}: {results[opt]['best_score']}")

    # Evaluate the best model and append results
    model_metrics.loc[len(model_metrics)] = evaluate_model_and_metrics(results[opt]['best_model'], x_test, y_test, X_scaled, default_model, f'dtree_estimator_grid_{opt}', 'Decision Tree', grid_search=True)









# Fitting the Random Forest classifier on the training data
rf = RandomForestClassifier(criterion='entropy', random_state=1)

rf.fit(x_train, y_train)


# Checking performance on the training data
y_pred_train_rf = rf.predict(x_train)

metrics_score(y_train, y_pred_train_rf)


# Checking the performance on the test data
model_metrics.loc[len(model_metrics)] = evaluate_model_and_metrics(rf, x_test, y_test, X_scaled, default_model, 'rf', 'Random Forest')


plot_feature_importances_or_coefficients(rf, X)


analyze_subset_features(
    model=rf,
    x_test=x_test,
    y_test=y_test,
    subset='false_negatives'
)











# Define the Random Forest model
rf_estimator_grid = RandomForestClassifier(random_state=1)

# Define the parameter grid to search over
param_grid = {
    'criterion': ['gini', 'entropy'],
    'n_estimators': [100, 200, 500],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'max_features': ['sqrt', 'log2', 0.5],
    'bootstrap': [True, False]
}

# Initialize results dictionary and DataFrame
results = {}

# Perform GridSearchCV for each scoring metric
for opt in optimum:
    print(f"Optimizing for: {opt}")

    # Set up GridSearchCV
    grid_search = GridSearchCV(
        estimator=rf_estimator_grid,
        param_grid=param_grid,
        scoring=opt,
        cv=5,
        n_jobs=-1,
        verbose=1
    )

    # Perform the grid search on the training data
    grid_search.fit(x_train, y_train)

    # Store best parameters and cross-validation score
    results[opt] = {
        'best_params': grid_search.best_params_,
        'best_score': grid_search.best_score_,
        'best_model': grid_search.best_estimator_
    }

    print(f"Best Parameters for {opt}: {results[opt]['best_params']}")
    print(f"Best Cross-Validation Score for {opt}: {results[opt]['best_score']}")

    # Evaluate the best model and append results
    model_metrics.loc[len(model_metrics)] = evaluate_model_and_metrics(results[opt]['best_model'], x_test, y_test, X_scaled, default_model, f'rf_estimator_grid_{opt}', 'Random Forest', grid_search=True)



# Trigger garbage collection and print the number of objects collected
collected_objects = gc.collect()
print(f"Garbage collector freed {collected_objects} objects.")








model_metrics














# Recreate the best model - rf_estimator_grid_recall, with the same parameters from gridsearch

# Define the model
rf_estimator_best = RandomForestClassifier(
    bootstrap=False,
    criterion='gini',
    max_depth=None,
    max_features='sqrt',
    min_samples_leaf=1,
    min_samples_split=5,
    n_estimators=100,
    random_state=1,
    n_jobs=-1
)

# Perform cross-validation
cv_scores = cross_val_score(rf_estimator_best, x_train, y_train, cv=5, scoring='recall')
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", cv_scores.mean())

# Fit the model on the entire training dataset
rf_estimator_best.fit(x_train, y_train)

# Make predictions on the test set
y_tuned = rf_estimator_best.predict(x_test)

# Evaluate the predictions
recall = recall_score(y_test, y_tuned)
print("Recall Score on Test Set:", recall)
metrics_score(y_test, y_tuned)


# Predict probabilities
y_probs_best = rf_estimator_best.predict_proba(x_test)[:, 1]


# Compute precision-recall curve
precision, recall, thresholds = precision_recall_curve(y_test, y_probs_best)

# Plot values of precisions, recalls, and thresholds
plt.figure(figsize = (10, 7))
plt.plot(thresholds, precision[:-1], 'r--', label = 'precision')
plt.plot(thresholds, recall[:-1], 'g--', label = 'recall')
plt.xlabel('Threshold')
plt.legend(loc = 'upper left')
plt.ylim([0, 1])
plt.show()


# Adjust the threshold
threshold = 0.4  # based on the precision recall curve above

pred_prob_rf_best = rf_estimator_best.predict_proba(X_scaled)
pred_prob_rf_thrsh = (pred_prob_rf_best >= threshold).astype(int)
default_model_best = default_model.copy()
default_model_best['prob_default'] = pred_prob_rf_thrsh[:, 1]
revenue_best, confidencef_best = calculate_expected_with_confidence(default_model_best, 'loan_amount_approved')
print(revenue_best, confidencef_best)

y_pred_threshold = (y_probs_best >= threshold).astype(int)
metrics_score(y_test, y_pred_threshold)


plot_feature_importances_or_coefficients(rf_estimator_best, X)


analyze_subset_features(rf_estimator_best, x_test, y_test)








import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import recall_score

# Define the recall thresholds
recall_thresholds = np.arange(0.3, 0.6, 0.1)  # 0.3 to 0.5 with a step of 0.1

# Initialize lists to store results
recall_values = []
revenue_values = []

# Iterate over the thresholds
for threshold in recall_thresholds:
    # Predict probabilities on x_test and apply threshold
    pred_prob_rf_best = rf_estimator_best.predict_proba(x_test)
    y_pred_threshold = (pred_prob_rf_best[:, 1] >= threshold).astype(int)

    # Calculate recall
    recall = recall_score(y_test, y_pred_threshold)
    recall_values.append(recall)

    # Update the default_model dataset with threshold-based predictions
    default_model_best = default_model.copy()
    pred_prob_scaled = rf_estimator_best.predict_proba(X_scaled)
    default_model_best['prob_default'] = (pred_prob_scaled[:, 1] >= threshold).astype(int)

    # Calculate expected revenue
    revenue_best, _ = calculate_expected_with_confidence(default_model_best, 'loan_amount_approved')
    revenue_values.append(revenue_best)

    print(f"Threshold: {threshold}, Recall: {recall:.3f}, Revenue: {revenue_best:.2f}")

# Create subplots
fig, axes = plt.subplots(2, 1, figsize=(10, 10), sharex=True)

# Plot Recall
axes[0].plot(recall_thresholds, recall_values, marker='o', label='Recall', color='blue')
axes[0].set_title('Recall vs Threshold', fontsize=14)
axes[0].set_ylabel('Recall', fontsize=12)
axes[0].grid(axis='y', linestyle='--', alpha=0.6)
axes[0].legend()

# Plot Revenue
axes[1].plot(recall_thresholds, revenue_values, marker='s', label='Revenue', color='green')
axes[1].set_title('Revenue vs Threshold', fontsize=14)
axes[1].set_xlabel('Threshold', fontsize=12)
axes[1].set_ylabel('Revenue', fontsize=12)
axes[1].grid(axis='y', linestyle='--', alpha=0.6)
axes[1].legend()

# Adjust layout
plt.tight_layout()
plt.show()








# Importing the XGBClassifier from the xgboost library
from xgboost import XGBClassifier

# XGBoost Classifier
xgb = XGBClassifier(random_state = 1, eval_metric = 'logloss')

# Fitting the model
xgb.fit(x_train,y_train)

# Model Performance on the test data
xgb_perf_test = model_performance_classification(xgb,x_test,y_test)

print(xgb_perf_test)

# Predict probabilities for revenue prediction
pred_prob_xgb_best = xgb.predict_proba(X_scaled)
pred_prob_xgb_thrsh = (pred_prob_xgb_best >= 0.5).astype(int)

# Copy default model and add probabilities
default_model_best_xgb = default_model.copy()
default_model_best_xgb['prob_default'] = pred_prob_xgb_thrsh[:, 1]

# Calculate expected revenue and confidence
revenue_best_xgb, confidence_best_xgb = calculate_expected_with_confidence(
    default_model_best_xgb, 'loan_amount_approved'
)
print("Revenue (XGBoost Best Model):", revenue_best_xgb)
print("Confidence (XGBoost Best Model):", confidence_best_xgb)

metrics_score(y_test, xgb.predict(x_test))


# Predict probabilities
y_probs_xgb = xgb.predict_proba(x_test)[:, 1]

# Compute precision-recall curve
precision, recall, thresholds = precision_recall_curve(y_test, y_probs_xgb)

# Plot values of precisions, recalls, and thresholds
plt.figure(figsize = (10, 7))
plt.plot(thresholds, precision[:-1], 'r--', label = 'precision')
plt.plot(thresholds, recall[:-1], 'g--', label = 'recall')
plt.xlabel('Threshold')
plt.legend(loc = 'upper left')
plt.ylim([0, 1])
plt.show()


# Adjust the threshold
threshold = 0.5  # based on the precision recall curve above

pred_prob_xgb = xgb.predict_proba(X_scaled)
pred_prob_xgb_thrsh = (pred_prob_xgb_best >= threshold).astype(int)
default_model_best = default_model.copy()
default_model_best['prob_default'] = pred_prob_xgb_thrsh[:, 1]
revenue_best, confidencef_best = calculate_expected_with_confidence(default_model_best, 'loan_amount_approved')
print(revenue_best, confidencef_best)

y_pred_threshold_xgb = (y_probs_xgb >= threshold).astype(int)
metrics_score(y_test, y_pred_threshold_xgb)


plot_feature_importances_or_coefficients(xgb, X)


analyze_subset_features(
    model=xgb,
    x_test=x_test,
    y_test=y_test,
    subset='false_negatives'
)


# # Define the model
# xgb_grid = XGBClassifier(objective='binary:logistic', random_state=42, use_label_encoder=False, eval_metric='logloss')

# # Define the parameter grid
# param_grid = {
#     'n_estimators': [50, 100, 200],  # Number of boosting rounds
#     'learning_rate': [0.01, 0.1, 0.2],  # Step size shrinkage
#     'max_depth': [3, 5, 7],  # Maximum tree depth
#     'subsample': [0.8, 1.0],  # Subsampling ratio
#     'colsample_bytree': [0.8, 1.0],  # Subsampling ratio of columns
#     'gamma': [0, 0.1, 0.2],  # Minimum loss reduction
# }

# # Set up GridSearchCV
# grid_search = GridSearchCV(
#     estimator=xgb_grid,
#     param_grid=param_grid,
#     scoring='accuracy',  # Metric for evaluation (can be changed to f1, recall, etc.)
#     cv=5,  # 5-fold cross-validation
#     verbose=1,
#     n_jobs=-1  # Use all processors
# )

# # Perform the grid search
# grid_search.fit(x_train, y_train)

# # Display the best parameters and best score
# print("Best Parameters:", grid_search.best_params_)
# print("Best Cross-Validation Score:", grid_search.best_score_)

# # Evaluate the best model on the test set
# best_model = grid_search.best_estimator_
# test_accuracy = best_model.score(x_test, y_test)
# print("Test Accuracy:", test_accuracy)





print('All Working')
# End time
script_end_time = time.time()
total_time = script_end_time - script_start_time
print(f"Total execution time for the script: {total_time:.2f} seconds")

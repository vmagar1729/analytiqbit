





























# installing the libraries to load transformers models
!pip install transformers -q


# Import necessary libraries for data manipulation and analysis
import pandas as pd
import numpy as np

# Import visualization libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Import modules from scikit-learn for machine learning tasks
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix

# Import TensorFlow for deep learning tasks
import tensorflow as tf

# importing library for text preprocessing
import re

# Import modules from the Hugging Face transformers library
from transformers import BertTokenizer, TFBertForSequenceClassification


# Set the seed for the TensorFlow random number generator to ensure reproducibility
tf.random.set_seed(42)





# loading the dataset
df = pd.read_csv('../data/Articles.csv')


# creating a copy of the dataset
data = df.copy()








# Print first 5 rows of data
data.head()


# checking an article
data.loc[3, 'Article text']





# print shape of data
data.shape








# Check for missing values
data.isnull().sum()








data.describe()








 # function to create labeled barplots

def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot








labeled_barplot(data, "Category", perc=True)








labeled_barplot(data, "Section", perc=True, n = 15)








data['year'] = pd.to_datetime(data['Date published']).dt.year


labeled_barplot(data, "year", perc=True)








dataset = data.copy()





def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove special characters and numbers
    text = re.sub(r'[^A-Za-z\s]', '', text)

    # Remove extra whitespaces
    text = re.sub(r'\s+', ' ', text).strip()

    return text


# preprocessing the textual column
dataset['Article_text_clean'] = dataset['Article text'].apply(preprocess_text)





# Creating dependent and independent variables
X = dataset['Article_text_clean']
y = dataset['Category']


from sklearn.model_selection import train_test_split

# Initial split into training (80%) and testing (20%)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)

# Further split the temporary set into validation (10%) and test (10%) sets
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42)


print(X_train.shape, X_test.shape, X_valid.shape)


print(y_train.shape, y_test.shape, y_valid.shape)








# Label Encoding
encoder = LabelEncoder()

# fitting the encoder to the training labels
y_train_enc = encoder.fit_transform(y_train)

# applying the encoder mapping from training labels to validation and test labels
y_valid_enc = encoder.transform(y_valid)
y_test_enc = encoder.transform(y_test)





# loading and creating an instance of the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)





# specifying the maximum length of the input
max_length = 512


X_train_tokenized = tokenizer(
    X_train.values.tolist(),    # passing the data as a list to the tokenizer
    max_length=max_length,    # specifies the maximum length of the tokenized data
    padding='max_length',    # padding the data to the specified maximum length
    truncation=True,    # truncating the input if it is longer than the specified maximum length
    return_attention_mask=True,    # specifying to return attention masks
    return_tensors='tf',    # specifying to return the output as tensorflow tensors
)


X_valid_tokenized = tokenizer(
    X_valid.values.tolist(),
    max_length=max_length,
    padding='max_length',
    truncation=True,
    return_attention_mask=True,
    return_tensors='tf',
)


X_test_tokenized = tokenizer(
    X_test.values.tolist(),
    max_length=max_length,
    padding='max_length',
    truncation=True,
    return_attention_mask=True,
    return_tensors='tf',
)








# defining the size of the batches
batch_size = 8

# converting the tokenized input and the output into a batched tensorflow dataset for training
train_tokenized_tf = tf.data.Dataset.from_tensor_slices((dict(X_train_tokenized), y_train_enc)).batch(batch_size)

# converting the tokenized input and the output into a batched tensorflow dataset for validation
valid_tokenized_tf = tf.data.Dataset.from_tensor_slices((dict(X_valid_tokenized), y_valid_enc)).batch(batch_size)

# converting the tokenized input and the output into a batched tensorflow dataset for testing
test_tokenized_tf = tf.data.Dataset.from_tensor_slices((dict(X_test_tokenized), y_test_enc)).batch(batch_size)











num_classes = y.nunique()
num_classes





# Model initialization using BERT for sequence classification
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)


# print the summary of the model
model.summary()





# setting the learning rate for the optimizer
learning_rate = 1e-5

# Setting the optimizer to Adam
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)

# Specify the loss function for the model
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# Define evaluation metric(s) for the model
metric = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]

# Compile the model with the chosen optimizer, loss function, and metrics
model.compile(optimizer=optimizer, loss=loss, metrics=metric)





# Calculate class weights for imbalanced dataset
cw = (y_train_enc.shape[0]) / np.bincount(y_train_enc)

# Create a dictionary mapping class indices to their respective class weights
cw_dict = {}
for i in range(cw.shape[0]):
    cw_dict[encoder.transform(encoder.classes_)[i]] = cw[i]





# Number of training epochs
n_epochs = 3

bert_base_tuned = model.fit(train_tokenized_tf, epochs=n_epochs, validation_data=valid_tokenized_tf, class_weight=cw_dict)


# Generate raw predictions on the validation dataset using the trained model
preds_raw_val = model.predict(valid_tokenized_tf)

# Extract predicted labels by finding the index with the highest probability for each example
preds_val = np.argmax(np.array(tf.nn.softmax(preds_raw_val.logits)), axis=1)

# Display the shape of the predicted labels array
preds_val.shape


# retrieving the labels from the tensorflow dataset
actual_val = np.concatenate([y for x, y in valid_tokenized_tf], axis=0)

# Creating confusion matrix
cnf_mt = confusion_matrix(actual_val, preds_val)

# Visualizing confusion matrix using a heatmap
labels = encoder.classes_.tolist()
sns.heatmap(cnf_mt, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()


# Display classification report
print('Classification Report:')
print(classification_report(actual_val, preds_val))








preds_raw_test = model.predict(test_tokenized_tf)
preds_test = np.argmax(np.array(tf.nn.softmax(preds_raw_test.logits)), axis=1)
preds_test.shape


actual_test = np.concatenate([y for x, y in test_tokenized_tf], axis=0)

cnf_mt = confusion_matrix(actual_test, preds_test)

labels = encoder.classes_.tolist()
sns.heatmap(cnf_mt, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()


print('Classification Report:')
print(classification_report(actual_test, preds_test))













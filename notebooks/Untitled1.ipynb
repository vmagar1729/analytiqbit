{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd266d00-3754-4af1-b3a7-c3f7d2255aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Function to evaluate a model and extract its metrics\n",
    "def evaluate_model(model_name, model_type, grid_search, y_true, y_pred, custom_param1, custom_param2):\n",
    "    \"\"\"\n",
    "    Evaluate a model and return a dictionary of metrics, confusion matrix values, and custom parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: str, name of the model.\n",
    "    - model_type: str, type of the model (e.g., \"Logistic Regression\", \"Decision Tree\", \"Random Forest\").\n",
    "    - grid_search: bool, indicates if the model is from grid search.\n",
    "    - y_true: array-like, true class labels.\n",
    "    - y_pred: array-like, predicted class labels.\n",
    "    - custom_param1: float, custom parameter 1 for the model.\n",
    "    - custom_param2: float, custom parameter 2 for the model.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Metrics, confusion matrix values, and custom parameters for the model.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "    # Build dictionary of metrics\n",
    "    metrics = {\n",
    "        \"Model Name\": model_name,\n",
    "        \"Model Type\": model_type,\n",
    "        \"Grid Search\": grid_search,\n",
    "        \"Accuracy\": report[\"accuracy\"],\n",
    "        \"Precision\": report[\"1\"][\"precision\"],\n",
    "        \"Recall\": report[\"1\"][\"recall\"],\n",
    "        \"F1-Score\": report[\"1\"][\"f1-score\"],\n",
    "        \"True Negatives (TN)\": tn,\n",
    "        \"False Positives (FP)\": fp,\n",
    "        \"False Negatives (FN)\": fn,\n",
    "        \"True Positives (TP)\": tp,\n",
    "        \"Custom Param 1\": custom_param1,\n",
    "        \"Custom Param 2\": custom_param2,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Replace with actual predictions and parameters\n",
    "model_results = [\n",
    "    {\n",
    "        \"model_name\": \"lg\",\n",
    "        \"model_type\": \"Logistic Regression\",\n",
    "        \"grid_search\": False,\n",
    "        \"y_pred\": y_pred_test,  # Replace with actual predictions for Logistic Regression\n",
    "        \"Expected Revenue\": 2698670.83,\n",
    "        \"Confidence\": 0.8378248172411435,\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"log_reg\",\n",
    "        \"model_type\": \"Logistic Regression\",\n",
    "        \"grid_search\": True,\n",
    "        \"y_pred\": y_pred_lr_grid,  # Replace with actual predictions for GridSearch Logistic Regression\n",
    "        \"custom_param1\": 0.6,\n",
    "        \"custom_param2\": 0.9,\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"Decision Tree\",\n",
    "        \"model_type\": \"Decision Tree\",\n",
    "        \"grid_search\": False,\n",
    "        \"y_pred\": y_pred_dt,  # Replace with actual predictions for Decision Tree\n",
    "        \"custom_param1\": 0.7,\n",
    "        \"custom_param2\": 0.85,\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"Decision Tree (GridSearch)\",\n",
    "        \"model_type\": \"Decision Tree\",\n",
    "        \"grid_search\": True,\n",
    "        \"y_pred\": y_pred_dt_grid,  # Replace with actual predictions for GridSearch Decision Tree\n",
    "        \"custom_param1\": 0.75,\n",
    "        \"custom_param2\": 0.88,\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"Random Forest\",\n",
    "        \"model_type\": \"Random Forest\",\n",
    "        \"grid_search\": False,\n",
    "        \"y_pred\": y_pred_rf,  # Replace with actual predictions for Random Forest\n",
    "        \"custom_param1\": 0.8,\n",
    "        \"custom_param2\": 0.92,\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"Random Forest (GridSearch)\",\n",
    "        \"model_type\": \"Random Forest\",\n",
    "        \"grid_search\": True,\n",
    "        \"y_pred\": y_pred_rf_grid,  # Replace with actual predictions for GridSearch Random Forest\n",
    "        \"custom_param1\": 0.85,\n",
    "        \"custom_param2\": 0.95,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Evaluate all models and combine results\n",
    "results = []\n",
    "for model in model_results:\n",
    "    metrics = evaluate_model(\n",
    "        model_name=model[\"model_name\"],\n",
    "        model_type=model[\"model_type\"],\n",
    "        grid_search=model[\"grid_search\"],\n",
    "        y_true=y_test,  # Assuming y_test is the same for all models\n",
    "        y_pred=model[\"y_pred\"],\n",
    "        custom_param1=model[\"custom_param1\"],\n",
    "        custom_param2=model[\"custom_param2\"],\n",
    "    )\n",
    "    results.append(metrics)\n",
    "\n",
    "# Create DataFrame for all results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Model Metrics and Parameters\", dataframe=results_df)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

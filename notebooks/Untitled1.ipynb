{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1faeff3b-59ed-41db-9f2d-80c29c09ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.33.0\n",
      "  Downloading transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\n",
      "Requirement already satisfied: filelock in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers==4.33.0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers==4.33.0) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers==4.33.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers==4.33.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers==4.33.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers==4.33.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers==4.33.0) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.0)\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-macosx_12_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers==4.33.0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from transformers==4.33.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from requests->transformers==4.33.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from requests->transformers==4.33.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from requests->transformers==4.33.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages (from requests->transformers==4.33.0) (2024.12.14)\n",
      "Downloading transformers-4.33.0-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading tokenizers-0.13.3-cp311-cp311-macosx_12_0_arm64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "Successfully installed tokenizers-0.13.3 transformers-4.33.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.33.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ba9f9c-f104-4429-9b5e-4a60bec78da4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "dlopen(/Users/vivekmagar/python310_env/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN3tsl8internal10LogMessageC1EPKcii\n  Referenced from: <D2EF42E3-3A7F-39DD-9982-FB6BCDC2853C> /Users/vivekmagar/python310_env/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib\n  Expected in:     <E6BA2FD2-654D-380C-8224-59AA45D1133C> /Users/vivekmagar/python310_env/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Check TensorFlow version\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorFlow version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tf\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[0;32m~/python310_env/lib/python3.10/site-packages/tensorflow/__init__.py:437\u001b[0m\n\u001b[1;32m    435\u001b[0m _plugin_dir \u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_s, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow-plugins\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_plugin_dir):\n\u001b[0;32m--> 437\u001b[0m   \u001b[43m_ll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_plugin_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;66;03m# Load Pluggable Device Library\u001b[39;00m\n\u001b[1;32m    439\u001b[0m   _ll\u001b[38;5;241m.\u001b[39mload_pluggable_device_library(_plugin_dir)\n",
      "File \u001b[0;32m~/python310_env/lib/python3.10/site-packages/tensorflow/python/framework/load_library.py:151\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(library_location)\u001b[0m\n\u001b[1;32m    148\u001b[0m     kernel_libraries \u001b[38;5;241m=\u001b[39m [library_location]\n\u001b[1;32m    150\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m lib \u001b[38;5;129;01min\u001b[39;00m kernel_libraries:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mpy_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_LoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m       errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[1;32m    156\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe file or folder to load kernel libraries from does not exist.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    157\u001b[0m       library_location)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: dlopen(/Users/vivekmagar/python310_env/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN3tsl8internal10LogMessageC1EPKcii\n  Referenced from: <D2EF42E3-3A7F-39DD-9982-FB6BCDC2853C> /Users/vivekmagar/python310_env/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib\n  Expected in:     <E6BA2FD2-654D-380C-8224-59AA45D1133C> /Users/vivekmagar/python310_env/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27054f52-3dab-4d6b-bf0f-891b484acc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivekmagar/miniconda3/envs/py311/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-12-26 19:42:42.950479: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2024-12-26 19:42:42.950569: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 96.00 GB\n",
      "2024-12-26 19:42:42.950583: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 36.00 GB\n",
      "2024-12-26 19:42:42.950623: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-26 19:42:42.950649: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Try loading the tokenizer and model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:2894\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2892\u001b[0m         model\u001b[38;5;241m.\u001b[39mbuild()  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[1;32m   2893\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2894\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[1;32m   2896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safetensors_from_pt:\n\u001b[1;32m   2897\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_pytorch_state_dict_in_tf2_model\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/keras/src/layers/layer.py:226\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[1;32m    225\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m current_path()\n\u001b[0;32m--> 226\u001b[0m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:1131\u001b[0m, in \u001b[0;36mTFPreTrainedModel.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1130\u001b[0m     call_context \u001b[38;5;241m=\u001b[39m get_call_context_function()\n\u001b[0;32m-> 1131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mcall_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39min_call:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "# Try loading the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c55f6-aebb-43f4-8527-83d2593583c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

"# **Loan Default Prediction**"
"## **Problem Definition**\n"
"\n"
"### **The Context:**\n"
"\n"
"Loan defaults pose a significant threat to a bank’s financial health. For instance, while the mortgage delinquency rate in the United States has been gradually declining since the COVID-19 pandemic, as of February 2024, the overall delinquency rate was 2.8%. Such non-performing assets (NPAs) erode profitability and constrain the bank’s ability to extend new credit. Moreover, the manual loan approval process is labor-intensive and susceptible to human errors and biases, leading to potential misjudgments in loan approvals or rejections. Addressing this issue is crucial to enhance operational efficiency, reduce financial risks, and uphold equitable lending practices.\n"
"\n"
"### **The objective:**\n"
"\n"
"The goal is to modernize and streamline the loan approval process by leveraging machine learning classification methods, capable of as accurately predicting loan defaults as possible using available data. This model should:\n"
" - Enhance operational efficiency by automating repetitive tasks and reducing the time spent on manual credit assessments.\n"
" - Mitigate risk by identifying high-risk applicants before loan issuance.\n"
" - Ensure fairness by eliminating biases that could disadvantage certain demographics, thus aligning with federal regulation, legal requirements and ethical standards.\n"
" - Ensure Regulatory Compliance by aligning with legal frameworks such as the Equal Credit Opportunity Act, which mandates non-discriminatory lending practices and requires transparent justifications for adverse decisions.\n"
" - Enable data-driven decision-making by providing interpretable insights into why a loan is approved or rejected, allowing for greater transparency and trust with stakeholders.\n"
" - Promote Fairness by eliminating biases inherent in human judgment, ensuring equitable treatment of all applicants.\n"
" - The model should be extensible to learn from future loan defaults\n"
"\n"
"### **The key questions:**\n"
"\n"
" - Which applicants are likely to default on their loans? Identifying potential defaulters enables targeted risk management strategies.\n"
" - What factors are most critical in assessing an applicant’s creditworthiness? Determining key indicators such as income stability, debt-to-income ratio, and credit history informs more accurate evaluations.\n"
" - How can the loan approval process be optimized for efficiency and fairness? Implementing automated, data-driven systems can streamline operations while upholding ethical standards.\n"
" - How can the model ensure compliance with regulatory requirements? Providing interpretable justifications for loan decisions is essential to meet legal obligations and maintain transparency.\n"
" - How can historical biases be prevented from influencing the model? Ensuring the model is trained on unbiased data is crucial to avoid perpetuating past discriminatory practices.\n"
"\n"
"### **The problem formulation**:\n"
"\n"
"Data science enables us to:\n"
" - Predict defaults: Use predictive analytics to assess the likelihood of applicants failing to meet their repayment obligations.\n"
" - Automate Decision-Making: Replace subjective human assessments with objective, data-driven evaluations, enhancing consistency and speed.\n"
" - Identify Key Risk Factors: Analyze data to pinpoint variables that significantly influence default risk, such as high debt-to-income ratios or unstable employment histories.\n"
" - Ensure Model Interpretability: Develop transparent models that provide clear explanations for their decisions, facilitating compliance with regulations and building stakeholder trust.\n"
" - Mitigate Bias: Implement fairness algorithms to detect and correct biases, ensuring the model’s decisions are equitable and just.\n"
"\n"
"By solving this problem, the bank not only reduces risk but also transforms its loan approval system into a benchmark for innovation and inclusivity."
"## **Data Description:**\n"
"The Home Equity dataset (HMEQ) contains baseline and loan performance information for 5,960 recent home equity loans. The target (BAD) is a binary variable that indicates whether an applicant has ultimately defaulted or has been severely delinquent. This adverse outcome occurred in 1,189 cases (20 percent). 12 input variables were registered for each applicant.\n"
"\n"
"\n"
"* **BAD:** 1 = Client defaulted on loan, 0 = loan repaid\n"
"\n"
"* **LOAN:** Amount of loan approved.\n"
"\n"
"* **MORTDUE:** Amount due on the existing mortgage.\n"
"\n"
"* **VALUE:** Current value of the property. \n"
"\n"
"* **REASON:** Reason for the loan request. (HomeImp = home improvement, DebtCon= debt consolidation which means taking out a new loan to pay off other liabilities and consumer debts) \n"
"\n"
"* **JOB:** The type of job that loan applicant has such as manager, self, etc.\n"
"\n"
"* **YOJ:** Years at present job.\n"
"\n"
"* **DEROG:** Number of major derogatory reports (which indicates a serious delinquency or late payments). \n"
"\n"
"* **DELINQ:** Number of delinquent credit lines (a line of credit becomes delinquent when a borrower does not make the minimum required payments 30 to 60 days past the day on which the payments were due). \n"
"\n"
"* **CLAGE:** Age of the oldest credit line in months. \n"
"\n"
"* **NINQ:** Number of recent credit inquiries. \n"
"\n"
"* **CLNO:** Number of existing credit lines.\n"
"\n"
"* **DEBTINC:** Debt-to-income ratio (all your monthly debt payments divided by your gross monthly income. This number is one way lenders measure your ability to manage the monthly payments to repay the money you plan to borrow."
"## **Import the necessary libraries and Data**"
"## **Data Overview**\n"
"- Reading the dataset\n"
"- Understanding the shape of the dataset\n"
"- Checking the data types\n"
"- Checking for missing values\n"
"- Checking for duplicated values"
"#### Let's check any inconsitencies in the data"
"#### Observations\n"
"1. Dataset contains several missing values. These need to be treated individually as the use of summary statistic may not be appropriate or sufficient for this this dataset i.e. morgage_amount_due can have missing values,but loan_request_reason or property_current_value cannot be. Also many of the missing values may depend on values in other columns, such as major_derogatory_report, delinquent_credit_lines, recent_credit_inquiries, existing_credit_lines and debt_to_income_ratio are all correlated\n"
"2. Features like debt_to_income_ratio and major_derogatory_reports have significant missing values (21.3% and 11.9%, respectively)\n"
"4. Dataset has 5960 rows and 13 columns (12 independant features)\n"
"5. Data is for approved loans only, as there are no missing values in the loan_amount_approved\n"
"6. No duplicate records, which is understandable, as most individuals or families requesting for loan are unique\n"
"7. All categorical values look clean - no typos, unique values and consistent labeling\n"
"8. Many of the features should be integrers, but are floats in the dataset - major_derogatory_reports, delinquent_credir_lines, recent_credit_inquiries and existing_credit_lines. If the actual values, indeed are floats, its a data descripency which should be addresses before further processing\n"
"9. Proportion of defaulting client is 20%, which is too high by industry standards, indicating a highly biased dataset\n"
"10. Several records had a very high loan to value ratio. This is quite unusual. All the records with ltv > 2.0 were dropped\n"
"11. Also, the features applicant job type and years at present job, may invite discimatory litigation (class action lawsuits)!"
"## Summary Statistics"
"#### Observations\n"
"\n"
"1. Skewness: major_derogatory_reports, delinquent_credit_lines, debt_to_income_ratio are concentrated near zero with a long tail, indicating skewness\n"
"2. Outliers: mortgage_amount_due, property_current_value, age_of_oldest_credit_line, and debt_to_income_ratio extreme high max values, with 75% percentile closer to the mean, indicating outliers.\n"
"\n"
"Vizualization of the distributions of these features will provide clarity on these observations."
"## **Exploratory Data Analysis (EDA) and Visualization**"
"- EDA is an important part of any project involving data.\n"
"- It is important to investigate and understand the data better before building a model with it.\n"
"- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.\n"
"- A thorough analysis of the data, in addition to the questions mentioned below, should be done."
"**Leading Questions**:\n"
"1. What is the range of values for the loan amount variable \"LOAN\"?\n"
"2. How does the distribution of years at present job \"YOJ\" vary across the dataset?\n"
"3. How many unique categories are there in the REASON variable?\n"
"4. What is the most common category in the JOB variable?\n"
"5. Is there a relationship between the REASON variable and the proportion of applicants who defaulted on their loan?\n"
"6. Do applicants who default have a significantly different loan amount compared to those who repay their loan?\n"
"7. Is there a correlation between the value of the property and the loan default rate?\n"
"8. Do applicants who default have a significantly different mortgage amount compared to those who repay their loan?"
"### **Data prep for EDA and subsequent processing**"
"#### Missing value treatement\n"
"I have used techniques from recommandation systems to impute the missing values **(Reference: \"Missing Data: A Gentle Introduction\" by Patrick E. McKnight, Katherine M. McKnight, Souraya Sidani, and Aurelio José Figueredo)**. Below is a summary of methods used for imputatation\n"
"\n"
"**1. KNN Imputation (Low/Moderate Missingness)**\n"
"\n"
"* mortgage_amount_due, property_current_value, loan_request_reason, applicant_job_type, age_of_oldest_credit_line, existing_credit_line columns have less than 10% missing values. KNN imputation is effective for low-to-moderate missingness by finding similar rows based on available features and averaging/matching the corresponding feature values.\n"
"\n"
"**2. Iterative Imputer (High Missingness)**\n"
"\n"
"* debt_to_income_ratio, major_derogatory_reports, delinquentt_credit_line, years_at_present_job, recent_credit_inquirie columns have higher missing percentages (10–21%). Iterative imputation models each feature as a function of the others, iteratively predicting missing values to best fit the observed data.\n"
"\n"
"**3. Encoding for Categorical Data**\n"
"\n"
"* loan_request_reason, applicant_job_type columns contain categorical data. They were encoded using LabelEncoder to convert string values into numeric categories, allowing compatibility with KNN imputer."
"#### Prepare data for EDA"
"### **Univariate Analysis**\n"
"#### Observation:\n"
"Mortgage amount due shows a slightly right skewed distribution, but with a lot of outliers. These may need to be treated based on the type of modelling technique used for modeling"
"#### Observation:\n"
"Current property value shows a slightly right skewed distribution, but with a lot of outliers. These may need to be treated based on the type of modelling technique used for modeling"
"#### Observation:\n"
"No of years at present job shows a moderately right skewed, multi-modal distribution, with a few outliers, indicating most loan requesters maybe in middle age group. Extreme outliers may have to be capped"
"#### Observations:\n"
"Major derogatory reports shows a highly right skewed distribution, with most values around zero. This may indicate a bias in the prescreening process"
"#### Observations:\n"
"Delinquent credit lines shows a highly right skewed distribution, with most values around zero. This may indicate a bias in the prescreening process"
"#### Observation:\n"
"Oldest credit line shows fairly uniform bi-modal distribution, with quite a few outliers. What maybe of interest here are the loan requesters with low or zero age of credit line. Also extreme outliers may have to be capped"
"#### Observations:\n"
"Recent credit inquiries shows a highly right skewed distribution, with most values around zero. This may indicate a bias in the prescreening process"
"#### Observations:\n"
"Existing credit lines shows a reasonably uniform distribution with slight right tail and a few outliers. Extreme outliers may need to be capped"
"#### Observations:\n"
"Debt to income ratio is slightly left skewed, with outliers on both sides. Extreme outlier on the high may need capping. Left skewness is a good thing as a low debt to income ratio is desirable\n"
"The distribution is also showing a sharp drop around ~ 40. Should investigate this further"
"#### Observation:\n"
"Amount of loan approved shows a slightly right skewed distribution, but with a lot of outliers. These may need to be treated based on the type of modelling technique used for modeling"
"#### Observations:\n"
"1. ~ 70% of requests are for debt consolidation, indication a fundamental bias in the dataset, or biased selection process\n"
"2. Majority (~ 45%) of the loan requests are from people who did not provide details of the profession, suggesting improvements in the data collection process"
"### **Bivariate Analysis**"
"### Check correlation for numeric variables"
"#### Observations:\n"
"1. Property value as of date seems to have a strong +ve correlation to mortgage amount due. This is only possible if all if the mortgages in the dataset were approved around the same time and the approved mortgage amount was a fixed % of the property value, irrespective of other factors. So age of the mortgage may be a useful feature (latent variable). Most other variables have weak correlations"
"#### Let's check the distribution of numerical variables to loan defaults"
"#### Observations\n"
"1. No of defaulters are highest where recent credit inquiries, delinquent credit lines, major derogatory reports are the lowest. This seems counter intuitive at first, how ever this is most likely due to these customers not having any credit records at all. This will happen if these customers were denied credit in the past and have not applied for credit in the recent time (for the duration during which this data was collected). Additional data is required to understand this anamoly - gross income, demographics, macroeconomic factors such severe recession or other financial distress\n"
"2. Default rate is the extra-ordinarily high (80%) for customers with debt to income ratio between 30-41. Understanding this anomaly likely requires a more detailed analysis of the borrower behavior and external conditions. This data set is insufficient for such analysis, as was commented in no 1 above"
"#### Let's check the distribution of categorical variables to loan defaults"
"#### Observations:\n"
"1. As expected, loan defaults are higher for clients who requested loan for debt consolidation, as these clients already had substantial debt and most likely not enough income to cover the debt\n"
"2. Applicants with job type as 'other' have the highest default rate. This may be because these clients do not have a steady job and as such chose not to disclose it. Also the higher default rate for 'Mgr' and 'ProfExe' is interesting and may need further analysis"
"#### Observation:\n"
"1. All the available features are significant for the prediction of target variable\n"
"2. Also, features related to clients credit posture seem to have higher significance, which is expected"
"#### Observation:\n"
"current property value shows a very high correlation with mortgage amount due, which is expected. These should be replaced with an engineered feature. Additional insights can be gained from priniciple component analysis"
"### **Multivariate Analysis**"
"#### Multivariate analysis using PCA"
"#### Prep the data for PCA"
"#### Observation\n"
"There is no single component which can be attributed with a large variance in the dataset"
"Let's check the contribution of each of the original features to the principle components"
"#### Create a new feature to replace Mortgage due and property value as they are highly correlated, and check if it impacts the PCs"
"#### Observations\n"
"Based on PCA analysis, it is clear that replacing 'mortgage_amount_due', 'property_current_value' by 'mortgage_to_property_ratio' will not impact modelling performance"
"## Treating Outliers - \n"
"Only the outliers for mortgage due and property value have been dropped, where the ratio of these features was improbable. See section to 1.4.0.1. All the other outliers are maintained as is as they seem to be realistic data points"
"## Treating Missing Values: See section 1.6.1.1"
"## **Important Insights from EDA**\n"
"\n"
"What are the the most important observations and insights from the data based on the EDA performed?"
"## **Model Building - Approach**\n"
"- Data preparation\n"
"- Partition the data into train and test set\n"
"- Build the model\n"
"- Fit on the train data\n"
"- Tune the model\n"
"- Test the model on test set"
"#### Confusion Matrix"
"| Actual/Predicted       | Predicted: 0 (No Default) | Predicted: 1 (Default)|\n"
"|:-----------------------|:--------------------------|:----------------------|\n"
"| Actual: 0 (No Default) | True Negative (TN)        | False Positive (FP)   |\n"
"| Actual: 1 (Default)    | False Negative (FN)       | True Positive (TP)    |"
"### Logistic Regression"
"### Decision Tree"
"### **Decision Tree - Hyperparameter Tuning**\n"
"\n"
"* Hyperparameter tuning is tricky in the sense that **there is no direct way to calculate how a change in the hyperparameter value will reduce the loss of your model**, so we usually resort to experimentation. We'll use Grid search to perform hyperparameter tuning.\n"
"* **Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters.** \n"
"* **It is an exhaustive search** that is performed on the specific parameter values of a model.\n"
"* The parameters of the estimator/model used to apply these methods are **optimized by cross-validated grid-search** over a parameter grid.\n"
"\n"
"**Criterion {“gini”, “entropy”}**\n"
"\n"
"The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n"
"\n"
"**max_depth** \n"
"\n"
"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n"
"\n"
"**min_samples_leaf**\n"
"\n"
"The minimum number of samples is required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n"
"\n"
"You can learn about more Hyperpapameters on this link and try to tune them. \n"
"\n"
"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"
"### **Building a Random Forest Classifier**\n"
"\n"
"**Random Forest is a bagging algorithm where the base models are Decision Trees.** Samples are taken from the training data and on each sample a decision tree makes a prediction. \n"
"\n"
"**The results from all the decision trees are combined together and the final prediction is made using voting or averaging.**"
"### **Random Forest Classifier Hyperparameter Tuning**"
"**1. Comparison of various techniques and their relative performance based on chosen Metric (Measure of success):** \n"
"- How do different techniques perform? Which one is performing relatively better? Is there scope to improve the performance further?"
"**2. Refined insights:** \n"
"- What are the most meaningful insights relevant to the problem?"
"**3. Proposal for the final solution design:** \n"
"- What model do you propose to be adopted? Why is this the best solution to adopt?"

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_iHEvciuTB9"
   },
   "source": [
    "# **Loan Default Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKyzwpUiuTB2"
   },
   "source": [
    "## **Problem Definition**\n",
    "\n",
    "### **The Context:**\n",
    "\n",
    "Loan defaults pose a significant threat to a bank’s financial health. For instance, while the mortgage delinquency rate in the United States has been gradually declining since the COVID-19 pandemic, as of February 2024, the overall delinquency rate was 2.8%. Such non-performing assets (NPAs) erode profitability and constrain the bank’s ability to extend new credit. Moreover, the manual loan approval process is labor-intensive and susceptible to human errors and biases, leading to potential misjudgments in loan approvals or rejections. Addressing this issue is crucial to enhance operational efficiency, reduce financial risks, and uphold equitable lending practices.\n",
    "\n",
    "### **The objective:**\n",
    "\n",
    "The goal is to modernize and streamline the loan approval process by leveraging machine learning classification methods, capable of as accurately predicting loan defaults as possible using available data. This model should:\n",
    " - Enhance operational efficiency by automating repetitive tasks and reducing the time spent on manual credit assessments.\n",
    " - Mitigate risk by identifying high-risk applicants before loan issuance.\n",
    " - Ensure fairness by eliminating biases that could disadvantage certain demographics, thus aligning with federal regulation, legal requirements and ethical standards.\n",
    " - Ensure Regulatory Compliance by aligning with legal frameworks such as the Equal Credit Opportunity Act, which mandates non-discriminatory lending practices and requires transparent justifications for adverse decisions.\n",
    " - Enable data-driven decision-making by providing interpretable insights into why a loan is approved or rejected, allowing for greater transparency and trust with stakeholders.\n",
    " - Promote Fairness by eliminating biases inherent in human judgment, ensuring equitable treatment of all applicants.\n",
    " - The model should be extensible to learn from future loan defaults\n",
    "\n",
    "### **The key questions:**\n",
    "\n",
    " - Which applicants are likely to default on their loans? Identifying potential defaulters enables targeted risk management strategies.\n",
    " - What factors are most critical in assessing an applicant’s creditworthiness? Determining key indicators such as income stability, debt-to-income ratio, and credit history informs more accurate evaluations.\n",
    " - How can the loan approval process be optimized for efficiency and fairness? Implementing automated, data-driven systems can streamline operations while upholding ethical standards.\n",
    " - How can the model ensure compliance with regulatory requirements? Providing interpretable justifications for loan decisions is essential to meet legal obligations and maintain transparency.\n",
    " - How can historical biases be prevented from influencing the model? Ensuring the model is trained on unbiased data is crucial to avoid perpetuating past discriminatory practices.\n",
    "\n",
    "### **The problem formulation**:\n",
    "\n",
    "Data science enables us to:\n",
    " - Predict defaults: Use predictive analytics to assess the likelihood of applicants failing to meet their repayment obligations.\n",
    " - Automate Decision-Making: Replace subjective human assessments with objective, data-driven evaluations, enhancing consistency and speed.\n",
    " - Identify Key Risk Factors: Analyze data to pinpoint variables that significantly influence default risk, such as high debt-to-income ratios or unstable employment histories.\n",
    " - Ensure Model Interpretability: Develop transparent models that provide clear explanations for their decisions, facilitating compliance with regulations and building stakeholder trust.\n",
    " - Mitigate Bias: Implement fairness algorithms to detect and correct biases, ensuring the model’s decisions are equitable and just.\n",
    "\n",
    "By solving this problem, the bank not only reduces risk but also transforms its loan approval system into a benchmark for innovation and inclusivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEHRGpcdo-KO"
   },
   "source": [
    "## **Data Description:**\n",
    "The Home Equity dataset (HMEQ) contains baseline and loan performance information for 5,960 recent home equity loans. The target (BAD) is a binary variable that indicates whether an applicant has ultimately defaulted or has been severely delinquent. This adverse outcome occurred in 1,189 cases (20 percent). 12 input variables were registered for each applicant.\n",
    "\n",
    "\n",
    "* **BAD:** 1 = Client defaulted on loan, 0 = loan repaid\n",
    "\n",
    "* **LOAN:** Amount of loan approved.\n",
    "\n",
    "* **MORTDUE:** Amount due on the existing mortgage.\n",
    "\n",
    "* **VALUE:** Current value of the property. \n",
    "\n",
    "* **REASON:** Reason for the loan request. (HomeImp = home improvement, DebtCon= debt consolidation which means taking out a new loan to pay off other liabilities and consumer debts) \n",
    "\n",
    "* **JOB:** The type of job that loan applicant has such as manager, self, etc.\n",
    "\n",
    "* **YOJ:** Years at present job.\n",
    "\n",
    "* **DEROG:** Number of major derogatory reports (which indicates a serious delinquency or late payments). \n",
    "\n",
    "* **DELINQ:** Number of delinquent credit lines (a line of credit becomes delinquent when a borrower does not make the minimum required payments 30 to 60 days past the day on which the payments were due). \n",
    "\n",
    "* **CLAGE:** Age of the oldest credit line in months. \n",
    "\n",
    "* **NINQ:** Number of recent credit inquiries. \n",
    "\n",
    "* **CLNO:** Number of existing credit lines.\n",
    "\n",
    "* **DEBTINC:** Debt-to-income ratio (all your monthly debt payments divided by your gross monthly income. This number is one way lenders measure your ability to manage the monthly payments to repay the money you plan to borrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcZcGaZruTB-"
   },
   "source": [
    "## **Import the necessary libraries and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing and dimensionality reduction\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Model selection and training\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Imputation and missing value handling\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from fancyimpute import KNN\n",
    "\n",
    "# Metrics and evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score, \n",
    "    precision_score, recall_score, f1_score, precision_recall_curve\n",
    ")\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Ignore warnings selectively\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Some other specific message.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*'force_all_finite' was renamed to 'ensure_all_finite'.*\")\n",
    "\n",
    "# Start time\n",
    "script_start_time = time.time()\n",
    "\n",
    "# Garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Add any additional functions or utility imports here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Functions\n",
    "def summarize_dataframe(df):\n",
    "    \"\"\"\n",
    "    Summarizes the structure and content of a DataFrame.\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        \"Column\": df.columns,\n",
    "        \"Non-Null Count\": df.notnull().sum(),\n",
    "        \"Data Type\": df.dtypes,\n",
    "        \"Unique Values\": df.nunique(),\n",
    "        \"Missing Values\": df.isnull().sum(),\n",
    "        \"Missing Values %\": (df.isnull().sum() / len(df) * 100).round(2),\n",
    "    }\n",
    "    return pd.DataFrame(summary).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def segregate_columns_by_dtype(df):\n",
    "    \"\"\"\n",
    "    Segregates DataFrame columns by their data types.\n",
    "    \"\"\"\n",
    "    return {dtype.name: df.select_dtypes(include=[dtype]).columns.tolist() for dtype in df.dtypes.unique()}\n",
    "\n",
    "\n",
    "def impute_missing_values(df, knn_cols, iterative_cols, categorical_cols):\n",
    "    \"\"\"\n",
    "    Imputes missing values using KNN and IterativeImputer.\n",
    "    \"\"\"\n",
    "    original_categories = {}\n",
    "    \n",
    "    # Encode categorical columns for KNN\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "        original_categories[col] = df[col].cat.categories\n",
    "        df[col] = df[col].cat.codes.replace(-1, np.nan)\n",
    "    \n",
    "    # Impute with KNN\n",
    "    df[knn_cols] = KNN(k=5, verbose=False).fit_transform(df[knn_cols])\n",
    "    \n",
    "    # Impute with IterativeImputer\n",
    "    df[iterative_cols] = IterativeImputer(max_iter=10, random_state=42).fit_transform(df[iterative_cols])\n",
    "\n",
    "    return df, original_categories\n",
    "\n",
    "\n",
    "def decode_categorical_columns(df, original_categories):\n",
    "    \"\"\"\n",
    "    Decodes categorical columns back to original values.\n",
    "    \"\"\"\n",
    "    for col, categories in original_categories.items():\n",
    "        df[col] = pd.Categorical.from_codes(df[col], categories)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_corr_and_vif(df, target):\n",
    "    \"\"\"\n",
    "    Calculates correlation matrix and VIF for numerical features.\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=[target], errors=\"ignore\")\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    corr_matrix = numeric_df.corr()\n",
    "    vif_data = pd.DataFrame({\n",
    "        \"Feature\": numeric_df.columns,\n",
    "        \"VIF\": [variance_inflation_factor(numeric_df.values, i) for i in range(numeric_df.shape[1])]\n",
    "    })\n",
    "    return corr_matrix, vif_data\n",
    "\n",
    "\n",
    "# Visualization Functions\n",
    "def plot_correlation_heatmap(data, title=\"Correlation Heatmap\"):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of correlations in the dataset.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(data.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pca(pca_object, title=\"Cumulative Explained Variance\"):\n",
    "    \"\"\"\n",
    "    Plots the cumulative explained variance ratio from a PCA object.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(pca_object.explained_variance_ratio_) + 1),\n",
    "             np.cumsum(pca_object.explained_variance_ratio_), 'r', marker=\"o\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Number of Principal Components\")\n",
    "    plt.ylabel(\"Cumulative Variance Explained\")\n",
    "    # plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def engineer_and_scale_features(data, ratio_cols, new_feature_name, drop_original=True):\n",
    "    \"\"\"\n",
    "    Engineer a new feature as the ratio of two columns, scale it, and optionally drop the original columns.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Input DataFrame.\n",
    "        ratio_cols (tuple): A tuple of two column names (numerator, denominator) to compute the ratio.\n",
    "        drop_original (bool): Whether to drop the original columns after creating the new feature. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with the new engineered and scaled feature.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "\n",
    "    # Create the new feature\n",
    "    df[new_feature_name] = df[ratio_cols[0]] / df[ratio_cols[1]]\n",
    "\n",
    "    # Drop the original columns if specified\n",
    "    if drop_original:\n",
    "        df.drop(list(ratio_cols), axis=1, inplace=True)\n",
    "\n",
    "    # Scale the new feature\n",
    "    df[new_feature_name] = StandardScaler().fit_transform(df[[new_feature_name]])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Model Evaluation Functions\n",
    "def metrics_score(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculates classification metrics and plots a confusion matrix.\n",
    "    \"\"\"\n",
    "    print(classification_report(actual, predicted))\n",
    "    cm = confusion_matrix(actual, predicted)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"rainbow_r\",\n",
    "                xticklabels=[\"Not Defaulted\", \"Defaulted\"],\n",
    "                yticklabels=[\"Not Defaulted\", \"Defaulted\"])\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "def calculate_expected_with_confidence(data, loan_value_col, interest_rate=0.085):\n",
    "    \"\"\"\n",
    "    Calculate the expected revenue/loss and average confidence from predictions.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Dataset containing the columns 'prob_default', 'actual', and the loan value column.\n",
    "        loan_value_col (str): The column name representing the loan values.\n",
    "        interest_rate (float): The interest rate applied to loans. Default is 8.5%.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - Total expected revenue (float)\n",
    "            - Average confidence level (float)\n",
    "    \"\"\"\n",
    "    # Add P(non-default) = 1 - P(default) to the dataset\n",
    "    data['prob_non_default'] = 1 - data['prob_default']\n",
    "    \n",
    "    # Calculate confidence scores as the maximum probability of the prediction\n",
    "    data['confidence'] = data[['prob_default', 'prob_non_default']].max(axis=1)\n",
    "    \n",
    "    # Revenue from True Negatives (TN): Correctly predicted non-defaults\n",
    "    data['revenue_tn'] = (\n",
    "        data['prob_non_default'] \n",
    "        * (1 - data['client_defaulted_on_loan'])  # Actual is non-default\n",
    "        * data['confidence']  # Adjust by confidence level\n",
    "        * data[loan_value_col] \n",
    "        * interest_rate\n",
    "    )\n",
    "    \n",
    "    # Loss from False Positives (FP): Predicted default but actual non-default\n",
    "    data['loss_fp'] = (\n",
    "        data['prob_default'] \n",
    "        * (1 - data['client_defaulted_on_loan'])  # Actual is non-default\n",
    "        * (1 - data['confidence'])  # Adjust by lack of confidence\n",
    "        * data[loan_value_col] \n",
    "        * interest_rate\n",
    "    )\n",
    "    \n",
    "    # Loss from False Negatives (FN): Predicted non-default but actual default\n",
    "    data['loss_fn'] = (\n",
    "        data['prob_non_default'] \n",
    "        * data['client_defaulted_on_loan']  # Actual is default\n",
    "        * (1 - data['confidence'])  # Adjust by lack of confidence\n",
    "        * data[loan_value_col] \n",
    "        * (1 + interest_rate)  # Account for the loan principal and interest\n",
    "    )\n",
    "    \n",
    "    # Total revenue = Sum of revenue from TN - Sum of losses from FP and FN\n",
    "    total_revenue = data['revenue_tn'].sum() - data['loss_fp'].sum() - data['loss_fn'].sum()\n",
    "    \n",
    "    # Average confidence level\n",
    "    avg_confidence = data['confidence'].mean()\n",
    "    \n",
    "    return total_revenue, avg_confidence\n",
    "\n",
    "\n",
    "def model_performance_classification(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a classification model.\n",
    "    \"\"\"\n",
    "    pred = model.predict(predictors)\n",
    "    scores = {\n",
    "        \"Precision\": precision_score(target, pred, average=\"macro\"),\n",
    "        \"Recall\": recall_score(target, pred, average=\"macro\"),\n",
    "        \"Accuracy\": accuracy_score(target, pred),\n",
    "    }\n",
    "    return pd.DataFrame([scores])\n",
    "\n",
    "# Function to evaluate the model and append results\n",
    "def evaluate_model_and_metrics(\n",
    "    model, x_test, y_test, X_scaled, default_model, model_name, model_type,\n",
    "    loan_value_col='loan_amount_approved', grid_search=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates the model on test data, calculates metrics, updates the dataset with probabilities, \n",
    "    and calculates expected revenue and confidence.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained machine learning model.\n",
    "        x_test (pd.DataFrame): Test set features.\n",
    "        y_test (pd.Series): Test set labels.\n",
    "        X_scaled (pd.DataFrame): Scaled dataset used for probability predictions.\n",
    "        default_model (pd.DataFrame): Original dataset to update with probabilities.\n",
    "        model_name (str): Name of the model for tracking.\n",
    "        model_type (str): Type of the model (e.g., Logistic Regression, Random Forest).\n",
    "        loan_value_col (str): Name of the column representing loan values (default: 'loan_amount_approved').\n",
    "        grid_search (bool): Whether the model was trained using GridSearchCV. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation metrics for appending to the model_metrics DataFrame.\n",
    "    \"\"\"\n",
    "    # Make predictions and calculate metrics\n",
    "    y_pred = model.predict(x_test)\n",
    "    metrics_score(y_test, y_pred)  # Display classification report and confusion matrix\n",
    "\n",
    "    # Update probabilities in the dataset\n",
    "    default_model_copy = default_model.copy()\n",
    "    pred_prob = model.predict_proba(X_scaled)\n",
    "    default_model_copy['prob_default'] = pred_prob[:, 1]\n",
    "\n",
    "    # Calculate expected revenue and confidence\n",
    "    revenue, confidence = calculate_expected_with_confidence(default_model_copy, loan_value_col)\n",
    "    print(f\"Total expected revenue: {revenue} with confidence of: {confidence}\")\n",
    "\n",
    "    # Generate classification report and confusion matrix\n",
    "    cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Compile metrics into a dictionary\n",
    "    metrics_dict = {\n",
    "        'Model Name': model_name,\n",
    "        'Model Type': model_type,\n",
    "        'Grid Search (Yes/No)': 'Yes' if grid_search else 'No',\n",
    "        'Recall': cr['1']['recall'],\n",
    "        'Precision': cr['1']['precision'],\n",
    "        'f1': cr['1']['f1-score'],\n",
    "        'Accuracy': cr['accuracy'],\n",
    "        'FPs': cm[0][1],\n",
    "        'FNs': cm[1][0],\n",
    "        'Revenue Prediction': revenue,\n",
    "        'Confidence': confidence,\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "# Data Visualization Functions\n",
    "def plot_grouped_barplots(data, features, target, figsize=(15, 6)):\n",
    "    \"\"\"\n",
    "    Plots grouped barplots for a list of features against a target variable.\n",
    "    \"\"\"\n",
    "    n_cols = 3\n",
    "    n_rows = math.ceil(len(features) / n_cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(figsize[0], figsize[1] * n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        grouped_data = data.groupby(feature, observed=True)[target].sum().reset_index()\n",
    "        sns.barplot(x=feature, y=target, data=grouped_data, ax=axes[i], color='green')\n",
    "        axes[i].set_title(f\"{target} by {feature}\")\n",
    "        axes[i].tick_params(axis=\"x\", rotation=90)\n",
    "    \n",
    "    for j in range(len(features), len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram(data, column, bins=None, xlabel=None, ylabel='Count', title=None, grid=True):\n",
    "    \"\"\"\n",
    "    Plots a histogram for a given column with customizable bins and styling.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The DataFrame containing the data.\n",
    "    - column (str): The name of the column to plot.\n",
    "    - bins (int, list, or np.ndarray, optional): Custom bins for the histogram. Default is None.\n",
    "    - xlabel (str, optional): Label for the x-axis. Default is the column name.\n",
    "    - ylabel (str, optional): Label for the y-axis. Default is 'Count'.\n",
    "    - title (str, optional): Title of the histogram. Default is None.\n",
    "    - grid (bool, optional): Whether to show gridlines. Default is True.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data[column], bins=bins, color='green', edgecolor='black', alpha=0.7)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    plt.xlabel(xlabel if xlabel else column)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title if title else f'Histogram of {column}')\n",
    "\n",
    "    # Add grid styling\n",
    "    if grid:\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def histogram_boxplot(data, feature, figsize=(12, 7), kde=True, bins=None):\n",
    "    \"\"\"\n",
    "    Combines a boxplot and histogram for a given feature in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The input dataset.\n",
    "    - feature (str): The feature to plot.\n",
    "    - figsize (tuple): Size of the figure (default: (12, 7)).\n",
    "    - kde (bool): Whether to show a kernel density estimate on the histogram (default: True).\n",
    "    - bins (int, list, or np.ndarray): Number of bins or edges for the histogram (default: None).\n",
    "    \"\"\"\n",
    "    # Create the subplots for boxplot and histogram\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,         # Number of rows in the subplot grid\n",
    "        sharex=True,     # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize, # Figure size\n",
    "    )\n",
    "\n",
    "    # Boxplot with the mean indicated\n",
    "    sns.boxplot(data=data, x=feature, ax=ax_box2, showmeans=True, color=\"red\")\n",
    "    ax_box2.set_title(f\"Boxplot of {feature}\")\n",
    "\n",
    "    # Automatically calculate bins if not provided\n",
    "    if bins is None:\n",
    "        q75, q25 = data[feature].quantile([0.75, 0.25])  # Interquartile range\n",
    "        iqr = q75 - q25\n",
    "        bin_width = 2 * iqr / (len(data[feature]) ** (1 / 3))  # Freedman-Diaconis rule\n",
    "        bin_width = max(bin_width, 1)  # Ensure minimum bin width of 1\n",
    "        bins = int((data[feature].max() - data[feature].min()) / bin_width)\n",
    "        bins = max(bins, 1)  # Ensure at least 1 bin\n",
    "\n",
    "    # Histogram\n",
    "    sns.histplot(data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, color=\"green\", alpha=0.6)\n",
    "    ax_hist2.set_title(f\"Histogram of {feature}\")\n",
    "    ax_hist2.set_xlabel(feature)\n",
    "    ax_hist2.set_ylabel(\"Count\")\n",
    "\n",
    "    # Add mean and median lines to the histogram\n",
    "    mean = data[feature].mean()\n",
    "    median = data[feature].median()\n",
    "    ax_hist2.axvline(mean, color=\"green\", linestyle=\"--\", label=f\"Mean: {mean:.2f}\")\n",
    "    ax_hist2.axvline(median, color=\"black\", linestyle=\"-\", label=f\"Median: {median:.2f}\")\n",
    "    ax_hist2.legend()\n",
    "\n",
    "    # Ensure the x-axis starts at 0 (optional, adjust as needed)\n",
    "    ax_hist2.set_xlim(left=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_categorical_distributions(data, categorical_columns, cols_per_row=3, figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Plots the distribution of categorical columns in the given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The dataset containing categorical columns.\n",
    "    - categorical_columns (list): List of categorical column names to plot.\n",
    "    - cols_per_row (int): Number of subplots per row. Default is 3.\n",
    "    - figsize (tuple): Base size of the figure. Default is (15, 5).\n",
    "\n",
    "    Returns:\n",
    "    - None: Displays the plots.\n",
    "    \"\"\"\n",
    "    # Determine the number of rows and columns for subplots\n",
    "    num_cols = len(categorical_columns)\n",
    "    num_rows = math.ceil(num_cols / cols_per_row)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_rows, cols_per_row, figsize=(figsize[0], figsize[1] * num_rows))\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "    # Plot each categorical column\n",
    "    for i, col in enumerate(categorical_columns):\n",
    "        sns.histplot(data, x=col, ax=axes[i], color='green')  # Use the corresponding subplot axis\n",
    "        axes[i].set_title(f\"Histogram for {col}\")\n",
    "        total = len(data[col])\n",
    "\n",
    "        # Add percentage annotations\n",
    "        for bar in axes[i].patches:\n",
    "            height = bar.get_height()\n",
    "            percentage = (height / total) * 100  # Calculate percentage\n",
    "            axes[i].text(bar.get_x() + bar.get_width() / 2, height, f'{percentage:.1f}%', \n",
    "                         ha='center', va='bottom', fontsize=10)\n",
    "        sns.despine()\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    # Adjust layout and display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_binned_features(data, features, status, bins=5, title=None):\n",
    "    \"\"\"\n",
    "    Plots subplots for binned bar charts of multiple features against a status variable.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing the features and status.\n",
    "        features (list): A list of continuous variables to be binned and plotted.\n",
    "        status (str): The variable to sum for each bin, plotted on the y-axis.\n",
    "        bins (int): Number of bins to create for the features.\n",
    "        title (str, optional): Title of the overall plot. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the subplots.\n",
    "    \"\"\"\n",
    "    # Calculate the grid dimensions\n",
    "    n_rows, n_cols = 6, 2  # Fixed number of rows and columns\n",
    "    total_subplots = n_rows * n_cols\n",
    "    num_features = len(features)\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))\n",
    "    fig.suptitle(title if title else f\"Features vs {status}\", fontsize=16)\n",
    "\n",
    "    # Flatten axes for easy indexing\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Iterate over features and create plots\n",
    "    for i, feature in enumerate(features):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Group by feature and calculate the sum of the status variable\n",
    "        grouped_data = data.groupby(feature)[status].sum().reset_index()\n",
    "\n",
    "        # Create bins for the feature\n",
    "        grouped_data['feature_bin'] = pd.cut(grouped_data[feature], bins=bins)\n",
    "\n",
    "        # Group by bins and calculate the sum of the status variable\n",
    "        binned_data = grouped_data.groupby('feature_bin', observed=True)[status].sum().reset_index()\n",
    "\n",
    "        # Generate bin labels\n",
    "        bin_edges = grouped_data['feature_bin'].cat.categories\n",
    "        bin_labels = [f\"{int(interval.left)}-{int(interval.right)}\" for interval in bin_edges]\n",
    "\n",
    "        # Bar plot\n",
    "        sns.barplot(data=binned_data, x='feature_bin', y=status, ax=ax, color='green')\n",
    "\n",
    "        # Add title and labels\n",
    "        ax.set_title(f\"{feature} vs {status}\")\n",
    "        ax.set_xlabel(f\"{feature}\")\n",
    "        ax.set_ylabel(f\"{status}\")\n",
    "\n",
    "        # Set x-axis tick labels\n",
    "        ax.set_xticks(range(len(bin_labels)))\n",
    "        ax.set_xticklabels(bin_labels, rotation=90)\n",
    "        total = grouped_data[status].sum()\n",
    "        for bar in ax.patches:\n",
    "            height = bar.get_height()\n",
    "            percentage = (height / total) * 100  # Calculate percentage\n",
    "            if percentage > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width() / 2, height, f'{percentage:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "        sns.despine()\n",
    "\n",
    "    # Turn off extra subplots\n",
    "    for i in range(num_features, total_subplots):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importances_or_coefficients(model, X, model_type=\"general\"):\n",
    "    \"\"\"\n",
    "    Plots feature importances or coefficients for a given model, \n",
    "    accounting for positive and negative coefficients in logistic regression.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained machine learning model.\n",
    "        X (pd.DataFrame): Feature DataFrame used for training the model.\n",
    "        model_type (str): Specify \"logistic_regression\" for logistic regression models \n",
    "                          or \"general\" for models with feature_importances_.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays a bar chart of feature importances or coefficients.\n",
    "    \"\"\"\n",
    "    # Retrieve importances or coefficients based on the model type\n",
    "    if model_type == \"logistic_regression\":\n",
    "        if hasattr(model, \"coef_\"):\n",
    "            importances = model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_\n",
    "            title = \"Feature Coefficients (Logistic Regression)\"\n",
    "        else:\n",
    "            raise ValueError(\"The provided model does not support coefficients.\")\n",
    "    elif hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "        title = \"Feature Importances\"\n",
    "    else:\n",
    "        raise ValueError(\"The provided model does not support feature importances or coefficients.\")\n",
    "\n",
    "    # Create DataFrame for sorting and plotting\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False, key=abs if model_type == \"logistic_regression\" else None)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['green' if val > 0 else 'red' for val in importance_df['Importance']]  # Positive/negative colors\n",
    "    plt.bar(importance_df['Feature'], importance_df['Importance'], color=colors, align='center')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance' if model_type != \"logistic_regression\" else 'Coefficient')\n",
    "    plt.title(title)\n",
    "    plt.axhline(0, color='black', linewidth=0.7, linestyle='--')  # Zero line for logistic regression\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_subset_features(model, x_test, y_test, subset='false_negatives'):\n",
    "    \"\"\"\n",
    "    Analyzes feature values for a specific subset of classification results (e.g., false negatives or false positives),\n",
    "    and aligns the color palette with the feature importances function.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained classification model.\n",
    "        x_test (pd.DataFrame): Test set features.\n",
    "        y_test (pd.Series): Test set labels.\n",
    "        subset (str): Subset to analyze. Options:\n",
    "                      - 'false_negatives': Predicted 0, Actual 1\n",
    "                      - 'false_positives': Predicted 1, Actual 0\n",
    "\n",
    "    Returns:\n",
    "        None: Displays a bar chart of mean feature values for the specified subset.\n",
    "    \"\"\"\n",
    "    # Predict probabilities and labels\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    # Filter data based on the selected subset\n",
    "    if subset == 'false_negatives':\n",
    "        subset_df = x_test[(y_pred == 0) & (y_test == 1)]\n",
    "        title = \"Mean Feature Values for False Negatives\"\n",
    "    elif subset == 'false_positives':\n",
    "        subset_df = x_test[(y_pred == 1) & (y_test == 0)]\n",
    "        title = \"Mean Feature Values for False Positives\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid subset option. Choose 'false_negatives' or 'false_positives'.\")\n",
    "\n",
    "    # Calculate mean feature values\n",
    "    subset_features = subset_df.mean().sort_values(ascending=False)\n",
    "\n",
    "    # Apply colors based on positive/negative values\n",
    "    colors = ['green' if val > 0 else 'red' for val in subset_features.values]\n",
    "\n",
    "    # Plot mean feature values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(subset_features.index, subset_features.values, color=colors, align='center')\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('Mean Value', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.axhline(0, color='black', linewidth=0.7, linestyle='--')  # Zero line for reference\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chi_squared_test(data, target, bins=5):\n",
    "    \"\"\"\n",
    "    Perform the Chi-Squared test for independence between all features\n",
    "    and the target variable, handling both categorical and continuous features.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The input dataset.\n",
    "        target (str): The name of the target variable.\n",
    "        bins (int): Number of bins to discretize continuous features. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing features, their P-values, \n",
    "                      and whether they are significant.\n",
    "    \"\"\"\n",
    "    # Separate target column\n",
    "    target_data = data[target]\n",
    "    features = data.drop(columns=[target], errors=\"ignore\")\n",
    "\n",
    "    # List to store results for each feature\n",
    "    results = []\n",
    "\n",
    "    for feature in features.columns:\n",
    "        if features[feature].dtype in ['object', 'category']:\n",
    "            # Categorical feature: Use directly\n",
    "            feature_data = features[feature]\n",
    "        else:\n",
    "            # Continuous feature: Discretize into bins\n",
    "            feature_data = pd.cut(features[feature], bins=bins, labels=False)\n",
    "\n",
    "        # Create a contingency table for the current feature and target\n",
    "        contingency_table = pd.crosstab(feature_data, target_data)\n",
    "\n",
    "        # Perform the Chi-Squared test for independence\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "        # Append the feature and p-value to the results list\n",
    "        results.append({'Feature': feature, 'P-value': p, 'Significant': p < 0.05})\n",
    "\n",
    "    # Convert results to a DataFrame for better visualization\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Sort the results by P-value in ascending order\n",
    "    sorted_results = results_df.sort_values(by='P-value', ascending=True)\n",
    "\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9ykJzCRuTCD"
   },
   "source": [
    "## **Data Overview**\n",
    "- Reading the dataset\n",
    "- Understanding the shape of the dataset\n",
    "- Checking the data types\n",
    "- Checking for missing values\n",
    "- Checking for duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "za7znZ1cBZtg"
   },
   "outputs": [],
   "source": [
    "# Import data from CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('../data/hmeq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the original dataframe for processing, so the original data stays intact\n",
    "default = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update column names for improved readability\n",
    "default.rename(columns={\n",
    "    'BAD': 'client_defaulted_on_loan',  # 1 = Client defaulted on loan, 0 = loan repaid\n",
    "    'LOAN': 'loan_amount_approved',      # Amount of loan approved\n",
    "    'MORTDUE': 'mortgage_amount_due',    # Amount due on the existing mortgage\n",
    "    'VALUE': 'property_current_value',    # Current value of the property\n",
    "    'REASON': 'loan_request_reason',      # Reason for the loan request\n",
    "    'JOB': 'applicant_job_type',          # Type of job that loan applicant has\n",
    "    'YOJ': 'years_at_present_job',        # Years at present job\n",
    "    'DEROG': 'major_derogatory_reports',  # Number of major derogatory reports\n",
    "    'DELINQ': 'delinquent_credit_lines',  # Number of delinquent credit lines\n",
    "    'CLAGE': 'age_of_oldest_credit_line',  # Age of the oldest credit line in months\n",
    "    'NINQ': 'recent_credit_inquiries',     # Number of recent credit inquiries\n",
    "    'CLNO': 'existing_credit_lines',       # Number of existing credit lines\n",
    "    'DEBTINC': 'debt_to_income_ratio'      # Debt-to-income ratio\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the shape of the dataset\n",
    "default.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the summarize_dataframe function and display the summary\n",
    "summarize_dataframe(default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates and drop if found\n",
    "num_duplicates = default.duplicated().sum()  # Count duplicates\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "    default.drop_duplicates(keep='first', inplace=True)  # Drop duplicates, keeping the first\n",
    "    print(len(df) - len(default), \"Duplicate rows dropped (keeping the first occurrence).\")\n",
    "    default.reset_index(drop=True, inplace=True)  # Reset index after dropping duplicates\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each non-numeric column and print its unique values\n",
    "for column in default.select_dtypes(include=['object']):\n",
    "    print(f\"{column}: {default[column].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check any inconsitencies in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for inconsisties in the data\n",
    "# Typical mortgage to property value ratio is ~ 0.8. Lets confirm it\n",
    "default['ltv'] = default['mortgage_amount_due'] / default['property_current_value']  # create a column ltv (loan to value ratio)\n",
    "high_ltv = default[default['ltv'] >= 2]  # Create a temp dataframe of rows with ltv > 2\n",
    "\n",
    "# Define bins as integer steps from the min to max value\n",
    "bins = np.arange(int(high_ltv['ltv'].min()), int(high_ltv['ltv'].max()) + 2) - 0.5\n",
    "\n",
    "# Plot the count of clients with ltv greater than 2.\n",
    "plot_histogram(\n",
    "    data=high_ltv,\n",
    "    column='ltv',\n",
    "    bins=bins,\n",
    "    xlabel='LTV Value',\n",
    "    ylabel='Count',\n",
    "    title='Histogram of LTV with Integer Bins'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all records with ltv greater than or equal to 2 and the ltv column as it is no longer necessary\n",
    "default.drop(default[default['ltv'] >= 2].index, axis=0, inplace=True)\n",
    "default.drop('ltv', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the temporary dataframe\n",
    "del high_ltv\n",
    "# Trigger garbage collection and print the number of objects collected\n",
    "collected_objects = gc.collect()\n",
    "print(f\"Garbage collector freed {collected_objects} objects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "1. Dataset contains several missing values. These need to be treated individually as the use of summary statistic may not be appropriate or sufficient for this this dataset i.e. morgage_amount_due can have missing values,but loan_request_reason or property_current_value cannot be. Also many of the missing values may depend on values in other columns, such as major_derogatory_report, delinquent_credit_lines, recent_credit_inquiries, existing_credit_lines and debt_to_income_ratio are all correlated\n",
    "2. Features like debt_to_income_ratio and major_derogatory_reports have significant missing values (21.3% and 11.9%, respectively)\n",
    "4. Dataset has 5960 rows and 13 columns (12 independant features)\n",
    "5. Data is for approved loans only, as there are no missing values in the loan_amount_approved\n",
    "6. No duplicate records, which is understandable, as most individuals or families requesting for loan are unique\n",
    "7. All categorical values look clean - no typos, unique values and consistent labeling\n",
    "8. Many of the features should be integrers, but are floats in the dataset - major_derogatory_reports, delinquent_credir_lines, recent_credit_inquiries and existing_credit_lines. If the actual values, indeed are floats, its a data descripency which should be addresses before further processing\n",
    "9. Proportion of defaulting client is 20%, which is too high by industry standards, indicating a highly biased dataset\n",
    "10. Several records had a very high loan to value ratio. This is quite unusual. All the records with ltv > 2.0 were dropped\n",
    "11. Also, the features applicant job type and years at present job, may invite discimatory litigation (class action lawsuits)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHTODkjLuTCT"
   },
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the key statistics of the data, transposed for better readability\n",
    "default.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "1. Skewness: major_derogatory_reports, delinquent_credit_lines, debt_to_income_ratio are concentrated near zero with a long tail, indicating skewness\n",
    "2. Outliers: mortgage_amount_due, property_current_value, age_of_oldest_credit_line, and debt_to_income_ratio extreme high max values, with 75% percentile closer to the mean, indicating outliers.\n",
    "\n",
    "Vizualization of the distributions of these features will provide clarity on these observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZcMbNvZuTCW"
   },
   "source": [
    "## **Exploratory Data Analysis (EDA) and Visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Td9x7DdBZti"
   },
   "source": [
    "- EDA is an important part of any project involving data.\n",
    "- It is important to investigate and understand the data better before building a model with it.\n",
    "- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.\n",
    "- A thorough analysis of the data, in addition to the questions mentioned below, should be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba_dqV0sBZti"
   },
   "source": [
    "**Leading Questions**:\n",
    "1. What is the range of values for the loan amount variable \"LOAN\"?\n",
    "2. How does the distribution of years at present job \"YOJ\" vary across the dataset?\n",
    "3. How many unique categories are there in the REASON variable?\n",
    "4. What is the most common category in the JOB variable?\n",
    "5. Is there a relationship between the REASON variable and the proportion of applicants who defaulted on their loan?\n",
    "6. Do applicants who default have a significantly different loan amount compared to those who repay their loan?\n",
    "7. Is there a correlation between the value of the property and the loan default rate?\n",
    "8. Do applicants who default have a significantly different mortgage amount compared to those who repay their loan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data prep for EDA and subsequent processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing value treatement\n",
    "I have used techniques from recommandation systems to impute the missing values **(Reference: \"Missing Data: A Gentle Introduction\" by Patrick E. McKnight, Katherine M. McKnight, Souraya Sidani, and Aurelio José Figueredo)**. Below is a summary of methods used for imputatation\n",
    "\n",
    "**1. KNN Imputation (Low/Moderate Missingness)**\n",
    "\n",
    "* mortgage_amount_due, property_current_value, loan_request_reason, applicant_job_type, age_of_oldest_credit_line, existing_credit_line columns have less than 10% missing values. KNN imputation is effective for low-to-moderate missingness by finding similar rows based on available features and averaging/matching the corresponding feature values.\n",
    "\n",
    "**2. Iterative Imputer (High Missingness)**\n",
    "\n",
    "* debt_to_income_ratio, major_derogatory_reports, delinquentt_credit_line, years_at_present_job, recent_credit_inquirie columns have higher missing percentages (10–21%). Iterative imputation models each feature as a function of the others, iteratively predicting missing values to best fit the observed data.\n",
    "\n",
    "**3. Encoding for Categorical Data**\n",
    "\n",
    "* loan_request_reason, applicant_job_type columns contain categorical data. They were encoded using LabelEncoder to convert string values into numeric categories, allowing compatibility with KNN imputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLk0Fgx-BZtj"
   },
   "outputs": [],
   "source": [
    "# Columns for KNN imputation\n",
    "columns_knn = [\n",
    "    'mortgage_amount_due', 'property_current_value',\n",
    "    'loan_request_reason', 'applicant_job_type',\n",
    "    'age_of_oldest_credit_line', 'existing_credit_lines'\n",
    "]\n",
    "# Columns for iterative imputation\n",
    "columns_iterative = [\n",
    "        'debt_to_income_ratio', 'major_derogatory_reports',\n",
    "        'delinquent_credit_lines', 'years_at_present_job',\n",
    "        'recent_credit_inquiries'\n",
    "    ]\n",
    "# Categorical Columns\n",
    "categorical_columns = ['loan_request_reason', 'applicant_job_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLk0Fgx-BZtj"
   },
   "outputs": [],
   "source": [
    "# Treat missing values using the custom function 'impute_missing_values', which implements the KNN and Iterative imputation based on % missing values for each column\n",
    "default_imputed_num, categories = impute_missing_values(default, columns_knn, columns_iterative, categorical_columns)\n",
    "\n",
    "# Upon inspection of the dataset after imputation, it is observed that in some of the rows, property value and mortgage due were imputed to 0. These zero values were set to the median value of the column\n",
    "for col in ['property_current_value', 'mortgage_amount_due']:\n",
    "    median_value = default_imputed_num[col].median()\n",
    "    default_imputed_num.loc[default_imputed_num[col] == 0, col] = median_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLk0Fgx-BZtj"
   },
   "outputs": [],
   "source": [
    "# Create a copy of the imputed data in numeric format (could be of use later!)\n",
    "default_imputed = default_imputed_num.copy()\n",
    "\n",
    "# Convert categorical columns back to original categories by using the saved original categories\n",
    "for col in ['loan_request_reason', 'applicant_job_type']:\n",
    "    default_imputed[col] = pd.Categorical.from_codes(default_imputed_num[col].round().astype(int), categories=categories[col])\n",
    "# Confirm no more missing values\n",
    "summarize_dataframe(default_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T14:44:26.475460Z",
     "iopub.status.busy": "2024-12-08T14:44:26.475340Z",
     "iopub.status.idle": "2024-12-08T14:44:26.494068Z",
     "shell.execute_reply": "2024-12-08T14:44:26.480102Z",
     "shell.execute_reply.started": "2024-12-08T14:44:26.475446Z"
    }
   },
   "source": [
    "#### Prepare data for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of columns by dtype(s)\n",
    "col_types = segregate_columns_by_dtype(default_imputed)\n",
    "\n",
    "# get numeric columns from the column type list\n",
    "num_cols = col_types['float64'] + col_types['int64']\n",
    "\n",
    "# Remove target column\n",
    "num_cols.remove('client_defaulted_on_loan')\n",
    "\n",
    "# get categorical columns from the column type list\n",
    "cat_cols = col_types['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65yxFJFVuTCW"
   },
   "source": [
    "### **Univariate Analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the numeric variables as histograms and box plots to check skewness and outliers\n",
    "histogram_boxplot(default_imputed, num_cols[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Mortgage amount due shows a slightly right skewed distribution, but with a lot of outliers. These may need to be treated based on the type of modelling technique used for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the numeric variables as histograms and box plots to check skewness and outliers\n",
    "histogram_boxplot(default_imputed, num_cols[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Current property value shows a slightly right skewed distribution, but with a lot of outliers. These may need to be treated based on the type of modelling technique used for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the numeric variables as histograms and box plots to check skewness and outliers\n",
    "histogram_boxplot(default_imputed, num_cols[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "No of years at present job shows a moderately right skewed, multi-modal distribution, with a few outliers, indicating most loan requesters maybe in middle age group. Extreme outliers may have to be capped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the numeric variables as histograms and box plots to check skewness and outliers\n",
    "histogram_boxplot(default_imputed, num_cols[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "Major derogatory reports shows a highly right skewed distribution, with most values around zero. This may indicate a bias in the prescreening process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the numeric variables as histograms and box plots to check skewness and outliers\n",
    "histogram_boxplot(default_imputed, num_cols[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "Delinquent credit lines shows a highly right skewed distribution, with most values around zero. This may indicate a bias in the prescreening process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the numeric variables as histograms and box plots to check skewness and outliers\n",
    "histogram_boxplot(default_imputed, num_cols[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Oldest credit line shows fairly uniform bi-modal distribution, with quite a few outliers. What maybe of interest here are the loan requesters with low or zero age of credit line. Also extreme outliers may have to be capped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the numeric variables as histograms and box plots to check skewness and outliers\n",
    "histogram_boxplot(default_imputed, num_cols[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "Recent credit inquiries shows a highly right skewed distribution, with most values around zero. This may indicate a bias in the prescreening process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the numeric variables as histograms and box plots to check skewness and outliers\n",
    "histogram_boxplot(default_imputed, num_cols[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "Existing credit lines shows a reasonably uniform distribution with slight right tail and a few outliers. Extreme outliers may need to be capped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the numeric variables as histograms and box plots to check skewness and outliers\n",
    "histogram_boxplot(default_imputed, num_cols[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "Debt to income ratio is slightly left skewed, with outliers on both sides. Extreme outlier on the high may need capping. Left skewness is a good thing as a low debt to income ratio is desirable\n",
    "The distribution is also showing a sharp drop around ~ 40. Should investigate this further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the numeric variables as histograms and box plots to check skewness and outliers\n",
    "histogram_boxplot(default_imputed, num_cols[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Amount of loan approved shows a slightly right skewed distribution, but with a lot of outliers. These may need to be treated based on the type of modelling technique used for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of categorical columns\n",
    "plot_categorical_distributions(data=default_imputed, categorical_columns=categorical_columns, cols_per_row=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "1. ~ 70% of requests are for debt consolidation, indication a fundamental bias in the dataset, or biased selection process\n",
    "2. Majority (~ 45%) of the loan requests are from people who did not provide details of the profession, suggesting improvements in the data collection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg5IFtbouTCa"
   },
   "source": [
    "### **Bivariate Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correlation for numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = default_imputed[num_cols].corr()\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(16, 8))  # Adjust figure size if needed\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='rainbow_r', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort correlations and exclude duplicates\n",
    "sorted_unique_correlations = (\n",
    "    correlation_matrix\n",
    "    .stack()                            # Flatten the matrix into a Series\n",
    "    .reset_index()                      # Reset index to access column pairs\n",
    "    .rename(columns={0: 'correlation'})  # Rename correlation column\n",
    "    .query(\"level_0 < level_1\")         # Keep only one direction (unique pairs)\n",
    "    .sort_values(by='correlation', ascending=False)  # Sort by correlation\n",
    ")\n",
    "# Print the columns with correlation values\n",
    "print(sorted_unique_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b60NSTMBZtj"
   },
   "source": [
    "#### Observations:\n",
    "1. Property value as of date seems to have a strong +ve correlation to mortgage amount due. This is only possible if all if the mortgages in the dataset were approved around the same time and the approved mortgage amount was a fixed % of the property value, irrespective of other factors. So age of the mortgage may be a useful feature (latent variable). Most other variables have weak correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check the distribution of numerical variables to loan defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binned_features(default_imputed, num_cols, 'client_defaulted_on_loan', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "1. No of defaulters are highest where recent credit inquiries, delinquent credit lines, major derogatory reports are the lowest. This seems counter intuitive at first, how ever this is most likely due to these customers not having any credit records at all. This will happen if these customers were denied credit in the past and have not applied for credit in the recent time (for the duration during which this data was collected). Additional data is required to understand this anamoly - gross income, demographics, macroeconomic factors such severe recession or other financial distress\n",
    "2. Default rate is the extra-ordinarily high (80%) for customers with debt to income ratio between 30-41. Understanding this anomaly likely requires a more detailed analysis of the borrower behavior and external conditions. This data set is insufficient for such analysis, as was commented in no 1 above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check the distribution of categorical variables to loan defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the categorical variable against the target variable\n",
    "plot_grouped_barplots(default_imputed, cat_cols, 'client_defaulted_on_loan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "1. As expected, loan defaults are higher for clients who requested loan for debt consolidation, as these clients already had substantial debt and most likely not enough income to cover the debt\n",
    "2. Applicants with job type as 'other' have the highest default rate. This may be because these clients do not have a steady job and as such chose not to disclose it. Also the higher default rate for 'Mgr' and 'ProfExe' is interesting and may need further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Chi-squared test to see if any of the features are not significant to predicting the target variable\n",
    "print(chi_squared_test(default_imputed, 'client_defaulted_on_loan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "1. All the available features are significant for the prediction of target variable\n",
    "2. Also, features related to clients credit posture seem to have higher significance, which is expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix and VIF, excluding the target variable\n",
    "corr_matrix, vif_values = calculate_corr_and_vif(default_imputed, 'client_defaulted_on_loan')\n",
    "\n",
    "# Display results\n",
    "print(\"Correlation Matrix:\")\n",
    "\n",
    "sorted_corr_matrix = corr_matrix[corr_matrix.columns[corr_matrix.iloc[0].argsort()[::-1]]]\n",
    "\n",
    "sorted_corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print VIF values\n",
    "print(vif_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "current property value shows a very high correlation with mortgage amount due, which is expected. These should be replaced with an engineered feature. Additional insights can be gained from priniciple component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc9wZJcGuTCm"
   },
   "source": [
    "### **Multivariate Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate analysis using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T15:31:22.414095Z",
     "iopub.status.busy": "2024-12-08T15:31:22.413194Z",
     "iopub.status.idle": "2024-12-08T15:31:22.419505Z",
     "shell.execute_reply": "2024-12-08T15:31:22.418365Z",
     "shell.execute_reply.started": "2024-12-08T15:31:22.414031Z"
    }
   },
   "source": [
    "#### Prep the data for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data using the dataset with all numeric values from the impute missing values function output (PCA works only with numeric values)\n",
    "default_imputed_scaled = default_imputed_num.drop('client_defaulted_on_loan', axis=1) # Drop the target variable\n",
    " # Scale the data using Z transformation\n",
    "scaler = StandardScaler()\n",
    "default_imputed_scaled = pd.DataFrame(scaler.fit_transform(default_imputed_scaled), columns=default_imputed_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=10)  # Reduce to 10 principal components\n",
    "default_pca = pd.DataFrame(pca.fit_transform(default_imputed_scaled), columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cumulative variance explained by each principle component for \n",
    "plot_pca(pca)\n",
    "# Print explained variance by each component\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio_, start=1):\n",
    "    print(f\"PC{i}: {ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "There is no single component which can be attributed with a large variance in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the contribution of each of the original features to the principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to represent the principal component loadings\n",
    "# Rows are the principal components (PCs), and columns are the original features\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_,  # PCA component weights\n",
    "    columns=default_imputed_scaled.columns,  # Original feature names\n",
    "    index=[f'PC{i+1}' for i in range(len(pca.components_))]  # Label PCs as PC1, PC2, etc.\n",
    ")\n",
    "\n",
    "# Sort the loadings of the first principal component (PC1) in descending order\n",
    "# loadings.iloc[0] refers to the first row (PC1), and argsort()[::-1] sorts indices in descending order\n",
    "sorted_loadings = loadings[loadings.columns[loadings.iloc[0].argsort()[::-1]]]\n",
    "\n",
    "# Display the sorted loadings\n",
    "sorted_loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new feature to replace Mortgage due and property value as they are highly correlated, and check if it impacts the PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_imputed_scaled_eng = engineer_and_scale_features(\n",
    "    data=default_imputed_scaled,\n",
    "    ratio_cols=('mortgage_amount_due', 'property_current_value'),\n",
    "    new_feature_name='ltv',\n",
    "    drop_original=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca_nopv = PCA(n_components=10)  # First 10 PCs\n",
    "default_pcanopv = pd.DataFrame(pca_nopv.fit_transform(default_imputed_scaled_eng), columns=['PCe1', 'PCe2', 'PCe3', 'PCe4', 'PCe5', 'PCe6', 'PCe7', 'PCe8', 'PCe9', 'PCe10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot explained variance ratio\n",
    "plot_pca(pca_nopv)\n",
    "\n",
    "# Print explained variance by each component\n",
    "for i, ratio in enumerate(pca_nopv.explained_variance_ratio_, start=1):\n",
    "    print(f\"PCe{i}: {ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings_noPV = pd.DataFrame(pca_nopv.components_, columns=default_imputed_scaled_eng.columns, index=[f'PCe{i+1}' for i in range(len(pca_nopv.components_))])\n",
    "sorted_loadings_noPV = loadings_noPV[loadings_noPV.columns[loadings_noPV.iloc[0].argsort()[::-1]]]\n",
    "sorted_loadings_noPV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "Based on PCA analysis, it is clear that replacing 'mortgage_amount_due', 'property_current_value' by 'mortgage_to_property_ratio' will not impact modelling performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEjMlq0quTCp"
   },
   "source": [
    "## Treating Outliers\n",
    "Only the outliers for mortgage due and property value have been dropped, where the ratio of these features was improbable. See section to 1.4.0.1. All the other outliers are maintained as is as they seem to be realistic data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8CEq24hBZtj"
   },
   "source": [
    "## Treating Missing Values: See section 1.6.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG_XM04vuTCs"
   },
   "source": [
    "## **Important Insights from EDA**\n",
    "\n",
    "### Key Data Insights for Classification\n",
    "\n",
    "#### Handling Missing Values\n",
    "- Critical features like `loan_request_reason` and `property_current_value` must have no missing values as they are essential for predicting loan defaults.\n",
    "- Features like `major_derogatory_reports` and `debt_to_income_ratio` (with missing rates of 11.9% and 21.3%) require imputation or exclusion based on their impact.\n",
    "\n",
    "#### Dataset Characteristics\n",
    "- **Records**: 5960 with no duplicates.\n",
    "- **Default Rate**: High (20%), indicating no class imbalance but potential dataset biases.\n",
    "- **Target Variable**: Well-defined (`loan_default`).\n",
    "\n",
    "#### Potential Predictors\n",
    "- Features such as `debt_to_income_ratio`, `major_derogatory_reports`, `recent_credit_inquiries`, and `existing_credit_lines` are likely predictive but require normalization and outlier treatment.\n",
    "\n",
    "#### Feature Engineering\n",
    "- Replace correlated variables (`mortgage_amount_due` and `property_current_value`) with `mortgage_to_property_ratio`.\n",
    "- Consider creating latent features such as \"age of mortgage\" or \"credit history duration\" for added predictive power.\n",
    "\n",
    "#### Skewness and Outliers\n",
    "- Many numerical features (e.g., `debt_to_income_ratio`, `mortgage_amount_due`) are right-skewed with significant outliers.\n",
    "- Apply transformations and outlier capping to improve model robustness.\n",
    "\n",
    "#### Bias and Feature Validity\n",
    "- Categorical variables like `applicant_job_type` and `loan_request_reason` show inherent bias (e.g., 70% of loans are for debt consolidation).\n",
    "- Features like `job_type` and `years_at_present_job` may raise ethical or legal concerns.\n",
    "\n",
    "#### Class Separation and Patterns\n",
    "- Default rates are high for clients with minimal credit records or specific debt-to-income ranges (30–41%).\n",
    "- This highlights actionable risk segments.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Statistical Insights\n",
    "\n",
    "#### Correlations\n",
    "- Strong correlation between `property_current_value` and `mortgage_amount_due` supports dimensionality reduction.\n",
    "- Weak correlations across most features suggest the need for engineered features or interaction terms.\n",
    "\n",
    "#### Categorical Variable Impact\n",
    "- High default rates for specific `job_type` categories (`other`, `Mgr`, `ProfExe`) suggest profession-based insights could improve classification if handled carefully.\n",
    "\n",
    "---\n",
    "\n",
    "### Recommendations for Modeling\n",
    "\n",
    "#### Preprocessing\n",
    "- Treat missing values based on correlation and domain importance.\n",
    "- Normalize or transform skewed features and cap outliers.\n",
    "- Create engineered features (e.g., `mortgage_to_property_ratio`).\n",
    "\n",
    "#### Bias Mitigation\n",
    "- Address potential dataset bias (e.g., over-representation of debt consolidation loans).\n",
    "- Carefully handle legally sensitive features like `job_type` to avoid bias in classification.\n",
    "\n",
    "#### Feature Selection\n",
    "- Prioritize credit-related features (`debt_to_income_ratio`, `recent_credit_inquiries`, `delinquent_credit_lines`) given their significance for predicting defaults.\n",
    "\n",
    "#### Advanced Analysis\n",
    "- Investigate latent variables and external data (e.g., economic conditions) to explain observed anomalies in defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cdqhtr8yuS-L"
   },
   "source": [
    "## **Model Building - Approach**\n",
    "- Data preparation\n",
    "- Partition the data into train and test set\n",
    "- Build the model\n",
    "- Fit on the train data\n",
    "- Tune the model\n",
    "- Test the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fs0QKb4ABZtk"
   },
   "outputs": [],
   "source": [
    "# Create a dataframe for modeling from original imputed dataframe\n",
    "default_model = copy.deepcopy(default_imputed)\n",
    "default_model['ltv'] = default_model['mortgage_amount_due']/default_model['property_current_value']  # Create the new ltv feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_model.drop(columns=['mortgage_amount_due', 'property_current_value'], inplace=True)  # Drop the original feature used to create ltv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_model = engineer_and_scale_features(\n",
    "    data=default_imputed,\n",
    "    ratio_cols=('mortgage_amount_due', 'property_current_value'),\n",
    "    new_feature_name='ltv',\n",
    "    drop_original=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the categorical variables using one-hot encoding\n",
    "default_model = pd.get_dummies(default_model, columns=['loan_request_reason', 'applicant_job_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target variable\n",
    "Y = default_model.client_defaulted_on_loan\n",
    "X = default_model.drop('client_defaulted_on_loan', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training anb testing datasets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled, Y, test_size = 0.3, random_state = 1, stratify = Y)\n",
    "# Reset index of test set for alignment\n",
    "x_test = x_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scoring metrics to be optimized during GridSearch\n",
    "optimum = ['accuracy', 'f1', 'recall', 'precision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: inline-block\">\n",
    "    \n",
    "| Actual/Predicted       | Predicted: 0 (No Default) | Predicted: 1 (Default)|\n",
    "|:-----------------------|:--------------------------|:----------------------|\n",
    "| Actual: 0 (No Default) | True Negative (TN)        | False Positive (FP)   |\n",
    "| Actual: 1 (Default)    | False Negative (FN)       | True Positive (TP)    |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification model should maximize the expected value of the revenue from lending. In order to acheive this:\n",
    "1. Maximize True Negatives - This will bring in the bulk of the interest revenue\n",
    "2. Minimize False Negatives - This will reduce the interest loss due to defaulted loans (principle can be recouped via foreclosure)\n",
    "3. Minimize False Positives - This will minimize the opportunity loss\n",
    "To achieve this, we will define 2 functions\n",
    "### Function to calculate the expected revenue:\n",
    "\n",
    "$\n",
    "\\text{Expected Revenue} = \\text{Revenue from True Negatives (TN)} - \\text{Loss from False Positives (FP)} - \\text{Loss from False Negatives (FN)}\n",
    "$\n",
    "\n",
    "1. Revenue from True Negatives (TN):\n",
    "\n",
    "$\n",
    "\\text{Revenue}{\\text{TN}} = P{\\text{ND}} \\cdot (1 - \\text{Actual}) \\cdot L \\cdot r\n",
    "$\n",
    "Where:\n",
    "\t•\t$P_{\\text{ND}} = 1 - P_{\\text{D}}$: Probability of non-default.\n",
    "\t•\t$\\text{Actual}$: Indicator of the actual outcome ($0$ = non-default, $1$ = default).\n",
    "\t•\t$L$: Loan amount.\n",
    "\t•\t$r$: Interest rate.\n",
    "\n",
    "2. Loss from False Positives (FP):\n",
    "\n",
    "$\n",
    "\\text{Loss}{\\text{FP}} = P{\\text{D}} \\cdot (1 - \\text{Actual}) \\cdot L \\cdot r\n",
    "$\n",
    "Where:\n",
    "\t•\t$P_{\\text{D}}$: Probability of default.\n",
    "\t•\t$(1 - \\text{Actual})$: Ensures the outcome is non-default ($\\text{Actual} = 0$).\n",
    "\t•\t$L \\cdot r$: Opportunity cost (lost interest revenue).\n",
    "\n",
    "3. Loss from False Negatives (FN):\n",
    "\n",
    "$\n",
    "\\text{Loss}{\\text{FN}} = P{\\text{ND}} \\cdot \\text{Actual} \\cdot L \\cdot (1 + r)\n",
    "$\n",
    "Where:\n",
    "\t•\t$P_{\\text{ND}}$: Probability of non-default.\n",
    "\t•\t$\\text{Actual}$: Ensures the outcome is default ($\\text{Actual} = 1$).\n",
    "\t•\t$L \\cdot (1 + r)$: Total loss (principal + interest).\n",
    "\n",
    "Final Formula:\n",
    "\n",
    "$\n",
    "\\text{Expected Revenue} = \\left( P_{\\text{ND}} \\cdot (1 - \\text{Actual}) \\cdot L \\cdot r \\right)\n",
    "\t•\t\\left( P_{\\text{D}} \\cdot (1 - \\text{Actual}) \\cdot L \\cdot r \\right)\n",
    "\t•\t\\left( P_{\\text{ND}} \\cdot \\text{Actual} \\cdot L \\cdot (1 + r) \\right)\n",
    "$\n",
    "\n",
    "Definitions:\n",
    "\t•\t$P_{\\text{D}}$: Probability of default.\n",
    "\t•\t$P_{\\text{ND}} = 1 - P_{\\text{D}}$: Probability of non-default.\n",
    "\t•\t$\\text{Actual}$: Binary variable ($0 = \\text{Non-default}, 1 = \\text{Default}$).\n",
    "\t•\t$L$: Loan amount.\n",
    "\t•\t$r$: Interest rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = pd.DataFrame(columns = ['Model Name', 'Model Type', 'Grid Search (Yes/No)', 'Recall', 'Precision', 'f1', 'Accuracy', 'FPs', 'FNs', 'Revenue Prediction', 'Confidence'])\n",
    "model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CGW7uh5BZtk"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit logistic regression model to training data with default parameters except solver = liblinear, since liblinear solver is optimized for linear problems with small datasets and binary classification\n",
    "lg = LogisticRegression(verbose=1, solver='liblinear', random_state=1)\n",
    "lg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the performance on the training data\n",
    "y_pred_train = lg.predict(x_train)\n",
    "metrics_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the performance on the test data\n",
    "lg_metric_dict = evaluate_model_and_metrics(lg, x_test, y_test, X_scaled, default_model, 'lg', 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics.loc[len(model_metrics)] = lg_metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check which features are driving the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances_or_coefficients(lg, X, 'logistic_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T02:30:59.890525Z",
     "iopub.status.busy": "2024-12-13T02:30:59.889868Z",
     "iopub.status.idle": "2024-12-13T02:30:59.894461Z",
     "shell.execute_reply": "2024-12-13T02:30:59.893674Z",
     "shell.execute_reply.started": "2024-12-13T02:30:59.890489Z"
    }
   },
   "source": [
    "#### Check the features/values driving the False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_subset_features(\n",
    "    model=lg,\n",
    "    x_test=x_test,\n",
    "    y_test=y_test,\n",
    "    subset='false_negatives'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "The performance of the model on the trained data and test data is almost identical.\\\n",
    "Primary driving factors are related to the -ve credit history of the client and the income.\\\n",
    "Loan amount approved and existing credit lines has a negative coefficient.\\\n",
    "\n",
    "Most of the False Negatives are due to more imporant feature(s) having mean near zero.\\\n",
    "We will check if this a pattern across different models.\\\n",
    "\n",
    "Overall the performance of Logistic regression is not satisfactory as the recall is very low.\n",
    "\n",
    "**Let's see if we can improve the performance by gridsearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "log_reg = LogisticRegression(solver='liblinear', random_state=1)\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization techniques\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "    'solver': ['lbfgs', 'liblinear', 'saga'],  # Solvers compatible with l1 and elasticnet penalties\n",
    "    'max_iter': [500, 1000, 2000, 3000]  # Number of iterations for convergence\n",
    "}\n",
    "\n",
    "results = {}  # Dictionary to store all results dynamically\n",
    "\n",
    "# Perform GridSearchCV for each scoring metric\n",
    "for opt in optimum:\n",
    "    print(f\"Optimizing for: {opt}\")\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=log_reg,\n",
    "        param_grid=param_grid,\n",
    "        scoring=opt,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Perform the grid search on the training data\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Store best parameters and cross-validation score\n",
    "    results[opt] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_model': grid_search.best_estimator_\n",
    "    }\n",
    "\n",
    "    print(f\"Best Parameters for {opt}: {results[opt]['best_params']}\")\n",
    "    print(f\"Best Cross-Validation Score for {opt}: {results[opt]['best_score']}\")\n",
    "\n",
    "    # Evaluate the best model and append results\n",
    "    model_metrics.loc[len(model_metrics)] = evaluate_model_and_metrics(results[opt]['best_model'], x_test, y_test, X_scaled, default_model, f'log_reg_{opt}', 'Logistic Regression', grid_search=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Even GridSearch is not able to provide a satisfactory performance, as is evident from recall scores. The best recall score is the same as the model without GridSearch.\\\n",
    "However, the best result is for the model optimized for recall\n",
    "\n",
    "The false positives seem to be primarily driven by credit-related factors (delinquent_credit_lines, debt_to_income_ratio, and major_derogatory_reports) overpowering stabilizing factors like small loan amounts, long credit histories, or stable job types. Addressing these imbalances in the model can improve its predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oYAeptGBZtk"
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfvaEVqsBZtk"
   },
   "outputs": [],
   "source": [
    "# Building decision tree model\n",
    "dt = DecisionTreeClassifier(criterion='entropy', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting decision tree model\n",
    "dt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the performance on the training data\n",
    "y_pred_train = lg.predict(x_train)\n",
    "metrics_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the performance on the test data\n",
    "model_metrics.loc[len(model_metrics)] = evaluate_model_and_metrics(dt, x_test, y_test, X_scaled, default_model, 'dt', 'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances_or_coefficients(dt, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_subset_features(\n",
    "    model=dt,\n",
    "    x_test=x_test,\n",
    "    y_test=y_test,\n",
    "    subset='false_negatives'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "Model is overfitting the training data, but the performance on the test data is very good compared to Logistic Regression model.\\\n",
    "Debt to income ratio is the most important factor is default, while the delinquent credit lines was the most important for Logistic Regression.\\\n",
    "The importance of the features is in line with general expectations.\\\n",
    "Once again we observe that the False Negatives have the mean value for the more important feature(s) near zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdO4E2btpoPD"
   },
   "source": [
    "### **Decision Tree - Hyperparameter Tuning**\n",
    "\n",
    "* Hyperparameter tuning is tricky in the sense that **there is no direct way to calculate how a change in the hyperparameter value will reduce the loss of your model**, so we usually resort to experimentation. We'll use Grid search to perform hyperparameter tuning.\n",
    "* **Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters.** \n",
    "* **It is an exhaustive search** that is performed on the specific parameter values of a model.\n",
    "* The parameters of the estimator/model used to apply these methods are **optimized by cross-validated grid-search** over a parameter grid.\n",
    "\n",
    "**Criterion {“gini”, “entropy”}**\n",
    "\n",
    "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n",
    "\n",
    "**max_depth** \n",
    "\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "**min_samples_leaf**\n",
    "\n",
    "The minimum number of samples is required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "You can learn about more Hyperpapameters on this link and try to tune them. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "id": "xycLhQBv4fwC",
    "outputId": "afa9b489-6288-4235-b01b-faace8483637"
   },
   "outputs": [],
   "source": [
    "# Define the Decision Tree model\n",
    "dtree_estimator_grid = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "parameters = {\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'max_features': ['sqrt', 'log2', 0.5],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize results dictionary and DataFrame\n",
    "results = {}\n",
    "\n",
    "# Perform GridSearchCV for each scoring metric\n",
    "for opt in optimum:\n",
    "    print(f\"Optimizing for: {opt}\")\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=dtree_estimator_grid,\n",
    "        param_grid=parameters,\n",
    "        scoring=opt,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Perform the grid search on the training data\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Store best parameters and cross-validation score\n",
    "    results[opt] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_model': grid_search.best_estimator_\n",
    "    }\n",
    "\n",
    "    print(f\"Best Parameters for {opt}: {results[opt]['best_params']}\")\n",
    "    print(f\"Best Cross-Validation Score for {opt}: {results[opt]['best_score']}\")\n",
    "\n",
    "    # Evaluate the best model and append results\n",
    "    model_metrics.loc[len(model_metrics)] = evaluate_model_and_metrics(results[opt]['best_model'], x_test, y_test, X_scaled, default_model, f'dtree_estimator_grid_{opt}', 'Decision Tree', grid_search=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T23:34:18.437299Z",
     "iopub.status.busy": "2024-12-09T23:34:18.436987Z",
     "iopub.status.idle": "2024-12-09T23:34:18.440215Z",
     "shell.execute_reply": "2024-12-09T23:34:18.439689Z",
     "shell.execute_reply.started": "2024-12-09T23:34:18.437286Z"
    }
   },
   "source": [
    "**Observations:**\n",
    "\n",
    "- The tuned models are performing marginally better in comparison to the model with default values of hyperparameters.\n",
    "- While these models are giving good results for recall, there is opportunity to improve the overall performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4x88goTBZtk"
   },
   "source": [
    "### **Building a Random Forest Classifier**\n",
    "\n",
    "**Random Forest is a bagging algorithm where the base models are Decision Trees.** Samples are taken from the training data and on each sample a decision tree makes a prediction. \n",
    "\n",
    "**The results from all the decision trees are combined together and the final prediction is made using voting or averaging.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the Random Forest classifier on the training data\n",
    "rf = RandomForestClassifier(criterion='entropy', random_state=1)\n",
    "\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance on the training data\n",
    "y_pred_train_rf = rf.predict(x_train)\n",
    "\n",
    "metrics_score(y_train, y_pred_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the performance on the test data\n",
    "model_metrics.loc[len(model_metrics)] = evaluate_model_and_metrics(rf, x_test, y_test, X_scaled, default_model, 'rf', 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances_or_coefficients(rf, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_subset_features(\n",
    "    model=rf,\n",
    "    x_test=x_test,\n",
    "    y_test=y_test,\n",
    "    subset='false_negatives'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "Random forest model has a much better overall performance than decisiontree model.\\\n",
    "The feature importance for top 5 features has also changed slightly, with ltv taking precedence.\\\n",
    "Adjusting the threshold may improve model performance and should be implemented for the final model.\\\n",
    "The theory about False Negatives still holds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAx1Ooocp72i"
   },
   "source": [
    "### **Random Forest Classifier Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Random Forest model\n",
    "rf_estimator_grid = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'max_features': ['sqrt', 'log2', 0.5],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize results dictionary and DataFrame\n",
    "results = {}\n",
    "\n",
    "# Perform GridSearchCV for each scoring metric\n",
    "for opt in optimum:\n",
    "    print(f\"Optimizing for: {opt}\")\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf_estimator_grid,\n",
    "        param_grid=param_grid,\n",
    "        scoring=opt,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Perform the grid search on the training data\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Store best parameters and cross-validation score\n",
    "    results[opt] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_model': grid_search.best_estimator_\n",
    "    }\n",
    "\n",
    "    print(f\"Best Parameters for {opt}: {results[opt]['best_params']}\")\n",
    "    print(f\"Best Cross-Validation Score for {opt}: {results[opt]['best_score']}\")\n",
    "\n",
    "    # Evaluate the best model and append results\n",
    "    model_metrics.loc[len(model_metrics)] = evaluate_model_and_metrics(results[opt]['best_model'], x_test, y_test, X_scaled, default_model, f'rf_estimator_grid_{opt}', 'Random Forest', grid_search=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger garbage collection and print the number of objects collected\n",
    "collected_objects = gc.collect()\n",
    "print(f\"Garbage collector freed {collected_objects} objects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Tuned RF models are the best-performing among all the models so far, and is giving us scores on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPywjJo6uS-9"
   },
   "source": [
    "**Model Comparision based on expected revenue and confidence level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPywjJo6uS-9"
   },
   "source": [
    "Overall performance is better for tree based models in comparision with logistic regression. This points to non-linearity in the dataset and existence of outliers, both of which are handled very effectievely by tree based models. \n",
    "\n",
    "As can be seen from the above comparision, Model 8 gives the best performance from a revenue standpoint, however, Models 11/12/13 have the best performance in terms of the metrics. This descrepency may be because Model 8 performed well against clients with higher loan values. However a more detailed analysis needs to be performed to understand the descrepency. \n",
    "\n",
    "There is of course room for improvement in all of the above models by fine tuning the hyperparameters to focus on reducing False Negatives (while not overly sacrificing the recall), as the cost of False Negatives is substantially higher than False positives (i.e. False negatives cost the lender the principle and the interest whereas False positives cost the lender only the interest). This can be achieved by adjusting the weights and fine tuning the threshold to penalize False Negatives more than False Positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mtOvTtEY7sM"
   },
   "source": [
    "**2. Refined insights and recommandations:** \n",
    "\n",
    "1. Improve Data Quality and capture additional data\n",
    "> - Encourage applicants to provide missing details, such as job_type and loan_request_reason\n",
    "> - Use dropdowns and predefined options for categorical data like job_type to minimize typos and inconsistencies.\n",
    "> - Include gross income and monthly expenses to calculate a more accurate debt-to-income ratio, as this is the most important factor in determining eligibility\n",
    "> - Collect data on savings, investment portfolios, and other assets to assess overall financial stability\n",
    "> - Obtain credit scores and histories from reputable bureaus to ensure a consistent, accurate representation of creditworthiness\n",
    "\n",
    "2. Remove bias in the dataset\n",
    "> - Ensure that all demographic groups and professions are fairly represented in the data to avoid skewing results\n",
    "> - Reduce overrepresentation of risky loan types (e.g., debt consolidation loans) by diversifying the dataset\n",
    "\n",
    "3. Business model\n",
    "> - Gather borrower feedback on application processes and loan terms to identify areas for business improvement\n",
    "> - Use the model to identify high-risk borrower segments (e.g., high debt-to-income ratios, multiple derogatory reports)\n",
    "> - Higher interest rates or stricter loan limits for high-risk borrowers\n",
    "> - Discounts or incentives for low-risk borrowers\n",
    "> - Offer customized repayment options for borrowers facing temporary financial difficulties\n",
    "> - Regular evaluation of bias and fairness to ensure legal and ethical compliance\n",
    "\n",
    "**Summary**\n",
    "The loan default problem is heavily influenced by credit history, debt burden, and dataset biases (e.g., debt consolidation). \n",
    "The most effective strategies will involve:\n",
    "> - Robust feature engineering to balance stabilizing and risk-indicating factors.\n",
    "> - Careful tuning of decision thresholds and class weights to minimize false negatives without inflating false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNJHd0R7Y7sM"
   },
   "source": [
    "**3. Proposal for the final solution design:**\n",
    "\n",
    "While it is tempting to adopt Model 8, due to its performance against revenue metric, it should be noted that these models may not perform well against future customers due to poorer metrics as compared to Model 11/12/13.\\\n",
    "So the lender should adopt a refined version of Model 13.\\\n",
    "In summary, for better future default prediction, **Model 13 (Random Forest with GridSearch, optimized for recall) is the best choice** due to its strong balance between recall, precision, and F1-score, along with high confidence.\\\n",
    "It is most likely to perform well in minimizing both false negatives (missed defaults) and false positives (incorrect rejections).\\\n",
    "The lender should also **implement the recommendations** from the section above to get the most out of predictive model on an ongoing basis which will benefit the lender in more than one ways.\n",
    "> - Improve model accuracy and robustness by reducing bias and missing information\n",
    "> - Ensures fair and personalized loan offerings, improving customer satisfaction\n",
    "> - Reduces legal and ethical risks by eliminating discriminatory practices\n",
    "> - Helps mitigate default risk and increases profitability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold optimization for the chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the best model - rf_estimator_grid_recall, with the same parameters from gridsearch\n",
    "\n",
    "# Define the model\n",
    "rf_estimator_best = RandomForestClassifier(\n",
    "    bootstrap=False,\n",
    "    criterion='gini',\n",
    "    max_depth=None,\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=5,\n",
    "    n_estimators=100,\n",
    "    random_state=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(rf_estimator_best, x_train, y_train, cv=5, scoring='recall')\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Score:\", cv_scores.mean())\n",
    "\n",
    "# Fit the model on the entire training dataset\n",
    "rf_estimator_best.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_tuned = rf_estimator_best.predict(x_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "recall = recall_score(y_test, y_tuned)\n",
    "print(\"Recall Score on Test Set:\", recall)\n",
    "metrics_score(y_test, y_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_probs_best = rf_estimator_best.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_probs_best)\n",
    "\n",
    "# Plot values of precisions, recalls, and thresholds\n",
    "plt.figure(figsize = (10, 7))\n",
    "plt.plot(thresholds, precision[:-1], 'r--', label = 'precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g--', label = 'recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the threshold\n",
    "threshold = 0.4  # based on the precision recall curve above\n",
    "\n",
    "pred_prob_rf_best = rf_estimator_best.predict_proba(X_scaled)\n",
    "pred_prob_rf_thrsh = (pred_prob_rf_best >= threshold).astype(int)\n",
    "default_model_best = default_model.copy()\n",
    "default_model_best['prob_default'] = pred_prob_rf_thrsh[:, 1]\n",
    "revenue_best, confidencef_best = calculate_expected_with_confidence(default_model_best, 'loan_amount_approved')\n",
    "print(revenue_best, confidencef_best)\n",
    "\n",
    "y_pred_threshold = (y_probs_best >= threshold).astype(int)\n",
    "metrics_score(y_test, y_pred_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances_or_coefficients(rf_estimator_best, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_subset_features(rf_estimator_best, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "Adjusting the threshold has significantly improved performance for the recall and the revenue expectation.\\\n",
    "No of False negatives has gone down substantially, which is driving higher revenue expectation.\\\n",
    "Feature importances are similar as well as the features driving False Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try fine tuning the threshold between 0.3 to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Define the recall thresholds\n",
    "recall_thresholds = np.arange(0.3, 0.6, 0.1)  # 0.3 to 0.5 with a step of 0.1\n",
    "\n",
    "# Initialize lists to store results\n",
    "recall_values = []\n",
    "revenue_values = []\n",
    "\n",
    "# Iterate over the thresholds\n",
    "for threshold in recall_thresholds:\n",
    "    # Predict probabilities on x_test and apply threshold\n",
    "    pred_prob_rf_best = rf_estimator_best.predict_proba(x_test)\n",
    "    y_pred_threshold = (pred_prob_rf_best[:, 1] >= threshold).astype(int)\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = recall_score(y_test, y_pred_threshold)\n",
    "    recall_values.append(recall)\n",
    "\n",
    "    # Update the default_model dataset with threshold-based predictions\n",
    "    default_model_best = default_model.copy()\n",
    "    pred_prob_scaled = rf_estimator_best.predict_proba(X_scaled)\n",
    "    default_model_best['prob_default'] = (pred_prob_scaled[:, 1] >= threshold).astype(int)\n",
    "\n",
    "    # Calculate expected revenue\n",
    "    revenue_best, _ = calculate_expected_with_confidence(default_model_best, 'loan_amount_approved')\n",
    "    revenue_values.append(revenue_best)\n",
    "\n",
    "    print(f\"Threshold: {threshold}, Recall: {recall:.3f}, Revenue: {revenue_best:.2f}\")\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "# Plot Recall\n",
    "axes[0].plot(recall_thresholds, recall_values, marker='o', label='Recall', color='blue')\n",
    "axes[0].set_title('Recall vs Threshold', fontsize=14)\n",
    "axes[0].set_ylabel('Recall', fontsize=12)\n",
    "axes[0].grid(axis='y', linestyle='--', alpha=0.6)\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot Revenue\n",
    "axes[1].plot(recall_thresholds, revenue_values, marker='s', label='Revenue', color='green')\n",
    "axes[1].set_title('Revenue vs Threshold', fontsize=14)\n",
    "axes[1].set_xlabel('Threshold', fontsize=12)\n",
    "axes[1].set_ylabel('Revenue', fontsize=12)\n",
    "axes[1].grid(axis='y', linestyle='--', alpha=0.6)\n",
    "axes[1].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "Threshold = 0.4 for balances risk (recall of 0.787) and business goals (revenue of 7,601,235.50).\\\n",
    "It reduces extreme trade-offs, ensuring the model performs reliably without overly conservative or aggressive predictions, for future customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try the XG Boost model for comparision with the best Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the XGBClassifier from the xgboost library\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# XGBoost Classifier\n",
    "xgb = XGBClassifier(random_state = 1, eval_metric = 'logloss')\n",
    "\n",
    "# Fitting the model\n",
    "xgb.fit(x_train,y_train)\n",
    "\n",
    "# Model Performance on the test data\n",
    "xgb_perf_test = model_performance_classification(xgb,x_test,y_test)\n",
    "\n",
    "print(xgb_perf_test)\n",
    "\n",
    "# Predict probabilities for revenue prediction\n",
    "pred_prob_xgb_best = xgb.predict_proba(X_scaled)\n",
    "pred_prob_xgb_thrsh = (pred_prob_xgb_best >= 0.5).astype(int)\n",
    "\n",
    "# Copy default model and add probabilities\n",
    "default_model_best_xgb = default_model.copy()\n",
    "default_model_best_xgb['prob_default'] = pred_prob_xgb_thrsh[:, 1]\n",
    "\n",
    "# Calculate expected revenue and confidence\n",
    "revenue_best_xgb, confidence_best_xgb = calculate_expected_with_confidence(\n",
    "    default_model_best_xgb, 'loan_amount_approved'\n",
    ")\n",
    "print(\"Revenue (XGBoost Best Model):\", revenue_best_xgb)\n",
    "print(\"Confidence (XGBoost Best Model):\", confidence_best_xgb)\n",
    "\n",
    "metrics_score(y_test, xgb.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_probs_xgb = xgb.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_probs_xgb)\n",
    "\n",
    "# Plot values of precisions, recalls, and thresholds\n",
    "plt.figure(figsize = (10, 7))\n",
    "plt.plot(thresholds, precision[:-1], 'r--', label = 'precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g--', label = 'recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the threshold\n",
    "threshold = 0.5  # based on the precision recall curve above\n",
    "\n",
    "pred_prob_xgb = xgb.predict_proba(X_scaled)\n",
    "pred_prob_xgb_thrsh = (pred_prob_xgb_best >= threshold).astype(int)\n",
    "default_model_best = default_model.copy()\n",
    "default_model_best['prob_default'] = pred_prob_xgb_thrsh[:, 1]\n",
    "revenue_best, confidencef_best = calculate_expected_with_confidence(default_model_best, 'loan_amount_approved')\n",
    "print(revenue_best, confidencef_best)\n",
    "\n",
    "y_pred_threshold_xgb = (y_probs_xgb >= threshold).astype(int)\n",
    "metrics_score(y_test, y_pred_threshold_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances_or_coefficients(xgb, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_subset_features(\n",
    "    model=xgb,\n",
    "    x_test=x_test,\n",
    "    y_test=y_test,\n",
    "    subset='false_negatives'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model\n",
    "# xgb_grid = XGBClassifier(objective='binary:logistic', random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],  # Number of boosting rounds\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],  # Step size shrinkage\n",
    "#     'max_depth': [3, 5, 7],  # Maximum tree depth\n",
    "#     'subsample': [0.8, 1.0],  # Subsampling ratio\n",
    "#     'colsample_bytree': [0.8, 1.0],  # Subsampling ratio of columns\n",
    "#     'gamma': [0, 0.1, 0.2],  # Minimum loss reduction\n",
    "# }\n",
    "\n",
    "# # Set up GridSearchCV\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=xgb_grid,\n",
    "#     param_grid=param_grid,\n",
    "#     scoring='accuracy',  # Metric for evaluation (can be changed to f1, recall, etc.)\n",
    "#     cv=5,  # 5-fold cross-validation\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1  # Use all processors\n",
    "# )\n",
    "\n",
    "# # Perform the grid search\n",
    "# grid_search.fit(x_train, y_train)\n",
    "\n",
    "# # Display the best parameters and best score\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# # Evaluate the best model on the test set\n",
    "# best_model = grid_search.best_estimator_\n",
    "# test_accuracy = best_model.score(x_test, y_test)\n",
    "# print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T13:50:33.621594Z",
     "iopub.status.busy": "2024-12-13T13:50:33.620879Z",
     "iopub.status.idle": "2024-12-13T13:50:33.625959Z",
     "shell.execute_reply": "2024-12-13T13:50:33.625455Z",
     "shell.execute_reply.started": "2024-12-13T13:50:33.621548Z"
    }
   },
   "source": [
    "## Final Thought\n",
    "- Random forest optimized using gridsearch and finetuned for threshold is the best option going forward\n",
    "- XG Boost behavior is very similar to the random forest models but the performance on the metrics is not as good as the final random forest model. This may be because the XG Boost model is not optimized using gridsearch. I was not able to run GridSearch with XGB die to some error which I could not resolve!\n",
    "- **False Negatives always have the values of the most important features near zero!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All Working')\n",
    "# End time\n",
    "script_end_time = time.time()\n",
    "total_time = script_end_time - script_start_time\n",
    "print(f\"Total execution time for the script: {total_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

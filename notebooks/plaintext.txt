# **Loan Default Prediction**
## **Problem Definition**



### **The Context:**



Loan defaults pose a significant threat to a bank’s financial health. For instance, while the mortgage delinquency rate in the United States has been gradually declining since the COVID-19 pandemic, as of February 2024, the overall delinquency rate was 2.8%. Such non-performing assets (NPAs) erode profitability and constrain the bank’s ability to extend new credit. Moreover, the manual loan approval process is labor-intensive and susceptible to human errors and biases, leading to potential misjudgments in loan approvals or rejections. Addressing this issue is crucial to enhance operational efficiency, reduce financial risks, and uphold equitable lending practices.



### **The objective:**



The goal is to modernize and streamline the loan approval process by leveraging machine learning classification methods, capable of as accurately predicting loan defaults as possible using available data. This model should:

 - Enhance operational efficiency by automating repetitive tasks and reducing the time spent on manual credit assessments.

 - Mitigate risk by identifying high-risk applicants before loan issuance.

 - Ensure fairness by eliminating biases that could disadvantage certain demographics, thus aligning with federal regulation, legal requirements and ethical standards.

 - Ensure Regulatory Compliance by aligning with legal frameworks such as the Equal Credit Opportunity Act, which mandates non-discriminatory lending practices and requires transparent justifications for adverse decisions.

 - Enable data-driven decision-making by providing interpretable insights into why a loan is approved or rejected, allowing for greater transparency and trust with stakeholders.

 - Promote Fairness by eliminating biases inherent in human judgment, ensuring equitable treatment of all applicants.

 - The model should be extensible to learn from future loan defaults



### **The key questions:**



 - Which applicants are likely to default on their loans? Identifying potential defaulters enables targeted risk management strategies.

 - What factors are most critical in assessing an applicant’s creditworthiness? Determining key indicators such as income stability, debt-to-income ratio, and credit history informs more accurate evaluations.

 - How can the loan approval process be optimized for efficiency and fairness? Implementing automated, data-driven systems can streamline operations while upholding ethical standards.

 - How can the model ensure compliance with regulatory requirements? Providing interpretable justifications for loan decisions is essential to meet legal obligations and maintain transparency.

 - How can historical biases be prevented from influencing the model? Ensuring the model is trained on unbiased data is crucial to avoid perpetuating past discriminatory practices.



### **The problem formulation**:



Data science enables us to:

 - Predict defaults: Use predictive analytics to assess the likelihood of applicants failing to meet their repayment obligations.

 - Automate Decision-Making: Replace subjective human assessments with objective, data-driven evaluations, enhancing consistency and speed.

 - Identify Key Risk Factors: Analyze data to pinpoint variables that significantly influence default risk, such as high debt-to-income ratios or unstable employment histories.

 - Ensure Model Interpretability: Develop transparent models that provide clear explanations for their decisions, facilitating compliance with regulations and building stakeholder trust.

 - Mitigate Bias: Implement fairness algorithms to detect and correct biases, ensuring the model’s decisions are equitable and just.



By solving this problem, the bank not only reduces risk but also transforms its loan approval system into a benchmark for innovation and inclusivity.
## **Data Description:**

The Home Equity dataset (HMEQ) contains baseline and loan performance information for 5,960 recent home equity loans. The target (BAD) is a binary variable that indicates whether an applicant has ultimately defaulted or has been severely delinquent. This adverse outcome occurred in 1,189 cases (20 percent). 12 input variables were registered for each applicant.





* **BAD:** 1 = Client defaulted on loan, 0 = loan repaid



* **LOAN:** Amount of loan approved.



* **MORTDUE:** Amount due on the existing mortgage.



* **VALUE:** Current value of the property. 



* **REASON:** Reason for the loan request. (HomeImp = home improvement, DebtCon= debt consolidation which means taking out a new loan to pay off other liabilities and consumer debts) 



* **JOB:** The type of job that loan applicant has such as manager, self, etc.



* **YOJ:** Years at present job.



* **DEROG:** Number of major derogatory reports (which indicates a serious delinquency or late payments). 



* **DELINQ:** Number of delinquent credit lines (a line of credit becomes delinquent when a borrower does not make the minimum required payments 30 to 60 days past the day on which the payments were due). 



* **CLAGE:** Age of the oldest credit line in months. 



* **NINQ:** Number of recent credit inquiries. 



* **CLNO:** Number of existing credit lines.



* **DEBTINC:** Debt-to-income ratio (all your monthly debt payments divided by your gross monthly income. This number is one way lenders measure your ability to manage the monthly payments to repay the money you plan to borrow.
## **Import the necessary libraries and Data**
## **Data Overview**

- Reading the dataset

- Understanding the shape of the dataset

- Checking the data types

- Checking for missing values

- Checking for duplicated values
#### Let's check any inconsitencies in the data
#### Observations

1. Dataset contains several missing values. These need to be treated individually as the use of summary statistic may not be appropriate or sufficient for this this dataset i.e. morgage_amount_due can have missing values,but loan_request_reason or property_current_value cannot be. Also many of the missing values may depend on values in other columns, such as major_derogatory_report, delinquent_credit_lines, recent_credit_inquiries, existing_credit_lines and debt_to_income_ratio are all correlated

2. Features like debt_to_income_ratio and major_derogatory_reports have significant missing values (21.3% and 11.9%, respectively)

4. Dataset has 5960 rows and 13 columns (12 independant features)

5. Data is for approved loans only, as there are no missing values in the loan_amount_approved

6. No duplicate records, which is understandable, as most individuals or families requesting for loan are unique

7. All categorical values look clean - no typos, unique values and consistent labeling

8. Many of the features should be integrers, but are floats in the dataset - major_derogatory_reports, delinquent_credir_lines, recent_credit_inquiries and existing_credit_lines. If the actual values, indeed are floats, its a data descripency which should be addresses before further processing

9. Proportion of defaulting client is 20%, which is too high by industry standards, indicating a highly biased dataset

10. Several records had a very high loan to value ratio. This is quite unusual. All the records with ltv > 2.0 were dropped

11. Also, the features applicant job type and years at present job, may invite discimatory litigation (class action lawsuits)!
## Summary Statistics
#### Observations



1. Skewness: major_derogatory_reports, delinquent_credit_lines, debt_to_income_ratio are concentrated near zero with a long tail, indicating skewness

2. Outliers: mortgage_amount_due, property_current_value, age_of_oldest_credit_line, and debt_to_income_ratio extreme high max values, with 75% percentile closer to the mean, indicating outliers.



Vizualization of the distributions of these features will provide clarity on these observations.
## **Exploratory Data Analysis (EDA) and Visualization**
- EDA is an important part of any project involving data.

- It is important to investigate and understand the data better before building a model with it.

- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.

- A thorough analysis of the data, in addition to the questions mentioned below, should be done.
**Leading Questions**:

1. What is the range of values for the loan amount variable "LOAN"?

2. How does the distribution of years at present job "YOJ" vary across the dataset?

3. How many unique categories are there in the REASON variable?

4. What is the most common category in the JOB variable?

5. Is there a relationship between the REASON variable and the proportion of applicants who defaulted on their loan?

6. Do applicants who default have a significantly different loan amount compared to those who repay their loan?

7. Is there a correlation between the value of the property and the loan default rate?

8. Do applicants who default have a significantly different mortgage amount compared to those who repay their loan?
### **Data prep for EDA and subsequent processing**
#### Missing value treatement

I have used techniques from recommandation systems to impute the missing values **(Reference: "Missing Data: A Gentle Introduction" by Patrick E. McKnight, Katherine M. McKnight, Souraya Sidani, and Aurelio José Figueredo)**. Below is a summary of methods used for imputatation



**1. KNN Imputation (Low/Moderate Missingness)**



* mortgage_amount_due, property_current_value, loan_request_reason, applicant_job_type, age_of_oldest_credit_line, existing_credit_line columns have less than 10% missing values. KNN imputation is effective for low-to-moderate missingness by finding similar rows based on available features and averaging/matching the corresponding feature values.



**2. Iterative Imputer (High Missingness)**



* debt_to_income_ratio, major_derogatory_reports, delinquentt_credit_line, years_at_present_job, recent_credit_inquirie columns have higher missing percentages (10–21%). Iterative imputation models each feature as a function of the others, iteratively predicting missing values to best fit the observed data.



**3. Encoding for Categorical Data**



* loan_request_reason, applicant_job_type columns contain categorical data. They were encoded using LabelEncoder to convert string values into numeric categories, allowing compatibility with KNN imputer.
#### Prepare data for EDA
### **Univariate Analysis**

#### Observation:

Mortgage amount due shows a slightly right skewed distribution, but with a lot of outliers. These may need to be treated based on the type of modelling technique used for modeling
#### Observation:

Current property value shows a slightly right skewed distribution, but with a lot of outliers. These may need to be treated based on the type of modelling technique used for modeling
#### Observation:

No of years at present job shows a moderately right skewed, multi-modal distribution, with a few outliers, indicating most loan requesters maybe in middle age group. Extreme outliers may have to be capped
#### Observations:

Major derogatory reports shows a highly right skewed distribution, with most values around zero. This may indicate a bias in the prescreening process
#### Observations:

Delinquent credit lines shows a highly right skewed distribution, with most values around zero. This may indicate a bias in the prescreening process
#### Observation:

Oldest credit line shows fairly uniform bi-modal distribution, with quite a few outliers. What maybe of interest here are the loan requesters with low or zero age of credit line. Also extreme outliers may have to be capped
#### Observations:

Recent credit inquiries shows a highly right skewed distribution, with most values around zero. This may indicate a bias in the prescreening process
#### Observations:

Existing credit lines shows a reasonably uniform distribution with slight right tail and a few outliers. Extreme outliers may need to be capped
#### Observations:

Debt to income ratio is slightly left skewed, with outliers on both sides. Extreme outlier on the high may need capping. Left skewness is a good thing as a low debt to income ratio is desirable

The distribution is also showing a sharp drop around ~ 40. Should investigate this further
#### Observation:

Amount of loan approved shows a slightly right skewed distribution, but with a lot of outliers. These may need to be treated based on the type of modelling technique used for modeling
#### Observations:

1. ~ 70% of requests are for debt consolidation, indication a fundamental bias in the dataset, or biased selection process

2. Majority (~ 45%) of the loan requests are from people who did not provide details of the profession, suggesting improvements in the data collection process
### **Bivariate Analysis**
### Check correlation for numeric variables
#### Observations:

1. Property value as of date seems to have a strong +ve correlation to mortgage amount due. This is only possible if all if the mortgages in the dataset were approved around the same time and the approved mortgage amount was a fixed % of the property value, irrespective of other factors. So age of the mortgage may be a useful feature (latent variable). Most other variables have weak correlations
#### Let's check the distribution of numerical variables to loan defaults
#### Observations

1. No of defaulters are highest where recent credit inquiries, delinquent credit lines, major derogatory reports are the lowest. This seems counter intuitive at first, how ever this is most likely due to these customers not having any credit records at all. This will happen if these customers were denied credit in the past and have not applied for credit in the recent time (for the duration during which this data was collected). Additional data is required to understand this anamoly - gross income, demographics, macroeconomic factors such severe recession or other financial distress

2. Default rate is the extra-ordinarily high (80%) for customers with debt to income ratio between 30-41. Understanding this anomaly likely requires a more detailed analysis of the borrower behavior and external conditions. This data set is insufficient for such analysis, as was commented in no 1 above
#### Let's check the distribution of categorical variables to loan defaults
#### Observations:

1. As expected, loan defaults are higher for clients who requested loan for debt consolidation, as these clients already had substantial debt and most likely not enough income to cover the debt

2. Applicants with job type as 'other' have the highest default rate. This may be because these clients do not have a steady job and as such chose not to disclose it. Also the higher default rate for 'Mgr' and 'ProfExe' is interesting and may need further analysis
#### Observation:

1. All the available features are significant for the prediction of target variable

2. Also, features related to clients credit posture seem to have higher significance, which is expected
#### Observation:

current property value shows a very high correlation with mortgage amount due, which is expected. These should be replaced with an engineered feature. Additional insights can be gained from priniciple component analysis
### **Multivariate Analysis**
#### Multivariate analysis using PCA
#### Prep the data for PCA
#### Observation

There is no single component which can be attributed with a large variance in the dataset
Let's check the contribution of each of the original features to the principle components
#### Create a new feature to replace Mortgage due and property value as they are highly correlated, and check if it impacts the PCs
#### Observations

Based on PCA analysis, it is clear that replacing 'mortgage_amount_due', 'property_current_value' by 'mortgage_to_property_ratio' will not impact modelling performance
## Treating Outliers

Only the outliers for mortgage due and property value have been dropped, where the ratio of these features was improbable. See section to 1.4.0.1. All the other outliers are maintained as is as they seem to be realistic data points
## Treating Missing Values: See section 1.6.1.1
## **Important Insights from EDA**



### Key Data Insights for Classification



#### Handling Missing Values

- Critical features like `loan_request_reason` and `property_current_value` must have no missing values as they are essential for predicting loan defaults.

- Features like `major_derogatory_reports` and `debt_to_income_ratio` (with missing rates of 11.9% and 21.3%) require imputation or exclusion based on their impact.



#### Dataset Characteristics

- **Records**: 5960 with no duplicates.

- **Default Rate**: High (20%), indicating no class imbalance but potential dataset biases.

- **Target Variable**: Well-defined (`loan_default`).



#### Potential Predictors

- Features such as `debt_to_income_ratio`, `major_derogatory_reports`, `recent_credit_inquiries`, and `existing_credit_lines` are likely predictive but require normalization and outlier treatment.



#### Feature Engineering

- Replace correlated variables (`mortgage_amount_due` and `property_current_value`) with `mortgage_to_property_ratio`.

- Consider creating latent features such as "age of mortgage" or "credit history duration" for added predictive power.



#### Skewness and Outliers

- Many numerical features (e.g., `debt_to_income_ratio`, `mortgage_amount_due`) are right-skewed with significant outliers.

- Apply transformations and outlier capping to improve model robustness.



#### Bias and Feature Validity

- Categorical variables like `applicant_job_type` and `loan_request_reason` show inherent bias (e.g., 70% of loans are for debt consolidation).

- Features like `job_type` and `years_at_present_job` may raise ethical or legal concerns.



#### Class Separation and Patterns

- Default rates are high for clients with minimal credit records or specific debt-to-income ranges (30–41%).

- This highlights actionable risk segments.



---



### Key Statistical Insights



#### Correlations

- Strong correlation between `property_current_value` and `mortgage_amount_due` supports dimensionality reduction.

- Weak correlations across most features suggest the need for engineered features or interaction terms.



#### Categorical Variable Impact

- High default rates for specific `job_type` categories (`other`, `Mgr`, `ProfExe`) suggest profession-based insights could improve classification if handled carefully.



---



### Recommendations for Modeling



#### Preprocessing

- Treat missing values based on correlation and domain importance.

- Normalize or transform skewed features and cap outliers.

- Create engineered features (e.g., `mortgage_to_property_ratio`).



#### Bias Mitigation

- Address potential dataset bias (e.g., over-representation of debt consolidation loans).

- Carefully handle legally sensitive features like `job_type` to avoid bias in classification.



#### Feature Selection

- Prioritize credit-related features (`debt_to_income_ratio`, `recent_credit_inquiries`, `delinquent_credit_lines`) given their significance for predicting defaults.



#### Advanced Analysis

- Investigate latent variables and external data (e.g., economic conditions) to explain observed anomalies in defaults.
## **Model Building - Approach**

- Data preparation

- Partition the data into train and test set

- Build the model

- Fit on the train data

- Tune the model

- Test the model on test set
#### Confusion Matrix
<div style="display: inline-block">

    

| Actual/Predicted       | Predicted: 0 (No Default) | Predicted: 1 (Default)|

|:-----------------------|:--------------------------|:----------------------|

| Actual: 0 (No Default) | True Negative (TN)        | False Positive (FP)   |

| Actual: 1 (Default)    | False Negative (FN)       | True Positive (TP)    |



</div>
Classification model should maximize the expected value of the revenue from lending. In order to acheive this:

1. Maximize True Negatives - This will bring in the bulk of the interest revenue

2. Minimize False Negatives - This will reduce the interest loss due to defaulted loans (principle can be recouped via foreclosure)

3. Minimize False Positives - This will minimize the opportunity loss

To achieve this, we will define 2 functions

### Function to calculate the expected revenue:



$

\text{Expected Revenue} = \text{Revenue from True Negatives (TN)} - \text{Loss from False Positives (FP)} - \text{Loss from False Negatives (FN)}

$



1. Revenue from True Negatives (TN):



$

\text{Revenue}{\text{TN}} = P{\text{ND}} \cdot (1 - \text{Actual}) \cdot L \cdot r

$

Where:

	•	$P_{\text{ND}} = 1 - P_{\text{D}}$: Probability of non-default.

	•	$\text{Actual}$: Indicator of the actual outcome ($0$ = non-default, $1$ = default).

	•	$L$: Loan amount.

	•	$r$: Interest rate.



2. Loss from False Positives (FP):



$

\text{Loss}{\text{FP}} = P{\text{D}} \cdot (1 - \text{Actual}) \cdot L \cdot r

$

Where:

	•	$P_{\text{D}}$: Probability of default.

	•	$(1 - \text{Actual})$: Ensures the outcome is non-default ($\text{Actual} = 0$).

	•	$L \cdot r$: Opportunity cost (lost interest revenue).



3. Loss from False Negatives (FN):



$

\text{Loss}{\text{FN}} = P{\text{ND}} \cdot \text{Actual} \cdot L \cdot (1 + r)

$

Where:

	•	$P_{\text{ND}}$: Probability of non-default.

	•	$\text{Actual}$: Ensures the outcome is default ($\text{Actual} = 1$).

	•	$L \cdot (1 + r)$: Total loss (principal + interest).



Final Formula:



$

\text{Expected Revenue} = \left( P_{\text{ND}} \cdot (1 - \text{Actual}) \cdot L \cdot r \right)

	•	\left( P_{\text{D}} \cdot (1 - \text{Actual}) \cdot L \cdot r \right)

	•	\left( P_{\text{ND}} \cdot \text{Actual} \cdot L \cdot (1 + r) \right)

$



Definitions:

	•	$P_{\text{D}}$: Probability of default.

	•	$P_{\text{ND}} = 1 - P_{\text{D}}$: Probability of non-default.

	•	$\text{Actual}$: Binary variable ($0 = \text{Non-default}, 1 = \text{Default}$).

	•	$L$: Loan amount.

	•	$r$: Interest rate.
### Logistic Regression
### Observation

The performance of the model on the trained data and test data is almost identical.\

Primary driving factors are related to the -ve credit history of the client and the income.\

Loan amount approved and existing credit lines has a negative coefficient.\

Overall the performance of Logistic regression is not satisfactory as the recall is very low.



**Let's see if we can improve the performance by gridsearch**
#### Observation:

Even GridSearch is not able to provide a satisfactory performance, as is evident from recall scores. The best recall score is the same as the model without GridSearch



However, the best result is for the model optimized for recall



The false positives seem to be primarily driven by credit-related factors (delinquent_credit_lines, debt_to_income_ratio, and major_derogatory_reports) overpowering stabilizing factors like small loan amounts, long credit histories, or stable job types. Addressing these imbalances in the model can improve its predictive accuracy.
### Decision Tree
#### Observation:



Model is overfitting the training data, but the performance on the test data is very good compared to Logistic Regression model

Debt to income ratio is the most important factor is default, while the delinquent credit lines was the most important for Logistic Regression

The importance of the features is in line with general expectations
### **Decision Tree - Hyperparameter Tuning**



* Hyperparameter tuning is tricky in the sense that **there is no direct way to calculate how a change in the hyperparameter value will reduce the loss of your model**, so we usually resort to experimentation. We'll use Grid search to perform hyperparameter tuning.

* **Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters.** 

* **It is an exhaustive search** that is performed on the specific parameter values of a model.

* The parameters of the estimator/model used to apply these methods are **optimized by cross-validated grid-search** over a parameter grid.



**Criterion {“gini”, “entropy”}**



The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.



**max_depth** 



The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.



**min_samples_leaf**



The minimum number of samples is required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.



You can learn about more Hyperpapameters on this link and try to tune them. 



https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html

**Observations:**



- The tuned models are performing marginally better in comparison to the model with default values of hyperparameters.

- While these models are giving good results for recall, there is opportunity to improve the overall performance

### **Building a Random Forest Classifier**



**Random Forest is a bagging algorithm where the base models are Decision Trees.** Samples are taken from the training data and on each sample a decision tree makes a prediction. 



**The results from all the decision trees are combined together and the final prediction is made using voting or averaging.**
#### Observation:



Random forest model has a much better overall performance than decisiontree model. The feature importance for top 5 features has also changed slightly, with ltv taking precedence

Adjusting the threshold also substantially improved model performance and should be implemented for the final model as well
### **Random Forest Classifier Hyperparameter Tuning**
#### Observation:

Tuned RF models are the best-performing among all the models so far, and is giving us scores on the test dataset.
**Model Comparision based on expected revenue and confidence level**

1. Model1 - Logistic Regression with liblinear solver: **Total expected revenue: \$2698670.84 with confidence of: 0.84**

2. Model2 - Best Logistic Regression model with GridSearch: **Total expected revenue: \$2694612.65 with confidence of: 0.84**

3. Model3 - Decisiontree with entropy: **Total expected revenue: \$7501717.5 with confidence of: 1.0**

4. Model4 - Best DecisionTree model with GridSearch: **Total expected revenue: \$7458707.5 with confidence of: 1.0**

5. Model5 - RandomForest with entropy: **Total expected revenue: \$5562512.65485 with confidence of: 0.92**

6. Model6 - Best RandomForest with GridSearch: **Total expected revenue: \$6651963.2723875 with confidence of: 0.97**



**Model Comparision based on metrics**



<div style="display: inline-block">



| **Model**                        | **Precision** | **Recall** | **F1-Score** | **Confidence** |

|----------------------------------|---------------|------------|--------------|----------------|

| **Model 1** (Logistic Regression) | 0.75          | 0.27       | 0.40         | 0.84           |

| **Model 2** (Logistic Regression, GridSearch) | 0.75 | 0.27 | 0.40 | 0.84 |

| **Model 3** (Decision Tree)       | 0.68          | 0.57       | 0.62         | 1.0            |

| **Model 4** (Decision Tree, GridSearch) | 0.63      | 0.57       | 0.60         | 1.0            |

| **Model 5** (Random Forest)       | 0.92          | 0.63       | 0.75         | 0.92           |

| **Model 6** (Random Forest, GridSearch) | 0.91     | 0.70       | 0.79         | 0.97           |



</div>



Overall performance is better for tree based models in comparision with logistic regression. This points to non-linearity in the dataset and existence of outliers, both of which are handled very effectievely by tree based models. 



As can be seen from the above comparision, Model 3 gives the best performance from a revenue standpoint, however, Model 6 has the best performance in terms of the metrics. This descrepency may be because Model 3 performed well against clients with higher loan values. However a more detailed analysis needs to be performed to understand the descrepency. 



There is of course room for improvement in all of the above models by fine tuning the hyperparameters to focus on reducing False Negatives (while not overly sacrificing the recall), as the cost of False Negatives is substantially higher than False positives (i.e. False negatives cost the lender the principle and the interest whereas False positives cost the lender only the interest). This can be achieved by adjusting the weights and fine tuning the threshold to penalize False Negatives more than False Positives.
**2. Refined insights and recommandations:** 



1. Improve Data Quality and capture additional data

> - Encourage applicants to provide missing details, such as job_type and loan_request_reason

> - Use dropdowns and predefined options for categorical data like job_type to minimize typos and inconsistencies.

> - Include gross income and monthly expenses to calculate a more accurate debt-to-income ratio, as this is the most important factor in determining eligibility

> - Collect data on savings, investment portfolios, and other assets to assess overall financial stability

> - Obtain credit scores and histories from reputable bureaus to ensure a consistent, accurate representation of creditworthiness



2. Remove bias in the dataset

> - Ensure that all demographic groups and professions are fairly represented in the data to avoid skewing results

> - Reduce overrepresentation of risky loan types (e.g., debt consolidation loans) by diversifying the dataset



3. Business model

> - Gather borrower feedback on application processes and loan terms to identify areas for business improvement

> - Use the model to identify high-risk borrower segments (e.g., high debt-to-income ratios, multiple derogatory reports)

> - Higher interest rates or stricter loan limits for high-risk borrowers

> - Discounts or incentives for low-risk borrowers

> - Offer customized repayment options for borrowers facing temporary financial difficulties

> - Regular evaluation of bias and fairness to ensure legal and ethical compliance



**Summary**

The loan default problem is heavily influenced by credit history, debt burden, and dataset biases (e.g., debt consolidation). 

The most effective strategies will involve:

> - Robust feature engineering to balance stabilizing and risk-indicating factors.

> - Careful tuning of decision thresholds and class weights to minimize false negatives without inflating false positives.
**3. Proposal for the final solution design:**



While it is tempting to adopt Model 3 or Model 4, due to its performance against revenue metric, it should be noted that these models may not perform well against future customers due to poorer metrics as compared to Model 5 and Model 6.\

So the lender should adopt a refined version of Model 6.\

In summary, for better future default prediction, **Model6 (Random Forest with GridSearch, optimized for f1) is the best choice** due to its strong balance between recall, precision, and F1-score, along with high confidence.\

It is most likely to perform well in minimizing both false negatives (missed defaults) and false positives (incorrect rejections).\

The lender should also **implement the recommendations** from the section above to get the most out of predictive model on an ongoing basis which will benefit the lender in more than one ways.

> - Improve model accuracy and robustness by reducing bias and missing information

> - Ensures fair and personalized loan offerings, improving customer satisfaction

> - Reduces legal and ethical risks by eliminating discriminatory practices

> - Helps mitigate default risk and increases profitability
